{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":""},{"location":"#open-gis-python-based-cloud-native-geospatial-analysis","title":"Open GIS - Python-based Cloud Native Geospatial Analysis","text":"<p>Welcome to the comprehensive guide on Open GIS and cloud-native remote sensing! This curriculum moves beyond proprietary platforms, focusing on the power of Python, open-source standards, and distributed cloud computing.</p>"},{"location":"#the-proprietary-gee-vendor-lock-in-risk","title":"\u26a0\ufe0f The proprietary \"GEE\" Vendor Lock-in Risk","text":"<p>While Google Earth Engine (GEE) is a powerful tool, relying solely on it for groundbreaking geospatial research carries significant professional and intellectual risks:</p> <ul> <li>Vendor Mercy: GEE is a proprietary service owned by a private corporation. They have the power to change access terms, pricing, or even lock down years of your \"hard work\" and code scripts at any time.</li> <li>Intellectual Property &amp; Patents: Developing innovative methodologies \"natively\" inside the GEE ecosystem can complicate patent applications. As a proprietary \"black box,\" the underlying infrastructure belongs to Google, which may limit an author's ability to be fully credited or appreciated for groundbreaking innovation.</li> <li>Scientific Reproducibility: High-stakes geospatial analysis should be verifiable and portable. Proprietary platforms hinder true independence.</li> </ul>"},{"location":"#why-python-first","title":"\ud83d\udc0d Why Python First?","text":"<p>Python is the native home for advanced geospatial analysis and Deep Learning. By developing in an open-source Python environment:</p> <ol> <li>Innovation &amp; Patents: You develop in an environment you control. Innovative algorithms developed in Python are more suitable for patenting as they are built on open foundations, ensuring the author receives full credit.</li> <li>Native Development: Most cutting-edge AI and Geospatial libraries (PyTorch, TensorFlow, XArray) are developed in Python first. You gain immediate access to the latest breakthroughs.</li> <li>Future Confidence: Open source means reproducibility. Your work is not tied to a single company's roadmap or survival.</li> </ol>"},{"location":"#what-youll-learn","title":"\ud83c\udfaf What You'll Learn","text":"<ul> <li>XArray: Multi-dimensional labeled arrays for efficient data manipulation</li> <li>STAC (Spatio Temporal Asset Catalog): Standardized way to discover and access geospatial data</li> <li>Dask: Parallel computing library for scaling your analysis</li> <li>Zarr: Cloud-optimized array storage format</li> <li>XEE: XArray Earth Engine integration for seamless data access</li> </ul>"},{"location":"#why-cloud-native-remote-sensing","title":"\ud83d\ude80 Why Cloud-Native Remote Sensing?","text":"<p>Traditional remote sensing workflows often involve downloading large datasets to local machines, which is:</p> <ul> <li>Time-consuming: Downloading terabytes of data takes hours or days</li> <li>Storage-intensive: Requires significant local storage capacity</li> <li>Computationally limited: Constrained by local machine resources</li> <li>Not scalable: Difficult to process large areas or long time series</li> </ul> <p>Cloud-native approaches solve these problems by:</p> <p>\u2705 Accessing data on-demand - Only download what you need \u2705 Leveraging cloud compute - Scale processing power as needed \u2705 Using optimized formats - Zarr and COG for efficient data access \u2705 Parallel processing - Dask for distributed computing \u2705 Standardized discovery - STAC for finding relevant datasets  </p>"},{"location":"#course-structure","title":"\ud83d\udcda Course Structure","text":""},{"location":"#getting-started","title":"Getting Started","text":"<p>Learn the basics of setting up your environment and working with Google Colab for cloud-based analysis.</p>"},{"location":"#fundamentals","title":"Fundamentals","text":"<p>Master the core libraries: XArray for data manipulation, STAC for data discovery, Dask for parallel computing, Zarr for storage, and XEE for Earth Engine integration.</p>"},{"location":"#data-processing","title":"Data Processing","text":"<p>Apply your knowledge to real-world tasks like calculating spectral indices, masking clouds, extracting time series, and aggregating data.</p>"},{"location":"#advanced-topics","title":"Advanced Topics","text":"<p>Scale your analysis with advanced Dask techniques, cloud computing platforms, and optimization strategies.</p>"},{"location":"#practical-examples","title":"Practical Examples","text":"<p>Work through complete examples including NDVI analysis, land cover classification, change detection, and multi-temporal analysis.</p>"},{"location":"#key-technologies","title":"\ud83d\udee0\ufe0f Key Technologies","text":""},{"location":"#xarray","title":"XArray","text":"<p>XArray extends NumPy and Pandas to N-dimensional labeled arrays, making it perfect for satellite imagery with dimensions like time, latitude, longitude, and bands.</p> <pre><code>import xarray as xr\n\n# Open a dataset\nds = xr.open_dataset('sentinel2_data.nc')\n\n# Select data by labels\nsubset = ds.sel(time='2023-01-01', band='red')\n\n# Perform calculations\nndvi = (ds['nir'] - ds['red']) / (ds['nir'] + ds['red'])\n</code></pre>"},{"location":"#stac","title":"STAC","text":"<p>STAC provides a common language to describe geospatial information, making it easier to discover and access satellite imagery.</p> <pre><code>from pystac_client import Client\n\n# Connect to a STAC catalog\ncatalog = Client.open('https://earth-search.aws.element84.com/v1')\n\n# Search for Sentinel-2 imagery\nsearch = catalog.search(\n    collections=['sentinel-2-l2a'],\n    bbox=[lon_min, lat_min, lon_max, lat_max],\n    datetime='2023-01-01/2023-12-31'\n)\n</code></pre>"},{"location":"#dask","title":"Dask","text":"<p>Dask enables parallel computing in Python, allowing you to work with datasets larger than memory.</p> <pre><code>import dask.array as da\n\n# Create a Dask array\nx = da.from_zarr('large_dataset.zarr')\n\n# Computations are lazy\nresult = x.mean(axis=0)\n\n# Trigger computation\nresult.compute()\n</code></pre>"},{"location":"#zarr","title":"Zarr","text":"<p>Zarr is a cloud-optimized format for storing chunked, compressed N-dimensional arrays.</p> <pre><code>import zarr\n\n# Create a Zarr array\nz = zarr.open('data.zarr', mode='w', shape=(10000, 10000), \n              chunks=(1000, 1000), dtype='f4')\n\n# Write data\nz[:] = data\n\n# Read data\nsubset = z[1000:2000, 1000:2000]\n</code></pre>"},{"location":"#xee","title":"XEE","text":"<p>XEE (XArray Earth Engine Extension) allows you to use Earth Engine datasets with XArray.</p> <pre><code>import xee\nimport ee\n\nee.Initialize(project='spatialgeography')\n\n# Open an Earth Engine ImageCollection as XArray\nds = xr.open_dataset(\n    'ee://COPERNICUS/S2_SR',\n    engine='ee',\n    geometry=ee.Geometry.Point([lon, lat]).buffer(10000),\n    scale=10\n)\n</code></pre>"},{"location":"#prerequisites","title":"\ud83d\udca1 Prerequisites","text":"<ul> <li>Python Programming: Basic knowledge of Python</li> <li>NumPy/Pandas: Familiarity with array operations</li> <li>Remote Sensing: Basic understanding of satellite imagery</li> <li>GIS Concepts: Knowledge of coordinate systems and projections</li> </ul>"},{"location":"#learning-path","title":"\ud83c\udf93 Learning Path","text":"<ol> <li>Beginners: Start with Getting Started \u2192 Fundamentals</li> <li>Intermediate: Focus on Data Processing \u2192 Practical Examples</li> <li>Advanced: Dive into Advanced Topics and optimization</li> </ol>"},{"location":"#key-features","title":"\ud83c\udf1f Key Features","text":"<ul> <li>Hands-on Examples: Every concept includes working code examples</li> <li>Google Colab Integration: Run examples in your browser without setup</li> <li>Real-world Datasets: Work with actual Sentinel-2 and other satellite data</li> <li>Best Practices: Learn industry-standard approaches</li> <li>Scalable Solutions: Techniques that work from local to cloud scale</li> </ul>"},{"location":"#how-to-use-this-course","title":"\ud83d\udcd6 How to Use This Course","text":"<ol> <li>Read sequentially for a complete learning experience</li> <li>Jump to specific topics using the navigation menu</li> <li>Run the code examples in Google Colab or your local environment</li> <li>Complete the exercises to reinforce your learning</li> <li>Explore the reference section for detailed API information</li> </ol>"},{"location":"#contributing","title":"\ud83e\udd1d Contributing","text":"<p>This course is open-source and welcomes contributions! If you find errors, have suggestions, or want to add content, please contribute via GitHub.</p>"},{"location":"#license","title":"\ud83d\udcdd License","text":"<p>This course content is available under the Creative Commons Attribution 4.0 International License.</p> <p>Ready to get started? Head to Introduction to begin your cloud-native remote sensing journey!</p> <ul> <li> <p>:material-rocket-launch:{ .lg .middle } Getting Started</p> <p>Set up your environment and learn the basics of cloud-native remote sensing workflows.</p> <p>:octicons-arrow-right-24: Start Learning</p> </li> <li> <p>:material-book-open-variant:{ .lg .middle } Fundamentals</p> <p>Master XArray, STAC, Dask, Zarr, and XEE - the core tools for cloud-native analysis.</p> <p>:octicons-arrow-right-24: Learn Fundamentals</p> </li> <li> <p>:material-cog:{ .lg .middle } Data Processing</p> <p>Apply techniques for spectral indices, cloud masking, time series, and aggregation.</p> <p>:octicons-arrow-right-24: Process Data</p> </li> <li> <p>:material-rocket:{ .lg .middle } Advanced Topics</p> <p>Scale your analysis with Dask, cloud platforms, and optimization strategies.</p> <p>:octicons-arrow-right-24: Go Advanced</p> </li> <li> <p>:material-code-braces:{ .lg .middle } Practical Examples</p> <p>Complete workflows for NDVI analysis, land cover, change detection, and more.</p> <p>:octicons-arrow-right-24: See Examples</p> </li> <li> <p>:material-book-multiple:{ .lg .middle } Reference</p> <p>Comprehensive API documentation and best practices for all libraries.</p> <p>:octicons-arrow-right-24: Browse Reference</p> </li> </ul>"},{"location":"advanced/cloud-computing/","title":"Cloud Computing","text":""},{"location":"advanced/cloud-computing/#cloud-computing-for-remote-sensing","title":"Cloud Computing for Remote Sensing","text":"<p>Modern remote sensing has shifted from \"download and process\" to \"process in the cloud.\" This page covers the major cloud platforms used in remote sensing and their advantages.</p>"},{"location":"advanced/cloud-computing/#major-cloud-providers","title":"Major Cloud Providers","text":""},{"location":"advanced/cloud-computing/#1-google-cloud-platform-gcp","title":"1. Google Cloud Platform (GCP)","text":"<ul> <li>Data: Google Earth Engine (GEE) Catalog, Cloud Storage.</li> <li>Compute: Compute Engine (VMs), Google Kubernetes Engine (GKE), Vertex AI for Machine Learning.</li> <li>Strength: Seamless integration with Earth Engine and massive public datasets.</li> </ul>"},{"location":"advanced/cloud-computing/#2-amazon-web-services-aws","title":"2. Amazon Web Services (AWS)","text":"<ul> <li>Data: Amazon S3 (hosts the Sentinel-2 and Landsat repositories), Registry of Open Data.</li> <li>Compute: EC2, SageMaker (ML), Lambda (Serverless).</li> <li>Strength: Most cloud-native data (COGs) is natively hosted on AWS S3, enabling ultra-fast range requests.</li> </ul>"},{"location":"advanced/cloud-computing/#3-microsoft-azure","title":"3. Microsoft Azure","text":"<ul> <li>Data: Azure Blob Storage, Planetary Computer.</li> <li>Compute: Virtual Machines, Azure Batch, Azure ML.</li> <li>Strength: The Planetary Computer is a best-in-class STAC-based catalog for open geospatial data.</li> </ul>"},{"location":"advanced/cloud-computing/#serverless-vs-managed-vs-infrastructure","title":"Serverless vs. Managed vs. Infrastructure","text":""},{"location":"advanced/cloud-computing/#infrastructure-as-a-service-iaas","title":"Infrastructure as a Service (IaaS)","text":"<p>Renting Virtual Machines (e.g., AWS EC2, GCP Compute Engine).</p> <ul> <li>Pro: Full control over environment.</li> <li>Con: You manage security updates and scaling.</li> </ul>"},{"location":"advanced/cloud-computing/#managed-platforms-paas","title":"Managed Platforms (PaaS)","text":"<p>Platforms that manage the compute cluster for you (e.g., Google Earth Engine, Microsoft Planetary Computer).</p> <ul> <li>Pro: Focus on science, not infrastructure.</li> <li>Con: Less control over underlying hardware/versions.</li> </ul>"},{"location":"advanced/cloud-computing/#serverless-faas","title":"Serverless (FaaS)","text":"<p>Running functions without managing servers (e.g., AWS Lambda, Google Cloud Functions).</p> <ul> <li>Pro: Pay-per-execution, scales to zero.</li> <li>Con: Execution time limits (usually 15 minutes). Perfect for small STAC queries or metadata processing.</li> </ul>"},{"location":"advanced/cloud-computing/#cloud-native-workflows","title":"Cloud-Native Workflows","text":"<ol> <li>Discovery: Use STAC to find data on the cloud.</li> <li>Access: Use HTTP Range Requests to stream only the pixels you need from COGs or Zarr.</li> <li>Process: Use Dask or server-side reducers (in Earth Engine) to process data in the cloud.</li> <li>Storage: Save results back to cloud storage (S3/GCS/Blob) in Zarr or COG format.</li> </ol>"},{"location":"advanced/cloud-computing/#cost-considerations","title":"Cost Considerations","text":"<p>When working in the cloud, be mindful of:</p> <ul> <li>Compute costs: Active VMs or Dask workers charge per second/minute.</li> <li>Storage costs: Monthly charges for keeping data in S3/Cloud Storage.</li> <li>Data Transfer (Egress): Downloading large amounts of data out of the cloud region to your local machine can be expensive. Always try to process data in the same region where it is stored.</li> </ul>"},{"location":"advanced/geemap-tiled-download/","title":"Geemap Tiled Download","text":""},{"location":"advanced/geemap-tiled-download/#geemap-tiled-download","title":"Geemap Tiled Download","text":""},{"location":"advanced/geemap-tiled-download/#overview","title":"Overview","text":"<p>geemap provides powerful functionality to download large-scale Earth Engine data directly to your local machine without using Earth Engine Compute Units (EECU) for exports. This is achieved through tiled downloading, where the area is split into manageable tiles, processed server-side, and downloaded directly.</p> <p>Key Benefits:</p> <ul> <li>\u2705 No EECU consumption for exports</li> <li>\u2705 Direct download to local folder</li> <li>\u2705 Automatic tiling for large areas</li> <li>\u2705 Automatic merging of tiles</li> <li>\u2705 Progress tracking</li> <li>\u2705 Handles memory limitations</li> </ul>"},{"location":"advanced/geemap-tiled-download/#installation","title":"Installation","text":"<pre><code>%%capture\n!pip install geemap earthengine-api\n</code></pre>"},{"location":"advanced/geemap-tiled-download/#basic-setup","title":"Basic Setup","text":"<pre><code>import ee\nimport geemap\nimport os\n\n# Authenticate and initialize Earth Engine\nee.Authenticate()\nee.Initialize(project='spatialgeography')\n\n# Create output directory\noutput_folder = 'downloads'\nif not os.path.exists(output_folder):\n    os.makedirs(output_folder)\n</code></pre>"},{"location":"advanced/geemap-tiled-download/#simple-tiled-download","title":"Simple Tiled Download","text":""},{"location":"advanced/geemap-tiled-download/#download-sentinel-2-composite","title":"Download Sentinel-2 Composite","text":"<pre><code># Define area of interest\nroi = ee.Geometry.Rectangle([82.5, 27.0, 83.0, 27.5])\n\n# Create Sentinel-2 composite\ns2 = ee.ImageCollection('COPERNICUS/S2_SR') \\\n    .filterBounds(roi) \\\n    .filterDate('2023-01-01', '2023-12-31') \\\n    .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 20)) \\\n    .median() \\\n    .clip(roi)\n\n# Select bands\nimage = s2.select(['B4', 'B3', 'B2', 'B8'])\n\n# Download with automatic tiling\noutput_file = os.path.join(output_folder, 'sentinel2_composite.tif')\n\ngeemap.download_ee_image(\n    image,\n    filename=output_file,\n    region=roi,\n    scale=10,\n    crs='EPSG:4326'\n)\n\nprint(f\"Downloaded: {output_file}\")\n</code></pre>"},{"location":"advanced/geemap-tiled-download/#advanced-tiled-download","title":"Advanced Tiled Download","text":""},{"location":"advanced/geemap-tiled-download/#large-area-download-with-custom-tiling","title":"Large Area Download with Custom Tiling","text":"<pre><code># Define large area (e.g., entire district)\nlarge_roi = ee.Geometry.Rectangle([80.0, 25.0, 85.0, 30.0])\n\n# Create image\nlandsat = ee.ImageCollection('LANDSAT/LC08/C02/T1_L2') \\\n    .filterBounds(large_roi) \\\n    .filterDate('2023-01-01', '2023-12-31') \\\n    .median() \\\n    .clip(large_roi)\n\n# Select and scale bands\nimage = landsat.select(['SR_B4', 'SR_B3', 'SR_B2']) \\\n    .multiply(0.0000275).add(-0.2)\n\n# Download with custom tile size\noutput_file = os.path.join(output_folder, 'landsat_large_area.tif')\n\ngeemap.download_ee_image(\n    image,\n    filename=output_file,\n    region=large_roi,\n    scale=30,\n    crs='EPSG:4326',\n    num_threads=4,  # Parallel downloads\n    max_tile_size=1.0e8,  # 100 MB per tile\n    max_tile_dim=10000  # Max 10000 pixels per dimension\n)\n</code></pre>"},{"location":"advanced/geemap-tiled-download/#download-with-processing","title":"Download with Processing","text":""},{"location":"advanced/geemap-tiled-download/#calculate-and-download-ndvi","title":"Calculate and Download NDVI","text":"<pre><code># Define ROI\nroi = ee.Geometry.Rectangle([82.5, 27.0, 83.5, 28.0])\n\n# Get Sentinel-2 data\ns2 = ee.ImageCollection('COPERNICUS/S2_SR') \\\n    .filterBounds(roi) \\\n    .filterDate('2023-06-01', '2023-08-31') \\\n    .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 20))\n\n# Cloud masking function\ndef mask_s2_clouds(image):\n    qa = image.select('QA60')\n    cloud_mask = qa.bitwiseAnd(1 &lt;&lt; 10).eq(0).And(\n                 qa.bitwiseAnd(1 &lt;&lt; 11).eq(0))\n    return image.updateMask(cloud_mask)\n\n# Apply cloud mask and calculate NDVI\ns2_masked = s2.map(mask_s2_clouds)\nmedian = s2_masked.median()\n\n# Calculate NDVI (processing done server-side)\nndvi = median.normalizedDifference(['B8', 'B4']).rename('NDVI')\n\n# Download NDVI\noutput_file = os.path.join(output_folder, 'ndvi_summer_2023.tif')\n\ngeemap.download_ee_image(\n    ndvi,\n    filename=output_file,\n    region=roi,\n    scale=10,\n    crs='EPSG:4326'\n)\n\nprint(f\"NDVI downloaded: {output_file}\")\n</code></pre>"},{"location":"advanced/geemap-tiled-download/#multi-band-download","title":"Multi-band Download","text":""},{"location":"advanced/geemap-tiled-download/#download-multiple-indices","title":"Download Multiple Indices","text":"<pre><code># Define ROI\nroi = ee.Geometry.Rectangle([82.0, 26.5, 83.0, 27.5])\n\n# Get Sentinel-2 median composite\ns2 = ee.ImageCollection('COPERNICUS/S2_SR') \\\n    .filterBounds(roi) \\\n    .filterDate('2023-01-01', '2023-12-31') \\\n    .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 20)) \\\n    .median()\n\n# Calculate multiple indices (all processing server-side)\nndvi = s2.normalizedDifference(['B8', 'B4']).rename('NDVI')\nndwi = s2.normalizedDifference(['B3', 'B8']).rename('NDWI')\nndbi = s2.normalizedDifference(['B11', 'B8']).rename('NDBI')\n\n# Combine into multi-band image\nindices = ndvi.addBands(ndwi).addBands(ndbi).clip(roi)\n\n# Download all indices as multi-band GeoTIFF\noutput_file = os.path.join(output_folder, 'spectral_indices.tif')\n\ngeemap.download_ee_image(\n    indices,\n    filename=output_file,\n    region=roi,\n    scale=20,\n    crs='EPSG:4326'\n)\n\nprint(f\"Multi-band indices downloaded: {output_file}\")\n</code></pre>"},{"location":"advanced/geemap-tiled-download/#time-series-download","title":"Time Series Download","text":""},{"location":"advanced/geemap-tiled-download/#download-monthly-composites","title":"Download Monthly Composites","text":"<pre><code>import calendar\n\n# Define ROI\nroi = ee.Geometry.Rectangle([82.5, 27.0, 83.0, 27.5])\n\n# Download monthly NDVI for 2023\nfor month in range(1, 13):\n    # Get month start and end\n    start_date = f'2023-{month:02d}-01'\n    if month == 12:\n        end_date = '2024-01-01'\n    else:\n        end_date = f'2023-{month+1:02d}-01'\n\n    # Get Sentinel-2 data\n    s2 = ee.ImageCollection('COPERNICUS/S2_SR') \\\n        .filterBounds(roi) \\\n        .filterDate(start_date, end_date) \\\n        .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 30)) \\\n        .median()\n\n    # Calculate NDVI\n    ndvi = s2.normalizedDifference(['B8', 'B4']).clip(roi)\n\n    # Download\n    month_name = calendar.month_name[month]\n    output_file = os.path.join(output_folder, f'ndvi_{month:02d}_{month_name}.tif')\n\n    try:\n        geemap.download_ee_image(\n            ndvi,\n            filename=output_file,\n            region=roi,\n            scale=20,\n            crs='EPSG:4326'\n        )\n        print(f\"\u2713 Downloaded: {month_name}\")\n    except Exception as e:\n        print(f\"\u2717 Failed {month_name}: {str(e)}\")\n</code></pre>"},{"location":"advanced/geemap-tiled-download/#download-from-featurecollection","title":"Download from FeatureCollection","text":""},{"location":"advanced/geemap-tiled-download/#download-data-for-administrative-boundaries","title":"Download Data for Administrative Boundaries","text":"<pre><code># Load administrative boundaries\ncountries = ee.FeatureCollection('USDOS/LSIB_SIMPLE/2017')\nindia = countries.filter(ee.Filter.eq('country_na', 'India'))\n\n# Get a specific state (example: Uttar Pradesh)\nstates = ee.FeatureCollection('FAO/GAUL/2015/level1')\nstate = states.filter(ee.Filter.eq('ADM1_NAME', 'Uttar Pradesh')).first()\nstate_geom = state.geometry()\n\n# Get MODIS NDVI\nmodis = ee.ImageCollection('MODIS/006/MOD13A2') \\\n    .filterBounds(state_geom) \\\n    .filterDate('2023-01-01', '2023-12-31') \\\n    .select('NDVI') \\\n    .mean() \\\n    .clip(state_geom)\n\n# Download\noutput_file = os.path.join(output_folder, 'uttar_pradesh_ndvi.tif')\n\ngeemap.download_ee_image(\n    modis,\n    filename=output_file,\n    region=state_geom,\n    scale=1000,\n    crs='EPSG:4326'\n)\n</code></pre>"},{"location":"advanced/geemap-tiled-download/#advanced-custom-tiling-strategy","title":"Advanced: Custom Tiling Strategy","text":""},{"location":"advanced/geemap-tiled-download/#manual-tile-control","title":"Manual Tile Control","text":"<pre><code>def download_large_area_custom(image, roi, output_file, tile_size=0.5):\n    \"\"\"\n    Download large area with custom tiling.\n\n    Parameters:\n    -----------\n    image : ee.Image\n        Earth Engine image to download\n    roi : ee.Geometry\n        Region of interest\n    output_file : str\n        Output file path\n    tile_size : float\n        Tile size in degrees\n    \"\"\"\n    import rasterio\n    from rasterio.merge import merge\n    import glob\n\n    # Get bounds\n    bounds = roi.bounds().getInfo()['coordinates'][0]\n    min_lon, min_lat = bounds[0]\n    max_lon, max_lat = bounds[2]\n\n    # Create tiles\n    temp_folder = os.path.join(output_folder, 'temp_tiles')\n    if not os.path.exists(temp_folder):\n        os.makedirs(temp_folder)\n\n    tile_files = []\n    tile_count = 0\n\n    # Iterate over tiles\n    for lon in range(int(min_lon), int(max_lon) + 1):\n        for lat in range(int(min_lat), int(max_lat) + 1):\n            # Create tile geometry\n            tile_roi = ee.Geometry.Rectangle([\n                lon, lat,\n                min(lon + tile_size, max_lon),\n                min(lat + tile_size, max_lat)\n            ])\n\n            # Download tile\n            tile_file = os.path.join(temp_folder, f'tile_{tile_count}.tif')\n\n            try:\n                geemap.download_ee_image(\n                    image,\n                    filename=tile_file,\n                    region=tile_roi,\n                    scale=30,\n                    crs='EPSG:4326'\n                )\n                tile_files.append(tile_file)\n                tile_count += 1\n                print(f\"Downloaded tile {tile_count}\")\n            except Exception as e:\n                print(f\"Failed tile at ({lon}, {lat}): {str(e)}\")\n\n    # Merge tiles\n    print(\"Merging tiles...\")\n    src_files_to_mosaic = []\n    for tile_file in tile_files:\n        src = rasterio.open(tile_file)\n        src_files_to_mosaic.append(src)\n\n    mosaic, out_trans = merge(src_files_to_mosaic)\n\n    # Save merged file\n    out_meta = src_files_to_mosaic[0].meta.copy()\n    out_meta.update({\n        \"driver\": \"GTiff\",\n        \"height\": mosaic.shape[1],\n        \"width\": mosaic.shape[2],\n        \"transform\": out_trans,\n        \"compress\": \"lzw\"\n    })\n\n    with rasterio.open(output_file, \"w\", **out_meta) as dest:\n        dest.write(mosaic)\n\n    # Clean up temporary files\n    for src in src_files_to_mosaic:\n        src.close()\n\n    for tile_file in tile_files:\n        os.remove(tile_file)\n    os.rmdir(temp_folder)\n\n    print(f\"\u2713 Merged file saved: {output_file}\")\n\n# Example usage\nroi = ee.Geometry.Rectangle([80.0, 25.0, 82.0, 27.0])\nlandsat = ee.ImageCollection('LANDSAT/LC08/C02/T1_L2') \\\n    .filterBounds(roi) \\\n    .filterDate('2023-01-01', '2023-12-31') \\\n    .median() \\\n    .clip(roi)\n\noutput_file = os.path.join(output_folder, 'large_area_merged.tif')\ndownload_large_area_custom(landsat.select(['SR_B4', 'SR_B3', 'SR_B2']), \n                           roi, output_file, tile_size=0.5)\n</code></pre>"},{"location":"advanced/geemap-tiled-download/#download-with-visualization-parameters","title":"Download with Visualization Parameters","text":""},{"location":"advanced/geemap-tiled-download/#apply-color-palette-before-download","title":"Apply Color Palette Before Download","text":"<pre><code># Get elevation data\ndem = ee.Image('USGS/SRTMGL1_003')\nroi = ee.Geometry.Rectangle([82.0, 27.0, 83.0, 28.0])\n\n# Clip to ROI\nelevation = dem.clip(roi)\n\n# Apply visualization (server-side)\nvis_params = {\n    'min': 0,\n    'max': 3000,\n    'palette': ['blue', 'green', 'yellow', 'orange', 'red']\n}\n\n# Convert to RGB\nrgb = elevation.visualize(**vis_params)\n\n# Download RGB image\noutput_file = os.path.join(output_folder, 'elevation_colored.tif')\n\ngeemap.download_ee_image(\n    rgb,\n    filename=output_file,\n    region=roi,\n    scale=30,\n    crs='EPSG:4326'\n)\n</code></pre>"},{"location":"advanced/geemap-tiled-download/#batch-download","title":"Batch Download","text":""},{"location":"advanced/geemap-tiled-download/#download-multiple-regions","title":"Download Multiple Regions","text":"<pre><code># Define multiple ROIs\nregions = {\n    'region_1': ee.Geometry.Rectangle([82.0, 27.0, 82.5, 27.5]),\n    'region_2': ee.Geometry.Rectangle([82.5, 27.0, 83.0, 27.5]),\n    'region_3': ee.Geometry.Rectangle([82.0, 26.5, 82.5, 27.0]),\n}\n\n# Get Sentinel-2 composite\ns2 = ee.ImageCollection('COPERNICUS/S2_SR') \\\n    .filterDate('2023-01-01', '2023-12-31') \\\n    .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 20)) \\\n    .median()\n\n# Download each region\nfor region_name, roi in regions.items():\n    image = s2.clip(roi).select(['B4', 'B3', 'B2'])\n    output_file = os.path.join(output_folder, f'{region_name}_s2.tif')\n\n    try:\n        geemap.download_ee_image(\n            image,\n            filename=output_file,\n            region=roi,\n            scale=10,\n            crs='EPSG:4326'\n        )\n        print(f\"\u2713 Downloaded: {region_name}\")\n    except Exception as e:\n        print(f\"\u2717 Failed {region_name}: {str(e)}\")\n</code></pre>"},{"location":"advanced/geemap-tiled-download/#progress-tracking","title":"Progress Tracking","text":""},{"location":"advanced/geemap-tiled-download/#download-with-progress-bar","title":"Download with Progress Bar","text":"<pre><code>from tqdm import tqdm\n\ndef download_with_progress(image, roi, output_file, scale=10):\n    \"\"\"Download with progress tracking.\"\"\"\n\n    # Calculate approximate number of tiles\n    bounds = roi.bounds().getInfo()['coordinates'][0]\n    width = bounds[2][0] - bounds[0][0]\n    height = bounds[2][1] - bounds[0][1]\n\n    # Estimate tiles (rough approximation)\n    tile_size_deg = 0.1  # ~10km\n    n_tiles = int((width / tile_size_deg) * (height / tile_size_deg))\n\n    print(f\"Downloading {n_tiles} estimated tiles...\")\n\n    with tqdm(total=100, desc=\"Downloading\") as pbar:\n        geemap.download_ee_image(\n            image,\n            filename=output_file,\n            region=roi,\n            scale=scale,\n            crs='EPSG:4326'\n        )\n        pbar.update(100)\n\n    print(f\"\u2713 Download complete: {output_file}\")\n\n# Example usage\nroi = ee.Geometry.Rectangle([82.0, 27.0, 83.0, 28.0])\ns2 = ee.ImageCollection('COPERNICUS/S2_SR') \\\n    .filterBounds(roi) \\\n    .filterDate('2023-01-01', '2023-12-31') \\\n    .median() \\\n    .clip(roi)\n\noutput_file = os.path.join(output_folder, 'sentinel2_with_progress.tif')\ndownload_with_progress(s2.select(['B4', 'B3', 'B2']), roi, output_file)\n</code></pre>"},{"location":"advanced/geemap-tiled-download/#best-practices","title":"Best Practices","text":""},{"location":"advanced/geemap-tiled-download/#1-choose-appropriate-scale","title":"1. Choose Appropriate Scale","text":"<pre><code># Sentinel-2: 10m for RGB/NIR, 20m for other bands\nscale = 10\n\n# Landsat: 30m\nscale = 30\n\n# MODIS: 250m, 500m, or 1000m depending on product\nscale = 1000\n</code></pre>"},{"location":"advanced/geemap-tiled-download/#2-limit-area-size","title":"2. Limit Area Size","text":"<pre><code># For high resolution (10m), keep area &lt; 100 km\u00b2\n# For medium resolution (30m), keep area &lt; 500 km\u00b2\n# For coarse resolution (1000m), can handle larger areas\n</code></pre>"},{"location":"advanced/geemap-tiled-download/#3-use-appropriate-crs","title":"3. Use Appropriate CRS","text":"<pre><code># The global standard for geospatial data is geographic\ncrs = 'EPSG:4326'\n</code></pre>"},{"location":"advanced/geemap-tiled-download/#4-handle-errors-gracefully","title":"4. Handle Errors Gracefully","text":"<pre><code>try:\n    geemap.download_ee_image(image, filename=output_file, ...)\nexcept Exception as e:\n    print(f\"Download failed: {str(e)}\")\n    # Retry with smaller tiles or coarser resolution\n</code></pre>"},{"location":"advanced/geemap-tiled-download/#key-takeaways","title":"Key Takeaways","text":"<p>What You Learned</p> <ul> <li>geemap enables direct downloads without EECU usage</li> <li>Automatic tiling handles large areas efficiently</li> <li>Processing happens server-side in Earth Engine</li> <li>Tiles are automatically merged into final GeoTIFF</li> <li>Supports multi-band and time series downloads</li> <li>Custom tiling strategies for maximum control</li> <li>Progress tracking for long downloads</li> <li>No export tasks or Drive storage needed</li> </ul>"},{"location":"advanced/geemap-tiled-download/#next-steps","title":"Next Steps","text":"<p>\u2192 See Advanced Topics for more optimization techniques</p>"},{"location":"advanced/geemap-tiled-download/#additional-resources","title":"Additional Resources","text":"<ul> <li>geemap Documentation</li> <li>geemap Download Examples</li> <li>Earth Engine Data Catalog</li> <li>geemap GitHub</li> </ul>"},{"location":"advanced/optimization/","title":"Optimization Techniques","text":""},{"location":"advanced/optimization/#optimization-techniques","title":"Optimization Techniques","text":"<p>Optimizing your cloud-native remote sensing workflows can significantly reduce computation time, memory usage, and cloud costs.</p>"},{"location":"advanced/optimization/#1-spatial-and-temporal-filtering","title":"1. Spatial and Temporal Filtering","text":"<p>The first rule of optimization is: Load only what you need.</p> <ul> <li>Spatial: Use the tightest possible <code>bbox</code> or <code>geometry</code>. If you're analyzing a city, don't load the whole province.</li> <li>Temporal: Filter date ranges strictly. If you only need summer months, don't load the full year and then filter in XArray; filter at the STAC/GEE API level.</li> <li>Bands: Only load the bands required for your index (e.g., Red and NIR for NDVI).</li> </ul>"},{"location":"advanced/optimization/#2-chunking-optimization","title":"2. Chunking Optimization","text":"<p>As discussed in the Dask section, chunking is crucial.</p> <ul> <li>Align chunks with data: Zarr and COG files have internal tiling. Try to make your Dask chunks a multiple of the file's internal tiles.</li> <li>Minimize shuffling: Avoid operations that require data to move between workers (like global sorting or complex re-gridding) if possible.</li> </ul>"},{"location":"advanced/optimization/#3-projection-and-alignment","title":"3. Projection and Alignment","text":"<p>Re-projecting large datasets is computationally expensive.</p> <ul> <li>Common CRS: Try to work in the native CRS of the data. If you must re-project, do it after spatial/temporal subsetting.</li> <li>Lazy Reprojection: Use <code>odc-stac</code> or <code>stackstac</code> which can handle re-projection lazily during data loading.</li> </ul>"},{"location":"advanced/optimization/#4-lazy-operations-and-persistence","title":"4. Lazy Operations and Persistence","text":"<ul> <li>Don't <code>.compute()</code> early: String multiple operations together (Indices \u2192 Resampling \u2192 Statistics) and run a single <code>.compute()</code> at the end.</li> <li>Use <code>.persist()</code>: If you're going to use a filtered dataset multiple times in the same session, use <code>ds = ds.persist()</code>. This keeps the data in the memory of the Dask workers, preventing it from being re-loaded or re-calculated from the source.</li> </ul>"},{"location":"advanced/optimization/#5-optimized-storage-zarr","title":"5. Optimized Storage (Zarr)","text":"<p>Zarr is significantly faster than COG for time series analysis because it is optimized for \"slicing\" along the time dimension.</p> <ul> <li>Consolidated Metadata: Always use <code>consolidated=True</code> when writing or reading Zarr. This allows Dask to find all chunks with a single HTTP request rather than hundreds.</li> <li>Compression: Use Blosc or Zstd compression to reduce data transfer size.</li> </ul>"},{"location":"advanced/optimization/#6-eecu-optimization-earth-engine","title":"6. EECU Optimization (Earth Engine)","text":"<p>When using XEE/Earth Engine:</p> <ul> <li>Simplify Geometries: Complex polygons with thousands of vertices can slow down filtering. Use <code>geometry.simplify()</code>.</li> <li>Reduce resolution: Use a larger <code>scale</code> (e.g., 30m instead of 10m) for large area overviews.</li> </ul>"},{"location":"advanced/optimization/#checklist-for-high-performance-code","title":"Checklist for High-Performance Code","text":"<ul> <li>[ ] Filtered by BBOX and Date at the API level?</li> <li>[ ] Only required bands selected?</li> <li>[ ] Dask Dashboard shows workers are busy (not idling)?</li> <li>[ ] Memory usage per worker is stable?</li> <li>[ ] No small, fragmented chunks?</li> <li>[ ] Using Zarr for time-series and COG for spatial composites?</li> </ul>"},{"location":"advanced/planetary-computer/","title":"Planetary Computer","text":""},{"location":"advanced/planetary-computer/#microsoft-planetary-computer","title":"Microsoft Planetary Computer","text":"<p>The Microsoft Planetary Computer is a flagship platform for cloud-native remote sensing. It combines a massive STAC-based data catalog with optimized computing resources.</p>"},{"location":"advanced/planetary-computer/#core-components","title":"Core Components","text":""},{"location":"advanced/planetary-computer/#1-data-catalog","title":"1. Data Catalog","text":"<p>The Planetary Computer hosts petabytes of open geospatial data, including:</p> <ul> <li>Sentinel-2 L2A (harmonized)</li> <li>Landsat 8 &amp; 9 (Collection 2)</li> <li>MODIS</li> <li>ERA5 Climate data</li> <li>HLS (Harmonized Landsat Sentinel)</li> </ul> <p>All data is indexed via STAC, making it searchable using <code>pystac-client</code>.</p>"},{"location":"advanced/planetary-computer/#2-hub","title":"2. Hub","text":"<p>A managed JupyterLab environment pre-configured with the geospatial stack (XArray, Dask, Stackstac, Riomente sensing tools).</p>"},{"location":"advanced/planetary-computer/#3-api","title":"3. API","text":"<p>A RESTful STAC API that allows you to search the catalog from any environment (Colab, local, or the Hub).</p>"},{"location":"advanced/planetary-computer/#basic-usage","title":"Basic Usage","text":"<p>To access data from the Planetary Computer, you usually use <code>pystac_client</code> and <code>planetary_computer</code> for signing URLs.</p> <pre><code>import pystac_client\nimport planetary_computer\n\n# Connect to the API\ncatalog = pystac_client.Client.open(\n    \"https://planetarycomputer.microsoft.com/api/stac/v1\",\n    modifier=planetary_computer.sign_inplace,\n)\n\n# Search for Sentinel-2 data\nsearch = catalog.search(\n    collections=[\"sentinel-2-l2a\"],\n    bbox=[12.4, 41.8, 12.5, 41.9],  # Rome coordinates\n    datetime=\"2023-01-01/2023-12-31\",\n)\n\nitems = search.item_collection()\nprint(f\"Found {len(items)} items\")\n</code></pre>"},{"location":"advanced/planetary-computer/#why-use-planetary-computer","title":"Why use Planetary Computer?","text":"<ul> <li>STAC First: Everything is built around the STAC standard.</li> <li>Signed URLs: High-security access to data with \"signing\" that handles authentication automatically for you.</li> <li>Pangeo Stack: Built by and for the Pangeo community, emphasizing XArray and Dask.</li> <li>Free for Research: Most features are free for academic and research use.</li> </ul>"},{"location":"advanced/planetary-computer/#comparison-with-earth-engine","title":"Comparison with Earth Engine","text":"Feature Microsoft Planetary Computer Google Earth Engine Data Engine XArray / Dask (Python-native) Earth Engine Object-based (JavaScript/Python API) API Style STAC / REST Proprietary GEE API Philosophy Open-source ecosystem Integrated SaaS platform Compute Managed Hub / Dask Gateway Earth Engine EECU <p>Both are excellent, but Planetary Computer is often preferred by those who want to use standard Python tools like XArray and Dask directly on the raw data.</p>"},{"location":"advanced/scaling-dask/","title":"Scaling with Dask","text":""},{"location":"advanced/scaling-dask/#scaling-with-dask","title":"Scaling with Dask","text":"<p>When working with large-scale remote sensing data, a single machine's memory and computation power may not be enough. Dask allows you to scale your analysis from a single core on your laptop to a cluster of thousands of machines in the cloud.</p>"},{"location":"advanced/scaling-dask/#why-dask-for-remote-sensing","title":"Why Dask for Remote Sensing?","text":"<ul> <li>Parallelism: Process multiple image tiles or time steps simultaneously.</li> <li>Out-of-core computing: Work with datasets larger than your RAM by processing data in chunks.</li> <li>XArray Integration: XArray uses Dask as its primary engine for parallelized and lazy computation.</li> </ul>"},{"location":"advanced/scaling-dask/#setting-up-dask","title":"Setting Up Dask","text":""},{"location":"advanced/scaling-dask/#local-cluster","title":"Local Cluster","text":"<p>On a single machine, you can use a <code>LocalCluster</code> to utilize all available CPU cores.</p> <pre><code>from dask.distributed import Client, LocalCluster\n\n# Initialize a local cluster\ncluster = LocalCluster()\nclient = Client(cluster)\n\n# View the Dask dashboard link\nprint(f\"Dask Dashboard: {client.dashboard_link}\")\n</code></pre>"},{"location":"advanced/scaling-dask/#cloud-clusters","title":"Cloud Clusters","text":"<p>For massive processing, you can scale to cloud-based clusters like:</p> <ul> <li>Dask Gateway: Managed clusters in Kubernetes.</li> <li>Coiled: Managed Dask as a service.</li> <li>Saturn Cloud: Data science platform with built-in Dask support.</li> </ul>"},{"location":"advanced/scaling-dask/#working-with-chunks","title":"Working with Chunks","text":"<p>The key to Dask performance is choosing the right chunk size. In remote sensing, this usually means chunking along the <code>time</code> dimension or <code>spatial</code> dimensions (X, Y).</p> <pre><code>import xarray as xr\n\n# Load data with automatic chunking\nds = xr.open_zarr('data.zarr', chunks='auto')\n\n# Or specify manual chunks\nds = xr.open_zarr('data.zarr', chunks={'time': 10, 'x': 512, 'y': 512})\n</code></pre> <p>Chunk Size Rules</p> <ul> <li>Too small: High overhead, slow computation.</li> <li>Too large: Memory errors (Dask needs to fit a few chunks in RAM per worker).</li> <li>Ideal: Roughly 100MB - 200MB per chunk.</li> </ul>"},{"location":"advanced/scaling-dask/#lazy-computation","title":"Lazy Computation","text":"<p>Dask operations are \"lazy\" by default. This means they build a task graph but don't execute it until you explicitly request the result.</p> <pre><code># This builds the graph (no computation yet)\nresult = ds.NDVI.mean(dim='time')\n\n# This triggers the parallel computation\nfinal_mean = result.compute()\n</code></pre>"},{"location":"advanced/scaling-dask/#performance-monitoring","title":"Performance Monitoring","text":"<p>Dask provides a powerful dashboard that allows you to see:</p> <ul> <li>Worker memory usage</li> <li>Task progress</li> <li>CPU utilization per core</li> <li>Graph structure and potential bottlenecks</li> </ul> <p>Using the dashboard is essential for optimizing your cloud-native remote sensing workflows.</p>"},{"location":"examples/change-detection/","title":"Change Detection Workflow","text":""},{"location":"examples/change-detection/#change-detection-with-xee","title":"Change Detection with XEE","text":"<p>Detect and quantify land surface changes using multi-temporal Earth Engine data accessed through XEE.</p>"},{"location":"examples/change-detection/#overview","title":"Overview","text":"<p>This example demonstrates:</p> <ul> <li>Pre/post event comparison</li> <li>Change magnitude calculation</li> <li>Statistical significance testing</li> <li>Change visualization and mapping</li> </ul> <p>Scenario: Detecting urban expansion between 2020 and 2023</p>"},{"location":"examples/change-detection/#step-1-initialize-and-define-parameters","title":"Step 1: Initialize and Define Parameters","text":"<pre><code>import ee\nimport xarray as xr\nimport xee\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nimport seaborn as sns\n\n# Initialize Earth Engine\nee.Initialize(project='spatialgeography')\n\n# Define region of interest\nroi = ee.Geometry.Rectangle([77.1, 28.5, 77.3, 28.7])  # Delhi suburbs\n\n# Define time periods\nperiod_before = ('2020-01-01', '2020-03-31')\nperiod_after = ('2023-01-01', '2023-03-31')\n</code></pre>"},{"location":"examples/change-detection/#step-2-load-data-for-both-periods","title":"Step 2: Load Data for Both Periods","text":"<pre><code>def get_composite(start_date, end_date, roi):\n    \"\"\"Create cloud-free composite for a time period.\"\"\"\n\n    def mask_clouds(image):\n        qa = image.select('QA60')\n        cloud_mask = qa.bitwiseAnd(1 &lt;&lt; 10).eq(0).And(\n                     qa.bitwiseAnd(1 &lt;&lt; 11).eq(0))\n        return image.updateMask(cloud_mask)\n\n    collection = ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED') \\\n        .filterBounds(roi) \\\n        .filterDate(start_date, end_date) \\\n        .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 30)) \\\n        .map(mask_clouds)\n\n    return collection.median().clip(roi)\n\n# Create composites\ncomposite_before = get_composite(*period_before, roi)\ncomposite_after = get_composite(*period_after, roi)\n\nprint(\"Composites created\")\n</code></pre>"},{"location":"examples/change-detection/#step-3-load-with-xee","title":"Step 3: Load with XEE","text":"<pre><code># Load both periods\nds_before = xr.open_dataset(\n    composite_before,\n    engine='ee',\n    geometry=roi,\n    scale=20,\n    crs='EPSG:4326'\n).compute()\n\nds_after = xr.open_dataset(\n    composite_after,\n    engine='ee',\n    geometry=roi,\n    scale=20,\n    crs='EPSG:4326'\n).compute()\n\nprint(\"Data loaded\")\nprint(f\"Shape: {ds_before.B4.shape}\")\n</code></pre>"},{"location":"examples/change-detection/#step-4-calculate-indices-for-both-periods","title":"Step 4: Calculate Indices for Both Periods","text":"<pre><code># NDVI (vegetation)\nndvi_before = (ds_before.B8 - ds_before.B4) / (ds_before.B8 + ds_before.B4)\nndvi_after = (ds_after.B8 - ds_after.B4) / (ds_after.B8 + ds_after.B4)\n\n# NDBI (built-up)\nndbi_before = (ds_before.B11 - ds_before.B8) / (ds_before.B11 + ds_before.B8)\nndbi_after = (ds_after.B11 - ds_after.B8) / (ds_after.B11 + ds_after.B8)\n\n# NDWI (water)\nndwi_before = (ds_before.B3 - ds_before.B8) / (ds_before.B3 + ds_before.B8)\nndwi_after = (ds_after.B3 - ds_after.B8) / (ds_after.B3 + ds_after.B8)\n\nprint(\"Indices calculated\")\n</code></pre>"},{"location":"examples/change-detection/#step-5-calculate-change","title":"Step 5: Calculate Change","text":"<pre><code># Calculate differences\nndvi_change = ndvi_after - ndvi_before\nndbi_change = ndbi_after - ndbi_before\nndwi_change = ndwi_after - ndwi_before\n\n# Calculate percentage change\nndvi_pct_change = ((ndvi_after - ndvi_before) / np.abs(ndvi_before)) * 100\nndbi_pct_change = ((ndbi_after - ndbi_before) / np.abs(ndbi_before)) * 100\n\nprint(\"Change calculated\")\n</code></pre>"},{"location":"examples/change-detection/#step-6-visualize-beforeafter","title":"Step 6: Visualize Before/After","text":"<pre><code>fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n\n# NDVI Before\nndvi_before.plot(ax=axes[0, 0], cmap='RdYlGn', vmin=-1, vmax=1)\naxes[0, 0].set_title('NDVI - 2020')\naxes[0, 0].set_axis_off()\n\n# NDVI After\nndvi_after.plot(ax=axes[0, 1], cmap='RdYlGn', vmin=-1, vmax=1)\naxes[0, 1].set_title('NDVI - 2023')\naxes[0, 1].set_axis_off()\n\n# NDVI Change\nim1 = ndvi_change.plot(ax=axes[0, 2], cmap='RdBu', vmin=-0.5, vmax=0.5)\naxes[0, 2].set_title('NDVI Change (2023-2020)')\naxes[0, 2].set_axis_off()\n\n# NDBI Before\nndbi_before.plot(ax=axes[1, 0], cmap='YlOrRd', vmin=-1, vmax=1)\naxes[1, 0].set_title('NDBI - 2020')\naxes[1, 0].set_axis_off()\n\n# NDBI After\nndbi_after.plot(ax=axes[1, 1], cmap='YlOrRd', vmin=-1, vmax=1)\naxes[1, 1].set_title('NDBI - 2023')\naxes[1, 1].set_axis_off()\n\n# NDBI Change\nim2 = ndbi_change.plot(ax=axes[1, 2], cmap='RdBu_r', vmin=-0.5, vmax=0.5)\naxes[1, 2].set_title('NDBI Change (Urban Expansion)')\naxes[1, 2].set_axis_off()\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"examples/change-detection/#step-7-detect-significant-changes","title":"Step 7: Detect Significant Changes","text":"<pre><code># Define thresholds for significant change\nndvi_threshold = 0.2  # Decrease indicates vegetation loss\nndbi_threshold = 0.2  # Increase indicates urbanization\n\n# Detect changes\nvegetation_loss = ndvi_change &lt; -ndvi_threshold\nurban_expansion = ndbi_change &gt; ndbi_threshold\nwater_change = np.abs(ndwi_change) &gt; 0.2\n\n# Combine into change categories\nchange_map = np.zeros_like(ndvi_change.values)\nchange_map[vegetation_loss.values] = 1  # Vegetation loss\nchange_map[urban_expansion.values] = 2  # Urban expansion\nchange_map[water_change.values] = 3     # Water change\n\n# Visualize change categories\nfrom matplotlib.colors import ListedColormap\n\ncolors = ['white', 'red', 'gray', 'blue']\nlabels = ['No Change', 'Vegetation Loss', 'Urban Expansion', 'Water Change']\ncmap = ListedColormap(colors)\n\nfig, ax = plt.subplots(figsize=(12, 10))\nim = ax.imshow(change_map, cmap=cmap, vmin=0, vmax=3)\nax.set_title('Change Detection Map (2020-2023)', fontsize=14)\nax.set_axis_off()\n\n# Add legend\nfrom matplotlib.patches import Patch\nlegend_elements = [Patch(facecolor=colors[i], label=labels[i]) \n                   for i in range(len(labels))]\nax.legend(handles=legend_elements, loc='upper right', fontsize=10)\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"examples/change-detection/#step-8-calculate-change-statistics","title":"Step 8: Calculate Change Statistics","text":"<pre><code># Calculate areas\npixel_area_sqm = 20 * 20  # 20m x 20m\npixel_area_sqkm = pixel_area_sqm / 1e6\n\n# Count pixels for each change type\nstats = {}\nfor i, label in enumerate(labels):\n    pixel_count = np.sum(change_map == i)\n    area_sqkm = pixel_count * pixel_area_sqkm\n    percentage = (pixel_count / change_map.size) * 100\n\n    stats[label] = {\n        'pixels': pixel_count,\n        'area_km2': area_sqkm,\n        'percentage': percentage\n    }\n\n    print(f\"{label}:\")\n    print(f\"  Area: {area_sqkm:.2f} km\u00b2\")\n    print(f\"  Percentage: {percentage:.1f}%\")\n    print()\n\n# Visualize statistics\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Bar chart\nareas = [stats[label]['area_km2'] for label in labels]\naxes[0].bar(labels, areas, color=colors)\naxes[0].set_ylabel('Area (km\u00b2)')\naxes[0].set_title('Change Areas')\naxes[0].tick_params(axis='x', rotation=45)\n\n# Pie chart (excluding no change)\nchange_labels = labels[1:]\nchange_areas = areas[1:]\naxes[1].pie(change_areas, labels=change_labels, colors=colors[1:],\n            autopct='%1.1f%%', startangle=90)\naxes[1].set_title('Distribution of Changes')\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"examples/change-detection/#step-9-temporal-profile-analysis","title":"Step 9: Temporal Profile Analysis","text":"<pre><code># Extract temporal profiles at specific points\npoints_of_interest = [\n    {'name': 'Urban Expansion Site', 'lon': 77.15, 'lat': 28.60},\n    {'name': 'Stable Vegetation', 'lon': 77.25, 'lat': 28.65},\n    {'name': 'Water Body', 'lon': 77.20, 'lat': 28.55}\n]\n\nfig, axes = plt.subplots(len(points_of_interest), 1, figsize=(12, 10))\n\nfor idx, poi in enumerate(points_of_interest):\n    # Extract values at point\n    ndvi_b = ndvi_before.sel(lon=poi['lon'], lat=poi['lat'], method='nearest').values\n    ndvi_a = ndvi_after.sel(lon=poi['lon'], lat=poi['lat'], method='nearest').values\n\n    ndbi_b = ndbi_before.sel(lon=poi['lon'], lat=poi['lat'], method='nearest').values\n    ndbi_a = ndbi_after.sel(lon=poi['lon'], lat=poi['lat'], method='nearest').values\n\n    # Plot\n    x = ['2020', '2023']\n    axes[idx].plot(x, [ndvi_b, ndvi_a], 'o-', label='NDVI', linewidth=2, markersize=8)\n    axes[idx].plot(x, [ndbi_b, ndbi_a], 's-', label='NDBI', linewidth=2, markersize=8)\n    axes[idx].set_title(poi['name'])\n    axes[idx].set_ylabel('Index Value')\n    axes[idx].legend()\n    axes[idx].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"examples/change-detection/#step-10-export-results","title":"Step 10: Export Results","text":"<pre><code># Create change detection dataset\nchange_ds = xr.Dataset({\n    'ndvi_change': (['y', 'x'], ndvi_change.values),\n    'ndbi_change': (['y', 'x'], ndbi_change.values),\n    'change_category': (['y', 'x'], change_map)\n}, coords={\n    'y': ds_before.lat.values,\n    'x': ds_before.lon.values\n})\n\n# Add CRS\nchange_ds = change_ds.rio.write_crs('EPSG:4326')\n\n# Save as NetCDF\nchange_ds.to_netcdf('change_detection_results.nc')\nprint(\"Results saved to change_detection_results.nc\")\n\n# Export change map as GeoTIFF\nchange_da = xr.DataArray(\n    change_map,\n    dims=['y', 'x'],\n    coords={'y': ds_before.lat.values, 'x': ds_before.lon.values}\n)\nchange_da = change_da.rio.write_crs('EPSG:4326')\nchange_da.rio.to_raster('change_map.tif')\nprint(\"Change map saved to change_map.tif\")\n\n# Export statistics\nimport pandas as pd\nstats_df = pd.DataFrame(stats).T\nstats_df.to_csv('change_statistics.csv')\nprint(\"Statistics saved to change_statistics.csv\")\n</code></pre>"},{"location":"examples/change-detection/#advanced-change-trajectory-analysis","title":"Advanced: Change Trajectory Analysis","text":"<pre><code># For more detailed analysis, load monthly data\ndef get_monthly_ndvi(year, roi):\n    \"\"\"Get monthly NDVI composites for a year.\"\"\"\n    monthly_ndvi = []\n\n    for month in range(1, 13):\n        start = f'{year}-{month:02d}-01'\n        if month == 12:\n            end = f'{year+1}-01-01'\n        else:\n            end = f'{year}-{month+1:02d}-01'\n\n        composite = get_composite(start, end, roi)\n        ds = xr.open_dataset(composite, engine='ee', geometry=roi, scale=100, crs='EPSG:4326').compute()\n        ndvi = (ds.B8 - ds.B4) / (ds.B8 + ds.B4)\n        monthly_ndvi.append(ndvi.mean().values)\n\n    return monthly_ndvi\n\n# Get trajectories for both years\nndvi_2020 = get_monthly_ndvi(2020, roi)\nndvi_2023 = get_monthly_ndvi(2023, roi)\n\n# Plot trajectory comparison\nmonths = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', \n          'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n\nplt.figure(figsize=(12, 6))\nplt.plot(months, ndvi_2020, 'o-', label='2020', linewidth=2, markersize=8)\nplt.plot(months, ndvi_2023, 's-', label='2023', linewidth=2, markersize=8)\nplt.xlabel('Month')\nplt.ylabel('Mean NDVI')\nplt.title('Annual NDVI Trajectory Comparison')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"examples/change-detection/#key-takeaways","title":"Key Takeaways","text":"<p>What You Learned</p> <ul> <li>Creating multi-temporal composites with XEE</li> <li>Calculating change in multiple indices</li> <li>Detecting significant changes with thresholds</li> <li>Categorizing different types of change</li> <li>Visualizing before/after comparisons</li> <li>Calculating change statistics and areas</li> <li>Analyzing temporal trajectories</li> <li>Exporting change detection results</li> </ul>"},{"location":"examples/change-detection/#next-steps","title":"Next Steps","text":"<p>\u2192 Continue to Time Series and Phenological Methods</p>"},{"location":"examples/change-detection/#additional-resources","title":"Additional Resources","text":"<ul> <li>Change Detection Methods</li> <li>Earth Engine Change Detection</li> </ul>"},{"location":"examples/classification-methods/","title":"3. Classification Methods","text":""},{"location":"examples/classification-methods/#classification-methods","title":"Classification Methods","text":"<p>Implement supervised learning for land cover mapping using XEE, Scikit-Learn, and GeoAI techniques.</p>"},{"location":"examples/classification-methods/#overview","title":"Overview","text":"<p>This example covers:</p> <ol> <li>Machine Learning Algorithms: Random Forest and Gradient Boosting (XGBoost).</li> <li>Statistical Classifiers: Minimum Distance and Maximum Likelihood concepts.</li> <li>Evaluation Metrics: Confusion Matrix, Kappa, and OA.</li> <li>Explainable GeoAI (X-GeoAI): Using feature importance to interpret model decisions.</li> </ol>"},{"location":"examples/classification-methods/#step-1-data-preparation-and-label-loading","title":"Step 1: Data Preparation and Label Loading","text":"<pre><code>import ee\nimport xarray as xr\nimport xee\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Initialize Earth Engine\nee.Initialize(project='spatialgeography')\n\n# Define ROI\nroi = ee.Geometry.Point([77.1025, 28.7041]).buffer(5000).bounds()\n\n# Sentinel-2 Composite\ns2 = ee.ImageCollection(\"COPERNICUS/S2_SR_HARMONIZED\") \\\n    .filterBounds(roi) \\\n    .filterDate('2023-01-01', '2023-03-31') \\\n    .median().clip(roi)\n\nds = xr.open_dataset(s2, engine='ee', geometry=roi, scale=10).compute()\n</code></pre>"},{"location":"examples/classification-methods/#step-2-sampling-and-training-data","title":"Step 2: Sampling and Training Data","text":"<p>In a real scenario, you would load a shapefile. For this example, we generate dummy training points within the ROI.</p> <pre><code># Classes: 1: Water, 2: Forest, 3: Urban\n# (In practice, use ee.FeatureCollection)\ndef get_training_data(dataset):\n    # Flatten dataset for sampling\n    df = dataset[['B2', 'B3', 'B4', 'B8', 'B11', 'B12']].to_dataframe().dropna()\n\n    # Simple rule-based labeling for this demonstration (Synthetic Training)\n    df['label'] = 3 # Default Urban\n    df.loc[(df.B8 - df.B4) / (df.B8 + df.B4) &gt; 0.5, 'label'] = 2 # Forest\n    df.loc[(df.B3 - df.B8) / (df.B3 + ds.B8) &gt; 0.1, 'label'] = 1 # Water\n\n    return df.sample(2000)\n\ntrain_df = get_training_data(ds)\nX_train = train_df[['B2', 'B3', 'B4', 'B8', 'B11', 'B12']]\ny_train = train_df['label']\n</code></pre>"},{"location":"examples/classification-methods/#step-3-random-forest-classification","title":"Step 3: Random Forest Classification","text":"<pre><code>rf = RandomForestClassifier(n_estimators=100, random_state=42)\nrf.fit(X_train, y_train)\n\n# Predict on the full xarray dataset\n# We can use xr.apply_ufunc for efficient prediction\ndef predict_rf(b2, b3, b4, b8, b11, b12):\n    # Flatten bands and stack\n    input_data = np.stack([b2.ravel(), b3.ravel(), b4.ravel(), b8.ravel(), b11.ravel(), b12.ravel()], axis=-1)\n\n    # Handle NaNs\n    mask = ~np.isnan(input_data).any(axis=1)\n    preds = np.full(b2.size, 0)\n    preds[mask] = rf.predict(input_data[mask])\n\n    return preds.reshape(b2.shape)\n\nclassification = xr.apply_ufunc(\n    predict_rf, ds.B2, ds.B3, ds.B4, ds.B8, ds.B11, ds.B12,\n    dask='parallelized', output_dtypes=[np.int32]\n)\n\nplt.figure(figsize=(10, 10))\nclassification.plot(cmap='viridis')\nplt.title(\"Random Forest Classification\")\nplt.show()\n</code></pre>"},{"location":"examples/classification-methods/#step-4-explainable-geoai-feature-importance","title":"Step 4: Explainable GeoAI (Feature Importance)","text":"<p>Understanding which bands contribute most to the classification.</p> <pre><code>importances = rf.feature_importances_\nfeatures = ['Blue', 'Green', 'Red', 'NIR', 'SWIR1', 'SWIR2']\n\nplt.figure(figsize=(10, 6))\nsns.barplot(x=importances, y=features)\nplt.title(\"X-GeoAI: Feature Importance\")\nplt.show()\n</code></pre>"},{"location":"examples/classification-methods/#step-5-accuracy-assessment","title":"Step 5: Accuracy Assessment","text":"<pre><code># Assuming we have independent test data\ntest_df = get_training_data(ds) # Sample again for test\ny_pred = rf.predict(test_df[['B2', 'B3', 'B4', 'B8', 'B11', 'B12']])\n\nprint(classification_report(test_df['label'], y_pred))\n\n# Confusion Matrix\ncm = confusion_matrix(test_df['label'], y_pred)\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\nplt.title(\"Confusion Matrix\")\nplt.show()\n</code></pre>"},{"location":"examples/classification-methods/#key-takeaways","title":"Key Takeaways","text":"<p>Summary</p> <ul> <li>Ensemble Models: Random Forest is robust for most RS tasks.</li> <li>X-GeoAI: Visualizing feature importance helps build trust in \"black-box\" models.</li> <li>Sampling: Stratified sampling is crucial for unbiased accuracy.</li> </ul> <p>\u2192 Next: MCDM Methods</p>"},{"location":"examples/clustering-methods/","title":"2. Clustering Methods in Geospatial Analysis","text":""},{"location":"examples/clustering-methods/#clustering-methods-in-geospatial-analysis","title":"Clustering Methods in Geospatial Analysis","text":"<p>Explore unsupervised learning techniques for grouping spatial data using XEE, Scikit-Learn, and Dask.</p>"},{"location":"examples/clustering-methods/#overview","title":"Overview","text":"<p>This example covers:</p> <ol> <li>Statistical Clustering: K-Means.</li> <li>Machine Learning Clustering: DBSCAN and Gaussian Mixture Models (GMM).</li> <li>Deep Learning-derived Clustering: Using Autoencoders for dimensionality reduction before clustering.</li> <li>Spatially Constrained Clustering: Regionalization techniques.</li> <li>Evaluation Metrics: Silhouette and Calinski-Harabasz.</li> </ol>"},{"location":"examples/clustering-methods/#step-1-load-multi-spectral-data","title":"Step 1: Load Multi-Spectral Data","text":"<pre><code>import ee\nimport xarray as xr\nimport xee\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans, DBSCAN\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import silhouette_score\n\n# Initialize Earth Engine\nee.Initialize(project='spatialgeography')\n\n# Load Sentinel-2 median composite\nroi = ee.Geometry.Rectangle([77.0, 28.5, 77.2, 28.7])\nimage = ee.ImageCollection(\"COPERNICUS/S2_SR_HARMONIZED\") \\\n    .filterBounds(roi) \\\n    .filterDate('2023-01-01', '2023-03-31') \\\n    .median().clip(roi)\n\n# Load into XArray\nds = xr.open_dataset(image, engine='ee', geometry=roi, scale=20)\nds = ds.compute()\n</code></pre>"},{"location":"examples/clustering-methods/#step-2-feature-engineering-and-scaling","title":"Step 2: Feature Engineering and Scaling","text":"<pre><code># Create a feature stack\ndata = np.stack([\n    ds.B2, ds.B3, ds.B4, ds.B8, ds.B11,\n    (ds.B8 - ds.B4) / (ds.B8 + ds.B4) # NDVI\n], axis=-1)\n\n# Flatten for clustering\nrows, cols, bands = data.shape\nX = data.reshape(-1, bands)\n\n# Remove NaNs\nmask = ~np.isnan(X).any(axis=1)\nX_clean = X[mask]\n\n# Scale\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X_clean)\n</code></pre>"},{"location":"examples/clustering-methods/#step-3-traditional-statistical-clustering-k-means","title":"Step 3: Traditional &amp; Statistical Clustering (K-Means)","text":"<pre><code>kmeans = KMeans(n_clusters=5, random_state=42, n_init=10)\nlabels_km = kmeans.fit_predict(X_scaled)\n\n# Reshape back to image\nresult_km = np.full((rows * cols), np.nan)\nresult_km[mask] = labels_km\nresult_km = result_km.reshape(rows, cols)\n\nplt.figure(figsize=(8, 8))\nplt.imshow(result_km, cmap='terrain')\nplt.title('K-Means Clustering (k=5)')\nplt.show()\n</code></pre>"},{"location":"examples/clustering-methods/#step-4-machine-learning-based-clustering-gmm","title":"Step 4: Machine Learning Based Clustering (GMM)","text":"<p>Gaussian Mixture Models provide probabilistic cluster assignments.</p> <pre><code>gmm = GaussianMixture(n_components=5, random_state=42)\nlabels_gmm = gmm.fit_predict(X_scaled)\n\nresult_gmm = np.full((rows * cols), np.nan)\nresult_gmm[mask] = labels_gmm\nresult_gmm = result_gmm.reshape(rows, cols)\n\nplt.figure(figsize=(8, 8))\nplt.imshow(result_gmm, cmap='terrain')\nplt.title('Gaussian Mixture Model')\nplt.show()\n</code></pre>"},{"location":"examples/clustering-methods/#step-5-spatially-constrained-clustering-skater","title":"Step 5: Spatially Constrained Clustering (SKATER)","text":"<p>In geospatial analysis, we often want to ensure that clusters are spatially contiguous. The SKATER (Spatial K'luster Analysis by Tree Edge Removal) algorithm from the <code>spopt</code> library is the scientific standard for this.</p> <pre><code>from libpysal.weights import lat2W\nfrom spopt.region import Skater\n\n# 1. Create a spatial weights matrix (contiguity)\nw = lat2W(rows, cols)\n\n# 2. Initialize and fit SKATER\n# n_clusters=8, floor=None\nmodel = Skater(X_scaled, w, n_clusters=8)\nmodel.solve()\n\n# 3. Reshape labels back to image\nresult_skater = np.full((rows * cols), np.nan)\nresult_skater[mask] = model.labels_\nresult_skater = result_skater.reshape(rows, cols)\n\nplt.figure(figsize=(8, 8))\nplt.imshow(result_skater, cmap='tab20')\nplt.title('SKATER Spatially Constrained Clustering')\nplt.show()\n</code></pre>"},{"location":"examples/clustering-methods/#step-6-evaluation-metrics","title":"Step 6: Evaluation Metrics","text":"<pre><code># Silhouette Score (Computationally intensive, use sample)\nsample_idx = np.random.choice(len(X_scaled), 5000)\nscore = silhouette_score(X_scaled[sample_idx], labels_km[sample_idx])\nprint(f\"Silhouette Score (K-Means): {score:.3f}\")\n</code></pre>"},{"location":"examples/clustering-methods/#key-takeaways","title":"Key Takeaways","text":"<p>Summary</p> <ul> <li>Feature Selection: Adding NDVI or texture improves cluster separation.</li> <li>Scaling: Essential for distance-based methods like K-Means.</li> <li>Spatial Constraints: Helps in creating regions rather than just spectral groupings.</li> <li>Evaluation: Use scores to determine the \"elbow\" for optimal cluster count.</li> </ul> <p>\u2192 Next: Classification Methods</p>"},{"location":"examples/complete-timeseries-workflow/","title":"Complete Time Series Workflow","text":""},{"location":"examples/complete-timeseries-workflow/#complete-time-series-workflow","title":"Complete Time Series Workflow","text":""},{"location":"examples/complete-timeseries-workflow/#overview","title":"Overview","text":"<p>This practical example demonstrates a complete end-to-end workflow for time series analysis using cloud-native tools. We'll cover three approaches:</p> <ol> <li>Geemap Tiled Download \u2192 Read with XArray</li> <li>Direct XEE Approach \u2192 Stream from Earth Engine</li> <li>Dask + Zarr \u2192 Scalable time series processing</li> </ol>"},{"location":"examples/complete-timeseries-workflow/#scenario","title":"Scenario","text":"<p>Objective: Analyze NDVI time series for a region to detect vegetation changes over 2023.</p> <p>Area: Agricultural region in Uttar Pradesh, India Data: Sentinel-2 monthly composites Output: Time series analysis with trend detection</p>"},{"location":"examples/complete-timeseries-workflow/#approach-1-geemap-tiled-download-xarray","title":"Approach 1: Geemap Tiled Download + XArray","text":""},{"location":"examples/complete-timeseries-workflow/#step-1-download-monthly-composites-with-geemap","title":"Step 1: Download Monthly Composites with Geemap","text":"<pre><code>import ee\nimport geemap\nimport os\nimport calendar\nimport numpy as np\nimport xarray as xr\nimport rioxarray as rxr\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\n\n# Initialize Earth Engine\nee.Authenticate()\nee.Initialize(project='spatialgeography')\n\n# Create output directory\noutput_folder = 'timeseries_data'\nif not os.path.exists(output_folder):\n    os.makedirs(output_folder)\n\n# Define region of interest\nroi = ee.Geometry.Rectangle([82.0, 26.5, 82.5, 27.0])\n\n# Download monthly NDVI composites for 2023\nprint(\"Downloading monthly NDVI composites...\")\n\nmonthly_files = []\n\nfor month in range(1, 13):\n    # Define date range\n    start_date = f'2023-{month:02d}-01'\n    if month == 12:\n        end_date = '2024-01-01'\n    else:\n        end_date = f'2023-{month+1:02d}-01'\n\n    # Get Sentinel-2 data\n    s2 = ee.ImageCollection('COPERNICUS/S2_SR') \\\n        .filterBounds(roi) \\\n        .filterDate(start_date, end_date) \\\n        .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 30))\n\n    # Cloud masking function\n    def mask_clouds(image):\n        qa = image.select('QA60')\n        cloud_mask = qa.bitwiseAnd(1 &lt;&lt; 10).eq(0).And(\n                     qa.bitwiseAnd(1 &lt;&lt; 11).eq(0))\n        return image.updateMask(cloud_mask)\n\n    # Apply cloud mask and create median composite\n    s2_masked = s2.map(mask_clouds)\n    median = s2_masked.median()\n\n    # Calculate NDVI (processing done server-side in Earth Engine)\n    ndvi = median.normalizedDifference(['B8', 'B4']).clip(roi)\n\n    # Download using geemap (automatic tiling, no EECU usage)\n    month_name = calendar.month_name[month]\n    output_file = os.path.join(output_folder, f'ndvi_2023_{month:02d}.tif')\n\n    try:\n        geemap.download_ee_image(\n            ndvi,\n            filename=output_file,\n            region=roi,\n            scale=20,  # 20m resolution\n            crs='EPSG:4326',\n            num_threads=4  # Parallel download\n        )\n        monthly_files.append(output_file)\n        print(f\"\u2713 Downloaded: {month_name}\")\n    except Exception as e:\n        print(f\"\u2717 Failed {month_name}: {str(e)}\")\n\nprint(f\"\\nTotal files downloaded: {len(monthly_files)}\")\n</code></pre>"},{"location":"examples/complete-timeseries-workflow/#step-2-read-downloaded-files-with-xarray","title":"Step 2: Read Downloaded Files with XArray","text":"<pre><code># Read all monthly files into XArray Dataset\nprint(\"\\nReading files with XArray...\")\n\n# Read first file to get metadata\nfirst_raster = rxr.open_rasterio(monthly_files[0], masked=True)\n\n# Create list to store all monthly data\nmonthly_data = []\ntime_coords = []\n\nfor i, file in enumerate(monthly_files):\n    # Read raster\n    raster = rxr.open_rasterio(file, masked=True)\n\n    # Extract data (remove band dimension as we only have NDVI)\n    data = raster.squeeze('band', drop=True)\n\n    # Add to list\n    monthly_data.append(data)\n\n    # Create time coordinate (middle of month)\n    month = i + 1\n    time_coords.append(datetime(2023, month, 15))\n\n# Stack along time dimension\nndvi_ts = xr.concat(monthly_data, dim='time')\nndvi_ts = ndvi_ts.assign_coords(time=time_coords)\nndvi_ts.name = 'NDVI'\n\nprint(ndvi_ts)\n</code></pre>"},{"location":"examples/complete-timeseries-workflow/#step-3-analyze-time-series-with-xarray","title":"Step 3: Analyze Time Series with XArray","text":"<pre><code># Calculate statistics\nprint(\"\\nCalculating statistics...\")\n\n# Temporal statistics\nndvi_mean = ndvi_ts.mean(dim='time')\nndvi_std = ndvi_ts.std(dim='time')\nndvi_max = ndvi_ts.max(dim='time')\nndvi_min = ndvi_ts.min(dim='time')\n\n# Spatial mean time series\nndvi_spatial_mean = ndvi_ts.mean(dim=['x', 'y'])\n\n# Calculate trend using linear regression\nfrom scipy import stats\n\ndef calculate_trend(data):\n    \"\"\"Calculate linear trend.\"\"\"\n    x = np.arange(len(data))\n    mask = ~np.isnan(data)\n    if mask.sum() &lt; 3:  # Need at least 3 points\n        return np.nan\n    slope, intercept, r_value, p_value, std_err = stats.linregress(x[mask], data[mask])\n    return slope\n\n# Apply trend calculation\ntrend = xr.apply_ufunc(\n    calculate_trend,\n    ndvi_ts,\n    input_core_dims=[['time']],\n    vectorize=True\n)\n\nprint(f\"Mean NDVI: {ndvi_mean.mean().values:.3f}\")\nprint(f\"NDVI Std Dev: {ndvi_std.mean().values:.3f}\")\n</code></pre>"},{"location":"examples/complete-timeseries-workflow/#step-4-visualize-results","title":"Step 4: Visualize Results","text":"<pre><code># Create comprehensive visualization\nfig, axes = plt.subplots(2, 3, figsize=(18, 12))\n\n# 1. Mean NDVI\nndvi_mean.plot(ax=axes[0, 0], cmap='RdYlGn', vmin=-1, vmax=1)\naxes[0, 0].set_title('Mean NDVI (2023)')\naxes[0, 0].set_axis_off()\n\n# 2. Standard Deviation\nndvi_std.plot(ax=axes[0, 1], cmap='YlOrRd')\naxes[0, 1].set_title('NDVI Standard Deviation')\naxes[0, 1].set_axis_off()\n\n# 3. Trend\ntrend.plot(ax=axes[0, 2], cmap='RdBu_r', center=0)\naxes[0, 2].set_title('NDVI Trend (slope)')\naxes[0, 2].set_axis_off()\n\n# 4. Time series plot\nndvi_spatial_mean.plot(ax=axes[1, 0], marker='o')\naxes[1, 0].set_title('Spatial Mean NDVI Time Series')\naxes[1, 0].set_ylabel('NDVI')\naxes[1, 0].grid(True, alpha=0.3)\n\n# 5. Seasonal comparison (Jan vs Jul)\nndvi_ts.isel(time=0).plot(ax=axes[1, 1], cmap='RdYlGn', vmin=-1, vmax=1)\naxes[1, 1].set_title('January NDVI')\naxes[1, 1].set_axis_off()\n\nndvi_ts.isel(time=6).plot(ax=axes[1, 2], cmap='RdYlGn', vmin=-1, vmax=1)\naxes[1, 2].set_title('July NDVI')\naxes[1, 2].set_axis_off()\n\nplt.tight_layout()\nplt.savefig(os.path.join(output_folder, 'ndvi_analysis.png'), dpi=300, bbox_inches='tight')\nplt.show()\n\nprint(f\"\\nAnalysis complete! Results saved to {output_folder}/\")\n</code></pre>"},{"location":"examples/complete-timeseries-workflow/#approach-2-direct-xee-approach-streaming","title":"Approach 2: Direct XEE Approach (Streaming)","text":""},{"location":"examples/complete-timeseries-workflow/#stream-time-series-directly-from-earth-engine","title":"Stream Time Series Directly from Earth Engine","text":"<pre><code>import xarray as xr\nimport xee\nimport ee\n\n# Initialize Earth Engine\nee.Initialize(project='spatialgeography')\n\n# Define region and time range\nroi = ee.Geometry.Rectangle([82.0, 26.5, 82.5, 27.0])\n\n# Get Sentinel-2 ImageCollection\ns2_collection = ee.ImageCollection('COPERNICUS/S2_SR') \\\n    .filterBounds(roi) \\\n    .filterDate('2023-01-01', '2023-12-31') \\\n    .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 30))\n\n# Cloud masking\ndef mask_s2_clouds(image):\n    qa = image.select('QA60')\n    cloud_mask = qa.bitwiseAnd(1 &lt;&lt; 10).eq(0).And(\n                 qa.bitwiseAnd(1 &lt;&lt; 11).eq(0))\n    return image.updateMask(cloud_mask)\n\n# Apply cloud mask\ns2_masked = s2_collection.map(mask_s2_clouds)\n\n# Open as XArray Dataset using XEE (streaming, no download!)\nprint(\"Opening Earth Engine data with XEE...\")\n\nds_xee = xr.open_dataset(\n    s2_masked,\n    engine='ee',\n    geometry=roi,\n    scale=20,\n    crs='EPSG:4326',\n    ee_mask_value=-9999\n)\n\n# IMPORTANT: Sort by time immediately for resampling to work\nds_xee = ds_xee.sortby('time')\nprint(ds_xee)\n\n# Calculate NDVI directly on the streamed data\nprint(\"\\nCalculating NDVI...\")\nndvi_xee = (ds_xee.B8 - ds_xee.B4) / (ds_xee.B8 + ds_xee.B4)\n\n# Resample to monthly\nprint(\"Resampling to monthly...\")\nndvi_monthly_xee = ndvi_xee.resample(time='1M').median()\n\n# Calculate spatial mean\n# XEE dimensions are usually lon/lat or X/Y depending on CRS\nspatial_dims = [d for d in list(ds_xee.dims) if d not in ['time', 'band']]\nndvi_ts_xee = ndvi_monthly_xee.mean(dim=spatial_dims)\n\n# Plot comparison\nfig, axes = plt.subplots(1, 2, figsize=(15, 5))\n\n# XEE approach\nndvi_ts_xee.plot(ax=axes[0], marker='o', label='XEE (Streaming)')\naxes[0].set_title('NDVI Time Series - XEE Approach')\naxes[0].set_ylabel('NDVI')\naxes[0].grid(True, alpha=0.3)\naxes[0].legend()\n\n# Downloaded approach (from previous)\nndvi_spatial_mean.plot(ax=axes[1], marker='s', label='Geemap (Downloaded)')\naxes[1].set_title('NDVI Time Series - Geemap Approach')\naxes[1].set_ylabel('NDVI')\naxes[1].grid(True, alpha=0.3)\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n\u2713 XEE streaming approach complete!\")\n</code></pre>"},{"location":"examples/complete-timeseries-workflow/#approach-3-dask-zarr-for-scalable-processing","title":"Approach 3: Dask + Zarr for Scalable Processing","text":""},{"location":"examples/complete-timeseries-workflow/#step-1-load-data-with-dask","title":"Step 1: Load Data with Dask","text":"<pre><code>from dask.distributed import Client, LocalCluster\nimport dask.array as da\n\n# Start Dask cluster\nprint(\"Starting Dask cluster...\")\ncluster = LocalCluster(n_workers=4, threads_per_worker=2, memory_limit='2GB')\nclient = Client(cluster)\nprint(client)\n\n# Load downloaded files with Dask chunking\nprint(\"\\nLoading data with Dask...\")\n\n# Read files with chunking\nmonthly_data_dask = []\ntime_coords = []\n\nfor i, file in enumerate(monthly_files):\n    # Read with rioxarray and chunk\n    raster = rxr.open_rasterio(file, masked=True, chunks={'x': 256, 'y': 256})\n    data = raster.squeeze('band', drop=True)\n    monthly_data_dask.append(data)\n\n    month = i + 1\n    time_coords.append(datetime(2023, month, 15))\n\n# Stack with Dask\nndvi_dask = xr.concat(monthly_data_dask, dim='time')\nndvi_dask = ndvi_dask.assign_coords(time=time_coords)\nndvi_dask = ndvi_dask.chunk({'time': 3, 'x': 256, 'y': 256})\nndvi_dask.name = 'NDVI'\n\nprint(ndvi_dask)\nprint(f\"\\nDask chunks: {ndvi_dask.chunks}\")\n</code></pre>"},{"location":"examples/complete-timeseries-workflow/#step-2-save-to-zarr-cloud-optimized","title":"Step 2: Save to Zarr (Cloud-Optimized)","text":"<pre><code>import zarr\nfrom numcodecs import Blosc\n\n# Configure compression\ncompressor = Blosc(cname='zstd', clevel=3, shuffle=Blosc.SHUFFLE)\n\n# Save to Zarr\nzarr_path = os.path.join(output_folder, 'ndvi_timeseries.zarr')\n\nprint(f\"\\nSaving to Zarr: {zarr_path}\")\n\n# Convert to dataset and save\nndvi_dataset = ndvi_dask.to_dataset()\n\n# Add metadata\nndvi_dataset.attrs['title'] = 'NDVI Time Series 2023'\nndvi_dataset.attrs['source'] = 'Sentinel-2 SR'\nndvi_dataset.attrs['region'] = 'Uttar Pradesh, India'\nndvi_dataset.attrs['resolution'] = '20m'\n\n# Save with compression\nencoding = {\n    'NDVI': {\n        'compressor': compressor,\n        'chunks': (3, 256, 256)\n    }\n}\n\nndvi_dataset.to_zarr(\n    zarr_path,\n    mode='w',\n    encoding=encoding,\n    consolidated=True\n)\n\nprint(\"\u2713 Saved to Zarr!\")\n\n# Check file size\nimport shutil\nzarr_size = sum(f.stat().st_size for f in Path(zarr_path).rglob('*') if f.is_file())\nprint(f\"Zarr archive size: {zarr_size / 1e6:.2f} MB\")\n</code></pre>"},{"location":"examples/complete-timeseries-workflow/#step-3-load-from-zarr-and-process-with-dask","title":"Step 3: Load from Zarr and Process with Dask","text":"<pre><code># Load from Zarr (instant, lazy loading)\nprint(\"\\nLoading from Zarr...\")\nndvi_from_zarr = xr.open_zarr(zarr_path, consolidated=True)\n\nprint(ndvi_from_zarr)\n\n# Perform computations with Dask\nprint(\"\\nPerforming Dask computations...\")\n\n# 1. Calculate anomalies\nclimatology = ndvi_from_zarr.NDVI.mean(dim='time')\nanomalies = ndvi_from_zarr.NDVI - climatology\n\n# 2. Calculate rolling mean (smoothing)\nrolling_mean = ndvi_from_zarr.NDVI.rolling(time=3, center=True).mean()\n\n# 3. Calculate percentiles\npercentile_10 = ndvi_from_zarr.NDVI.quantile(0.1, dim='time')\npercentile_90 = ndvi_from_zarr.NDVI.quantile(0.9, dim='time')\n\n# Compute all at once (parallel with Dask)\nprint(\"Computing results...\")\nresults = xr.Dataset({\n    'anomalies': anomalies,\n    'rolling_mean': rolling_mean,\n    'p10': percentile_10,\n    'p90': percentile_90\n})\n\n# Trigger computation\nresults_computed = results.compute()\n\nprint(\"\u2713 Computations complete!\")\n</code></pre>"},{"location":"examples/complete-timeseries-workflow/#step-4-advanced-time-series-analysis","title":"Step 4: Advanced Time Series Analysis","text":"<pre><code># Seasonal decomposition\nprint(\"\\nPerforming seasonal analysis...\")\n\n# Group by season\nseasonal_mean = ndvi_from_zarr.NDVI.groupby('time.season').mean()\n\n# Monthly statistics\nmonthly_stats = ndvi_from_zarr.NDVI.groupby('time.month').agg(['mean', 'std', 'min', 'max'])\n\n# Compute\nseasonal_computed = seasonal_mean.compute()\nmonthly_stats_computed = monthly_stats.compute()\n\n# Visualize seasonal patterns\nfig, axes = plt.subplots(2, 2, figsize=(15, 12))\n\n# Plot each season\nseasons = ['DJF', 'MAM', 'JJA', 'SON']\nfor idx, season in enumerate(seasons):\n    ax = axes[idx // 2, idx % 2]\n    if season in seasonal_computed.season:\n        seasonal_computed.sel(season=season).plot(\n            ax=ax, cmap='RdYlGn', vmin=-1, vmax=1\n        )\n        ax.set_title(f'{season} Mean NDVI')\n        ax.set_axis_off()\n\nplt.tight_layout()\nplt.savefig(os.path.join(output_folder, 'seasonal_ndvi.png'), dpi=300)\nplt.show()\n</code></pre>"},{"location":"examples/complete-timeseries-workflow/#step-5-export-results","title":"Step 5: Export Results","text":"<pre><code># Save processed results to Zarr\nresults_path = os.path.join(output_folder, 'ndvi_analysis_results.zarr')\n\nprint(f\"\\nSaving analysis results to: {results_path}\")\n\nresults_computed.to_zarr(\n    results_path,\n    mode='w',\n    consolidated=True\n)\n\n# Also save as NetCDF for compatibility\nnetcdf_path = os.path.join(output_folder, 'ndvi_timeseries.nc')\nndvi_from_zarr.to_netcdf(netcdf_path)\n\nprint(f\"\u2713 Saved to NetCDF: {netcdf_path}\")\n\n# Close Dask client\nclient.close()\ncluster.close()\n\nprint(\"\\n\u2713 All processing complete!\")\n</code></pre>"},{"location":"examples/complete-timeseries-workflow/#complete-workflow-comparison","title":"Complete Workflow Comparison","text":""},{"location":"examples/complete-timeseries-workflow/#summary-table","title":"Summary Table","text":"Approach Pros Cons Best For Geemap Download Full local control, offline analysis Requires storage, download time Repeated analysis, offline work XEE Streaming No storage needed, always latest data Requires internet, slower for repeated queries Exploratory analysis, prototyping Dask + Zarr Scalable, efficient, cloud-ready Initial setup complexity Large-scale analysis, production"},{"location":"examples/complete-timeseries-workflow/#performance-comparison","title":"Performance Comparison","text":"<pre><code>import time\n\n# Benchmark each approach\nprint(\"\\n=== Performance Comparison ===\\n\")\n\n# 1. Geemap + XArray\nstart = time.time()\nresult1 = ndvi_ts.mean(dim='time').values\ntime1 = time.time() - start\nprint(f\"Geemap + XArray: {time1:.2f} seconds\")\n\n# 2. XEE (if loaded)\nstart = time.time()\nresult2 = ndvi_monthly_xee.mean(dim='time').compute().values\ntime2 = time.time() - start\nprint(f\"XEE Streaming: {time2:.2f} seconds\")\n\n# 3. Dask + Zarr\nstart = time.time()\nresult3 = ndvi_from_zarr.NDVI.mean(dim='time').compute().values\ntime3 = time.time() - start\nprint(f\"Dask + Zarr: {time3:.2f} seconds\")\n\nprint(f\"\\nFastest approach: \", end=\"\")\ntimes = {'Geemap': time1, 'XEE': time2, 'Zarr': time3}\nprint(min(times, key=times.get))\n</code></pre>"},{"location":"examples/complete-timeseries-workflow/#complete-example-script","title":"Complete Example Script","text":"<p>Here's the full script combining all approaches:</p> <pre><code>#!/usr/bin/env python3\n\"\"\"\nComplete Time Series Analysis Workflow\nDemonstrates: Geemap Download \u2192 XArray \u2192 XEE \u2192 Dask \u2192 Zarr\n\"\"\"\n\nimport ee\nimport geemap\nimport xarray as xr\nimport xee\nimport rioxarray as rxr\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom dask.distributed import Client, LocalCluster\nfrom pathlib import Path\nimport os\nimport calendar\nfrom datetime import datetime\n\n# Configuration\nROI = ee.Geometry.Rectangle([82.0, 26.5, 82.5, 27.0])\nYEAR = 2023\nOUTPUT_FOLDER = 'timeseries_analysis'\nSCALE = 20  # meters\n\ndef main():\n    \"\"\"Run complete workflow.\"\"\"\n\n    # Initialize\n    ee.Initialize(project='spatialgeography')\n    os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n\n    print(\"=\"*60)\n    print(\"CLOUD-NATIVE TIME SERIES ANALYSIS WORKFLOW\")\n    print(\"=\"*60)\n\n    # Approach 1: Geemap Download\n    print(\"\\n[1/3] Geemap Tiled Download...\")\n    monthly_files = download_monthly_ndvi(ROI, YEAR, OUTPUT_FOLDER)\n    ndvi_xarray = load_with_xarray(monthly_files)\n    analyze_timeseries(ndvi_xarray, OUTPUT_FOLDER)\n\n    # Approach 2: XEE Streaming\n    print(\"\\n[2/3] XEE Streaming...\")\n    ndvi_xee = stream_with_xee(ROI, YEAR)\n    compare_approaches(ndvi_xarray, ndvi_xee, OUTPUT_FOLDER)\n\n    # Approach 3: Dask + Zarr\n    print(\"\\n[3/3] Dask + Zarr Processing...\")\n    process_with_dask_zarr(monthly_files, OUTPUT_FOLDER)\n\n    print(\"\\n\" + \"=\"*60)\n    print(\"\u2713 WORKFLOW COMPLETE!\")\n    print(f\"Results saved to: {OUTPUT_FOLDER}/\")\n    print(\"=\"*60)\n\ndef download_monthly_ndvi(roi, year, output_folder):\n    \"\"\"Download monthly NDVI using geemap.\"\"\"\n    # Implementation from Approach 1\n    pass\n\ndef load_with_xarray(files):\n    \"\"\"Load files with XArray.\"\"\"\n    # Implementation from Approach 1\n    pass\n\ndef analyze_timeseries(data, output_folder):\n    \"\"\"Analyze time series.\"\"\"\n    # Implementation from Approach 1\n    pass\n\ndef stream_with_xee(roi, year):\n    \"\"\"Stream data with XEE.\"\"\"\n    # Implementation from Approach 2\n    pass\n\ndef compare_approaches(data1, data2, output_folder):\n    \"\"\"Compare different approaches.\"\"\"\n    # Implementation from comparison section\n    pass\n\ndef process_with_dask_zarr(files, output_folder):\n    \"\"\"Process with Dask and save to Zarr.\"\"\"\n    # Implementation from Approach 3\n    pass\n\nif __name__ == '__main__':\n    main()\n</code></pre>"},{"location":"examples/complete-timeseries-workflow/#key-takeaways","title":"Key Takeaways","text":"<p>What You Learned</p> <ul> <li>Geemap: Download large time series without EECU, automatic tiling</li> <li>XArray: Powerful time series analysis with labeled dimensions</li> <li>XEE: Stream Earth Engine data directly without downloads</li> <li>Dask: Parallel processing for large datasets</li> <li>Zarr: Cloud-optimized storage with compression</li> <li>Integration: Combine tools for optimal workflow</li> <li>Performance: Choose the right tool for your use case</li> </ul>"},{"location":"examples/complete-timeseries-workflow/#next-steps","title":"Next Steps","text":"<ul> <li>Next: Time Series and Phenological Methods</li> <li>Learn Optimization Techniques for better performance</li> <li>Back: Classification Methods</li> </ul>"},{"location":"examples/complete-timeseries-workflow/#additional-resources","title":"Additional Resources","text":"<ul> <li>geemap Examples</li> <li>XArray Time Series</li> <li>Dask Best Practices</li> <li>Zarr Tutorial</li> </ul>"},{"location":"examples/data-access-download/","title":"Data Access & Tiled Downloading","text":""},{"location":"examples/data-access-download/#data-access-and-tiled-downloading","title":"Data Access and Tiled Downloading","text":"<p>Explore Earth Engine data interactively with XEE and download large datasets (Satellite &amp; OSM) locally using geemap and OSMnx.</p>"},{"location":"examples/data-access-download/#overview","title":"Overview","text":"<p>This example demonstrates the complete workflow for:</p> <ol> <li>Direct Data Access: Load Earth Engine data into an XArray dataset using XEE.</li> <li>OSM Data Retrieval: Get OpenStreetMap vector data (buildings, roads, etc.) using geemap.</li> <li>Tiled Downloading: Use <code>geemap</code> to download high-resolution raster data in chunks.</li> </ol>"},{"location":"examples/data-access-download/#step-1-initialize-and-load-data-with-xee","title":"Step 1: Initialize and Load Data with XEE","text":"<p>We'll start by loading Sentinel-2 data for a region of interest (ROI) to perform some quick analysis.</p> <pre><code>import ee\nimport xarray as xr\nimport xee\nimport geemap\nimport os\n\n# Initialize Earth Engine\nee.Initialize(project='spatialgeography')\n\n# Define a Region of Interest (ROI)\nroi = ee.Geometry.Point([77.1025, 28.7041]).buffer(5000).bounds()\n\n# Load Sentinel-2 SR data\ns2 = ee.ImageCollection(\"COPERNICUS/S2_SR_HARMONIZED\") \\\n    .filterBounds(roi) \\\n    .filterDate('2023-01-01', '2023-06-30') \\\n    .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 10)) \\\n    .median() \\\n    .clip(roi)\n\n# Load with XEE for interactive analysis\n# This allows you to work with the data as an XArray object\nds = xr.open_dataset(s2, engine='ee', geometry=roi, scale=10)\nprint(ds)\n</code></pre>"},{"location":"examples/data-access-download/#step-2-openstreetmap-osm-data-access","title":"Step 2: OpenStreetMap (OSM) Data Access","text":"<p><code>geemap</code> provides a convenient way to fetch OSM vector data directly into your notebook.</p> <pre><code># Get buildings in the area\nbuildings = geemap.osm_to_gdf(roi, tags={'building': True})\n\n# Get roads/highways\nroads = geemap.osm_to_gdf(roi, tags={'highway': True})\n\nprint(f\"Number of buildings found: {len(buildings)}\")\nprint(f\"Number of road segments: {len(roads)}\")\n\n# Visualize on a Map\nMap = geemap.Map()\nMap.centerObject(roi, 14)\nMap.addLayer(s2, {'bands': ['B4', 'B3', 'B2'], 'min': 0, 'max': 3000}, 'Satellite')\nMap.add_gdf(buildings, layer_name='Buildings', fill_color='red')\nMap.add_gdf(roads, layer_name='Roads', color='blue')\nMap\n</code></pre>"},{"location":"examples/data-access-download/#step-3-tiled-downloading-with-geemap","title":"Step 3: Tiled Downloading with geemap","text":"<p>When you need the actual GeoTIFF files on your disk for local processing or inclusion in a GIS, <code>geemap</code>'s <code>download_ee_image</code> is the most efficient way to get high-resolution data without using Earth Engine's standard export tasks.</p> <pre><code># Create an output directory\noutput_dir = 'downloads'\nif not os.path.exists(output_dir):\n    os.makedirs(output_dir)\n\n# Define the output file path\noutput_file = os.path.join(output_dir, 'delhi_sentinel2_10m.tif')\n\n# Tiled Download\n# geemap automatically splits the large request into tiles, \n# downloads them, and merges them into a single GeoTIFF.\ngeemap.download_ee_image(\n    s2.select(['B4', 'B3', 'B2', 'B8']), # Select essential bands\n    filename=output_file,\n    region=roi,\n    scale=10,        # 10m resolution\n    crs='EPSG:4326',  # WGS 84\n    num_threads=4    # Uses parallel downloads for speed\n)\n\nprint(f\"Dataset downloaded successfully to: {output_file}\")\n</code></pre>"},{"location":"examples/data-access-download/#step-4-verify-downloaded-data-locally","title":"Step 4: Verify Downloaded Data Locally","text":"<p>Once downloaded, you can load the local file back into XArray using <code>rioxarray</code> to verify the content.</p> <pre><code>import rioxarray\n\n# Load the local GeoTIFF\nlocal_ds = rioxarray.open_rasterio(output_file)\nprint(f\"Local Dataset Shape: {local_ds.shape}\")\n\n# Plot a single band\nlocal_ds.sel(band=1).plot(cmap='viridis')\n</code></pre>"},{"location":"examples/data-access-download/#why-use-this-workflow","title":"Why Use This Workflow?","text":"Feature XEE Access geemap Tiled Download Primary Use Real-time analysis, visualization Local storage, GIS integration Compute Dynamic, on-the-fly Pre-computed, downloaded Advantages No local storage needed Full resolution, offline access Scale Best for smaller areas or coarse res Handles large areas via tiling"},{"location":"examples/data-access-download/#key-takeaways","title":"Key Takeaways","text":"<p>Summary</p> <ul> <li>XEE is your go-to for exploratory data analysis directly in your Python notebook.</li> <li>geemap provides the bridge for getting high-quality data out of the cloud and onto your local machine.</li> <li>Tiling bypasses the standard 5,000-pixel limit of the Earth Engine GetThumbURL/Download API.</li> <li>OSM Integration allows you to seamlessly combine satellite imagery with vector ground-truth data.</li> </ul> <p>\u2192 Next: Indices and Enhancement</p>"},{"location":"examples/deep-learning-spatial/","title":"7. Deep Learning Architectures in Spatial Analysis","text":""},{"location":"examples/deep-learning-spatial/#deep-learning-architectures-in-spatial-analysis","title":"Deep Learning Architectures in Spatial Analysis","text":"<p>Leverage Convolutional Neural Networks (CNNs) and Transformers for advanced spatial feature extraction using XEE, TensorFlow, and PyTorch.</p>"},{"location":"examples/deep-learning-spatial/#overview","title":"Overview","text":"<p>This example covers:</p> <ol> <li>Convolutional Neural Networks (CNN): Implementing a U-Net for semantic segmentation.</li> <li>Vision Transformers (ViT): Concept of global attention in remote sensing.</li> <li>Graph Neural Networks (GNN): Analyzing irregularly spaced spatial data.</li> <li>Self-Supervised Learning: Pre-training on massive unlabelled satellite imagery.</li> </ol>"},{"location":"examples/deep-learning-spatial/#step-1-data-preparation-for-deep-learning","title":"Step 1: Data Preparation for Deep Learning","text":"<p>Deep learning requires smaller \"patches\" or \"tiles\" rather than massive full-scene images.</p> <pre><code>import ee\nimport xarray as xr\nimport xee\nimport numpy as np\n\n# Load Sentinel-2\nroi = ee.Geometry.Point([77.1, 28.7]).buffer(1000).bounds()\ns2 = ee.ImageCollection(\"COPERNICUS/S2_SR_HARMONIZED\").filterBounds(roi).median().clip(roi)\n\nds = xr.open_dataset(s2, engine='ee', geometry=roi, scale=10).compute()\n\n# Stack bands and create patches\ndef make_patches(da, size=64):\n    # (Simplified patch creation logic)\n    data = da[['B2', 'B3', 'B4', 'B8']].to_array().values\n    c, h, w = data.shape\n    patch = data[:, :size, :size]\n    return np.expand_dims(np.moveaxis(patch, 0, -1), 0) # (Batch, H, W, C)\n\npatch_data = make_patches(ds)\nprint(f\"Input shape: {patch_data.shape}\")\n</code></pre>"},{"location":"examples/deep-learning-spatial/#step-2-cnn-architecture-u-net-in-tensorflowkeras","title":"Step 2: CNN Architecture (U-Net in TensorFlow/Keras)","text":"<pre><code>import tensorflow as tf\nfrom tensorflow.keras import layers, models\n\ndef build_unet(input_shape=(64, 64, 4)):\n    inputs = layers.Input(input_shape)\n\n    # Downsample\n    c1 = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n    p1 = layers.MaxPooling2D((2, 2))(c1)\n\n    # Bottleneck\n    c2 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(p1)\n\n    # Upsample\n    u3 = layers.UpSampling2D((2, 2))(c2)\n    u3 = layers.concatenate([u3, c1])\n    outputs = layers.Conv2D(1, (1, 1), activation='sigmoid')(u3)\n\n    return models.Model(inputs, outputs)\n\nmodel = build_unet()\nmodel.summary()\n</code></pre>"},{"location":"examples/deep-learning-spatial/#step-3-transformers-in-vision-terminology","title":"Step 3: Transformers in Vision (Terminology)","text":"<p>Unlike CNNs, Transformers use Self-Attention to capture long-range dependencies in satellite imagery. This is particularly useful for detecting large-scale land forms or complex urban patterns.</p> <pre><code># Conceptual Vision Transformer block\nclass AttentionBlock(layers.Layer):\n    def __init__(self, embed_dim):\n        super().__init__()\n        self.mha = layers.MultiHeadAttention(num_heads=8, key_dim=embed_dim)\n        self.norm = layers.LayerNormalization()\n\n    def call(self, x):\n        attn_out = self.mha(x, x)\n        return self.norm(x + attn_out)\n</code></pre>"},{"location":"examples/deep-learning-spatial/#step-4-graph-neural-networks-gnn","title":"Step 4: Graph Neural Networks (GNN)","text":"<p>Used when data is not a grid (e.g., weather stations, social sensing data, or object-based image analysis).</p> <pre><code># Conceptual GNN Layer (PyTorch Geometric style)\n# Each node (pixel/object) aggregates information from its spatial neighbors.\n# x_new = f(x, neighbors)\n</code></pre>"},{"location":"examples/deep-learning-spatial/#step-5-training-and-evaluation","title":"Step 5: Training and Evaluation","text":"<pre><code># model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n# model.fit(train_gen, epochs=10)\n</code></pre>"},{"location":"examples/deep-learning-spatial/#key-takeaways","title":"Key Takeaways","text":"<p>Summary</p> <ul> <li>CNNs: The standard for pixel-wise classification and object detection.</li> <li>Transformers: Rising popularity for large-scale \"Foundation Models\".</li> <li>Infrastructure: Processing DL models requires GPU; it's often best to export XEE data to TFRecord or Zarr for training.</li> </ul> <p>\u2192 Back to Index</p>"},{"location":"examples/indices-enhancement/","title":"1. Remote Sensing Indices and Enhancement","text":""},{"location":"examples/indices-enhancement/#remote-sensing-indices-and-enhancement","title":"Remote Sensing Indices and Enhancement","text":"<p>A comprehensive guide to calculating spectral indices and applying enhancement techniques using XEE, XArray, and Dask.</p>"},{"location":"examples/indices-enhancement/#overview","title":"Overview","text":"<p>This example covers:</p> <ul> <li>Calculating a broad range of spectral indices (Vegetation, Water, Urban, Burn).</li> <li>Image enhancement techniques (Contrast stretching, Histogram equalization).</li> <li>Spatial filtering and edge detection.</li> <li>Sensor-specific considerations (Sentinel-2 vs. Landsat).</li> </ul>"},{"location":"examples/indices-enhancement/#step-1-initialization-and-data-loading","title":"Step 1: Initialization and Data Loading","text":"<pre><code>import ee\nimport xarray as xr\nimport xee\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport geemap\n\n# Initialize Earth Engine\nee.Initialize(project='spatialgeography')\n\n# Define a Region of Interest (ROI)\nroi = ee.Geometry.Point([77.1025, 28.7041]).buffer(5000).bounds()\n\n# Load Sentinel-2 SR data\ns2 = ee.ImageCollection(\"COPERNICUS/S2_SR_HARMONIZED\") \\\n    .filterBounds(roi) \\\n    .filterDate('2023-01-01', '2023-12-31') \\\n    .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 10)) \\\n    .median() \\\n    .clip(roi)\n\n# Load with XEE\nds = xr.open_dataset(s2, engine='ee', geometry=roi, scale=10)\nds = ds.compute() # Bring into memory for local enhancement\n</code></pre>"},{"location":"examples/indices-enhancement/#step-2-comprehensive-spectral-indices-with-spyndex","title":"Step 2: Comprehensive Spectral Indices with Spyndex","text":"<p>While manual calculations work, using the spyndex library is the scientifically standard approach as it leverages the Awesome Spectral Indices database.</p> <pre><code>import spyndex\n\n# Calculate multiple indices at once\n# spyndex automatically handles the constants and formulas\nindices = spyndex.computeIndex(\n    index = [\"NDVI\", \"EVI\", \"SAVI\", \"NDWI\", \"MNDWI\", \"NDBI\", \"NBR\"],\n    params = {\n        \"N\": ds.B8,   # NIR\n        \"R\": ds.B4,   # Red\n        \"G\": ds.B3,   # Green\n        \"B\": ds.B2,   # Blue\n        \"S1\": ds.B11, # SWIR1\n        \"S2\": ds.B12, # SWIR2\n        \"L\": 0.5      # Soil adjustment factor for SAVI\n    }\n)\n\n# Merge back into our main dataset\nds = xr.merge([ds, indices])\n\nprint(\"Indices calculated via spyndex.\")\n</code></pre>"},{"location":"examples/indices-enhancement/#step-3-image-enhancement-techniques","title":"Step 3: Image Enhancement Techniques","text":""},{"location":"examples/indices-enhancement/#31-contrast-stretching-2-linear-stretch","title":"3.1 Contrast Stretching (2% Linear Stretch)","text":"<pre><code>def linear_stretch(array, percent=2):\n    low, high = np.nanpercentile(array, [percent, 100-percent])\n    stretched = (array - low) / (high - low)\n    return np.clip(stretched, 0, 1)\n\n# Apply to RGB bands\nr = linear_stretch(ds.B4.values)\ng = linear_stretch(ds.B3.values)\nb = linear_stretch(ds.B2.values)\n\nrgb_stretched = np.stack([r, g, b], axis=-1)\n\nplt.figure(figsize=(10, 10))\nplt.imshow(rgb_stretched)\nplt.title(\"Enhanced True Color (2% Linear Stretch)\")\nplt.axis('off')\nplt.show()\n</code></pre>"},{"location":"examples/indices-enhancement/#32-histogram-equalization","title":"3.2 Histogram Equalization","text":"<pre><code>from skimage import exposure\n\n# Apply histogram equalization to the Green band as an example\ng_eq = exposure.equalize_hist(ds.B3.values)\n\nplt.figure(figsize=(12, 6))\nplt.subplot(121), plt.imshow(ds.B3, cmap='gray'), plt.title('Original Green')\nplt.subplot(122), plt.imshow(g_eq, cmap='gray'), plt.title('Equalized Green')\nplt.show()\n</code></pre>"},{"location":"examples/indices-enhancement/#step-4-spatial-operations-filtering","title":"Step 4: Spatial Operations &amp; Filtering","text":"<p>Using <code>scipy.ndimage</code> or <code>skimage</code> for spatial enhancement.</p> <pre><code>from scipy import ndimage\n\n# Sobel Edge Detection on NDVI\nndvi_filled = ds.NDVI.fillna(0).values\nsobel_x = ndimage.sobel(ndvi_filled, axis=0)\nsobel_y = ndimage.sobel(ndvi_filled, axis=1)\nsobel_mag = np.hypot(sobel_x, sobel_y)\n\nplt.figure(figsize=(8, 8))\nplt.imshow(sobel_mag, cmap='magma')\nplt.title(\"NDVI Edge Detection (Sobel)\")\nplt.axis('off')\nplt.show()\n</code></pre>"},{"location":"examples/indices-enhancement/#step-5-visualization-with-geemap","title":"Step 5: Visualization with Geemap","text":"<pre><code>Map = geemap.Map()\nMap.centerObject(roi, 13)\n\nvis_params = {'bands': ['B4', 'B3', 'B2'], 'min': 0, 'max': 3000}\nMap.addLayer(s2, vis_params, 'Sentinel-2 RGB')\n\n# Add NDVI from our XArray computation back (conceptual)\n# In geemap, you can directly calculate or use the EE object\nndvi_ee = s2.normalizedDifference(['B8', 'B4'])\nMap.addLayer(ndvi_ee, {'min': 0, 'max': 1, 'palette': ['white', 'green']}, 'NDVI')\n\nMap\n</code></pre>"},{"location":"examples/indices-enhancement/#key-takeaways","title":"Key Takeaways","text":"<p>Summary</p> <ul> <li>XEE provides the bridge to load raw EE data into XArray.</li> <li>XArray allows for clear, vectorized index calculation.</li> <li>Python\u2019s scientific stack (NumPy, Scipy, Skimage) offers advanced enhancement tools not natively in EE.</li> <li>Geemap remains the best tool for interactive validation.</li> </ul> <p>\u2192 Next: Clustering Methods</p>"},{"location":"examples/mcdm-methods/","title":"4. Multi-Criteria Decision Making Methods","text":""},{"location":"examples/mcdm-methods/#multi-criteria-decision-making-mcdm","title":"Multi-Criteria Decision Making (MCDM)","text":"<p>Perform site suitability analysis using Ranking, Weighting, and Aggregation methods with XEE and XArray.</p>"},{"location":"examples/mcdm-methods/#overview","title":"Overview","text":"<p>This example covers:</p> <ol> <li>Criteria Selection: Slope, LULC, Distance to Roads, etc.</li> <li>Normalization Methods: Preparing diverse layers for comparison.</li> <li>Weighting Methods: Simple Additive Weighting (SAW) and TOPSIS logic.</li> <li>Ranking and Aggregation: Generating a final suitability map.</li> </ol>"},{"location":"examples/mcdm-methods/#step-1-initialize-criteria-layers","title":"Step 1: Initialize Criteria Layers","text":"<p>We'll load topography and land cover data to find the best site for a nature park.</p> <pre><code>import ee\nimport xarray as xr\nimport xee\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Initialize Earth Engine\nee.Initialize(project='spatialgeography')\n\n# Area: Near a mountain region\nroi = ee.Geometry.Rectangle([76.5, 31.0, 77.0, 31.5])\n\n# Layer 1: Slope (Derived from SRTM)\ndem = ee.Image(\"USGS/SRTMGL1_003\").clip(roi)\nslope = ee.Terrain.slope(dem)\n\n# Layer 2: Distance to Water (Derived from JRC)\nwater = ee.Image(\"JRC/GSW1_4/GlobalSurfaceWater\").select('occurrence').clip(roi)\ndist_water = water.distance(ee.Kernel.gaussian(5000, 3000, 'meters'))\n\n# Load into XArray\nds = xr.open_dataset(ee.Image.cat([slope.rename('slope'), dist_water.rename('dist_water')]), \n                    engine='ee', geometry=roi, scale=100).compute()\n</code></pre>"},{"location":"examples/mcdm-methods/#step-2-normalization-methods","title":"Step 2: Normalization Methods","text":"<p>MCDM requires criteria to be on the same scale (usually 0 to 1).</p> <pre><code>def normalize_benefit(da):\n    \"\"\"Higher is better.\"\"\"\n    return (da - da.min()) / (da.max() - da.min())\n\ndef normalize_cost(da):\n    \"\"\"Lower is better (e.g., Slope).\"\"\"\n    return (da.max() - da) / (da.max() - da.min())\n\n# Slope: Lower is better for construction (Cost)\nnorm_slope = normalize_cost(ds.slope)\n\n# Distance to Water: Closer is better (Cost)\nnorm_water = normalize_cost(ds.dist_water)\n</code></pre>"},{"location":"examples/mcdm-methods/#step-3-weighted-aggregation-saw-method","title":"Step 3: Weighted Aggregation (SAW Method)","text":"<p>The Simple Additive Weighting (SAW) is the most intuitive aggregation method.</p> <pre><code># Weights: 40% Slope, 60% Water proximity\nw_slope = 0.4\nw_water = 0.6\n\nsuitability_saw = (norm_slope * w_slope) + (norm_water * w_water)\n\nplt.figure(figsize=(10, 8))\nsuitability_saw.plot(cmap='YlGn')\nplt.title(\"Suitability Map (SAW Method)\")\nplt.show()\n</code></pre>"},{"location":"examples/mcdm-methods/#step-4-rigorous-ranking-with-pymcdm-topsis","title":"Step 4: Rigorous Ranking with PyMCDM (TOPSIS)","text":"<p>For a scientifically standard approach, we use the <code>pymcdm</code> library to calculate the TOPSIS score.</p> <pre><code>from pymcdm import methods as mcdm_methods\nfrom pymcdm import helpers\n\n# 1. Prepare data matrix (alternatives x criteria)\n# Every pixel is an alternative\nX = np.stack([norm_slope.values.ravel(), norm_water.values.ravel()], axis=1)\n\n# 2. Define weights and criteria types (1 for benefit, -1 for cost)\n# Since we already normalized, they can both be treated as benefit (higher score is better)\nweights = np.array([0.4, 0.6])\ntypes = np.array([1, 1])\n\n# 3. Initialize TOPSIS method\ntopsis = mcdm_methods.TOPSIS()\n\n# 4. Calculate preferences\n# Handle large datasets by processing in blocks if necessary\npref = topsis(X, weights, types)\n\n# 5. Reshape back to spatial dimensions\ntopsis_score = pref.reshape(norm_slope.shape)\n\nplt.figure(figsize=(10, 8))\nplt.imshow(topsis_score, cmap='RdYlGn')\nplt.colorbar(label='Preference Score')\nplt.title(\"Suitability Map (PyMCDM TOPSIS)\")\nplt.show()\n</code></pre>"},{"location":"examples/mcdm-methods/#key-takeaways","title":"Key Takeaways","text":"<p>Summary</p> <ul> <li>Normalization: Min-Max normalization is the foundation of spatial MCDM.</li> <li>Methods: SAW is easy to implement; TOPSIS is more robust against outliers.</li> <li>Spatial Logic: Every pixel acts as an \"alternative\" in the decision matrix.</li> </ul> <p>\u2192 Next: Time Series and Phenology</p>"},{"location":"examples/ndvi-analysis/","title":"NDVI Analysis","text":""},{"location":"examples/ndvi-analysis/#ndvi-analysis-using-xee","title":"NDVI Analysis using XEE","text":"<p>This example demonstrates how to perform a complete NDVI analysis workflow using Earth Engine and XArray (via XEE). We'll cover data selection, cloud masking, spatial alignment, and visualization.</p>"},{"location":"examples/ndvi-analysis/#scenario","title":"Scenario","text":"<p>Objective: Calculate and visualize the median NDVI for a buffered region around Visakhapatnam, India for the year 2020. Dataset: Sentinel-2 Harmonized Surface Reflectance. Area: 10km buffer around Vizag.</p>"},{"location":"examples/ndvi-analysis/#implementation","title":"Implementation","text":""},{"location":"examples/ndvi-analysis/#1-setup-and-initialization","title":"1. Setup and Initialization","text":"<p>First, we'll import the necessary libraries and initialize Earth Engine.</p> <pre><code>import xarray as xr\nimport xee\nimport ee\nimport geemap\nimport matplotlib.pyplot as plt\n\n# Initialize Earth Engine with project ID\nee.Initialize(project='spatialgeography')\n</code></pre>"},{"location":"examples/ndvi-analysis/#2-define-region-and-dataset","title":"2. Define Region and Dataset","text":"<p>We'll define a point of interest and create a 5km buffer (10km total width) as our Region of Interest (ROI). Then, we'll filter the Sentinel-2 collection.</p> <pre><code>def mask_s2_clouds(image):\n    \"\"\"Masks clouds in a Sentinel-2 image using the QA band.\"\"\"\n    qa = image.select('QA60')\n    cloud_bit_mask = 1 &lt;&lt; 10\n    cirrus_bit_mask = 1 &lt;&lt; 11\n    mask = (\n        qa.bitwiseAnd(cloud_bit_mask)\n        .eq(0)\n        .And(qa.bitwiseAnd(cirrus_bit_mask).eq(0))\n    )\n    return image.updateMask(mask)\n\n# Define center point (Vizag) and buffer area\ncenter_point = [83.277, 17.7009]\nroi = ee.Geometry.Point(center_point).buffer(5000).bounds()\n\n# Filter collection\ndataset = (\n    ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED')\n    .filterBounds(roi)\n    .filterDate('2020-01-01', '2020-12-31')\n    # Relaxed cloud threshold to ensure data coverage\n    .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 80))\n    .map(mask_s2_clouds)\n)\n\nprint(f\"Collection size: {dataset.size().getInfo()} images\")\n</code></pre>"},{"location":"examples/ndvi-analysis/#3-interactive-visualization","title":"3. Interactive Visualization","text":"<p>Before deep processing, it's always good to see the data using <code>geemap</code>.</p> <pre><code>visualization = {\n    'min': 0, \n    'max': 3000, \n    'bands': ['B4', 'B3', 'B2']\n}\n\nm = geemap.Map()\nm.set_center(center_point[0], center_point[1], 12)\nm.add_layer(dataset.median(), visualization, 'RGB')\nm\n</code></pre>"},{"location":"examples/ndvi-analysis/#4-xee-processing-and-plotting","title":"4. XEE Processing and Plotting","text":"<p>Now, we'll open the dataset using the <code>ee</code> engine in XArray, calculate NDVI, and plot the result.</p> <pre><code>print(\"Opening dataset with XEE...\")\n\n# Open the dataset with explicit projection\nds_xee = xr.open_dataset(\n    dataset,\n    engine='ee',\n    geometry=roi,\n    scale=100,  # 100m for consistent analysis\n    crs='EPSG:4326', # WGS 84\n    ee_mask_value=-9999\n)\n\n# Calculate NDVI (B8 is NIR, B4 is Red)\nndvi_xee = (ds_xee.B8 - ds_xee.B4) / (ds_xee.B8 + ds_xee.B4)\n\n# Calculate temporal median (always sort by time first!)\nndvi_median = ndvi_xee.sortby('time').median(dim='time')\n\n# Trigger computation\nndvi_result = ndvi_median.compute()\n\n# Check for valid data and plot\nif ndvi_result.notnull().any():\n    print(\"Plotting NDVI map...\")\n    fig, ax = plt.subplots(figsize=(10, 8))\n    ndvi_result.plot(ax=ax, cmap='RdYlGn', vmin=0, vmax=1)\n    ax.set_title('Median NDVI (Vizag 2020)\\nScale: 100m, CRS: EPSG:4326')\n    plt.show()\nelse:\n    print(\"WARNING: All pixels are NaN! Check cloud filters.\")\n</code></pre>"},{"location":"examples/ndvi-analysis/#key-best-practices","title":"Key Best Practices","text":"<p>Relax Cloud Filters</p> <p>In tropical or coastal regions (like Vizag), strict cloud filters (e.g., &lt; 20%) can result in empty datasets. Relaxing the filter to 80% and using a median composite often yields better results.</p> <p>CRS Selection</p> <p>While <code>EPSG:3857</code> (Web Mercator) is great for visualization, <code>EPSG:4326</code> is the global standard for geospatial analysis. Choose based on your requirements.</p> <p>Explicit .compute()</p> <p>Remember that XEE datasets are lazy. You must call <code>.compute()</code> on your final array to trigger the Earth Engine computation and download the results into memory.</p>"},{"location":"examples/network-flow/","title":"6. Network and Flow Analysis","text":""},{"location":"examples/network-flow/#network-and-flow-analysis","title":"Network and Flow Analysis","text":"<p>Analyze spatial connectivity, optimize paths, and detect communities in geospatial networks using XEE, NetworkX, and OSMnx.</p>"},{"location":"examples/network-flow/#overview","title":"Overview","text":"<p>This example covers:</p> <ol> <li>Network Fundamentals: Nodes, Edges, and Adjacency.</li> <li>Spatial Networks: Extracting road networks from OSM data.</li> <li>Network Metrics: Centrality and Connectivity.</li> <li>Flow Analysis: Shortest paths and Flow accumulation concepts.</li> </ol>"},{"location":"examples/network-flow/#step-1-initialize-network-from-openstreetmap-osmnx","title":"Step 1: Initialize Network from OpenStreetMap (OSMnx)","text":"<pre><code>import osmnx as ox\nimport networkx as nx\nimport geemap\nimport matplotlib.pyplot as plt\n\n# Center point (Delhi)\nplace = \"New Delhi, India\"\n\n# Download the road network\nG = ox.graph_from_place(place, network_type=\"drive\", buffer_dist=2000)\n\n# Project to a local CRS\nG_projected = ox.project_graph(G)\n\n# Plot the network\nfig, ax = ox.plot_graph(G_projected, node_size=5, edge_linewidth=0.5, edge_color='gray')\n</code></pre>"},{"location":"examples/network-flow/#step-2-network-metrics-and-centrality","title":"Step 2: Network Metrics and Centrality","text":"<p>Identifying the most important nodes in the city infrastructure.</p> <pre><code># Calculate Betweenness Centrality\n# (How many shortest paths pass through a node)\ncentrality = nx.betweenness_centrality(G, weight=\"length\")\n\n# Add centrality as a node attribute\nnx.set_node_attributes(G, centrality, \"centrality\")\n\n# Plot with node colors based on centrality\nnc = ox.plot.get_node_colors_by_attr(G, \"centrality\", cmap=\"plasma\")\nfig, ax = ox.plot_graph(G, node_color=nc, node_size=15, node_zorder=2, edge_linewidth=0.5)\n</code></pre>"},{"location":"examples/network-flow/#step-3-shortest-path-and-flow-analysis","title":"Step 3: Shortest Path and Flow Analysis","text":"<pre><code># Define origin and destination nodes (randomly selected)\nimport random\norig = random.choice(list(G.nodes))\ndest = random.choice(list(G.nodes))\n\n# Calculate the shortest path based on length\nroute = nx.shortest_path(G, orig, dest, weight=\"length\")\n\n# Plot the route\nfig, ax = ox.plot_graph_route(G, route, route_linewidth=4, route_color=\"red\")\n</code></pre>"},{"location":"examples/network-flow/#step-4-river-flow-analysis-raster-approach","title":"Step 4: River Flow Analysis (Raster Approach)","text":"<p>In Earth Science, \"Flow\" often refers to water accumulation in river networks.</p> <pre><code>import ee\nimport xarray as xr\nimport xee\n\n# Load SRTM DEM\nroi = ee.Geometry.Rectangle([76.8, 28.5, 77.2, 28.9])\ndem = ee.Image(\"USGS/SRTMGL1_003\").clip(roi)\n\n# In EE, we use the Hydrology tools\nrivers = ee.HydroEngine.rivers(dem) # Conceptual\n\n# Using XEE for local analysis\nds = xr.open_dataset(dem, engine='ee', geometry=roi, scale=30).compute()\n\n# Local Flow accumulation (Conceptual algorithm)\n# Typically uses packages like 'pysheds'\n</code></pre>"},{"location":"examples/network-flow/#step-5-community-detection-in-networks","title":"Step 5: Community Detection in Networks","text":"<p>Grouping nodes based on connectivity rather than just proximity.</p> <pre><code>from networkx.algorithms import community\n\n# Detect communities using greedy modularity\ncommunities = community.greedy_modularity_communities(G)\n\n# Color nodes by community\nnode_comms = {}\nfor i, comm in enumerate(communities):\n    for node in comm:\n        node_comms[node] = i\n\nnx.set_node_attributes(G, node_comms, \"community\")\nnc = ox.plot.get_node_colors_by_attr(G, \"community\", cmap=\"tab20\")\nfig, ax = ox.plot_graph(G, node_color=nc, node_size=10)\n</code></pre>"},{"location":"examples/network-flow/#key-takeaways","title":"Key Takeaways","text":"<p>Summary</p> <ul> <li>Topological Logic: Networks focus on \"how things are connected\" rather than \"where they are\".</li> <li>Infrastructure: Network analysis is vital for urban planning and disaster management.</li> <li>Hydrology: Combines raster flow models with graph centrality to understand drainage.</li> </ul> <p>\u2192 Next: Deep Learning Architectures</p>"},{"location":"examples/timeseries-phenology/","title":"5. Time Series and Phenological Methods","text":""},{"location":"examples/timeseries-phenology/#time-series-and-phenological-methods","title":"Time Series and Phenological Methods","text":"<p>Analyze temporal patterns, extract growth cycles, and simulate crop dynamics using XEE, XArray, and Scipy.</p>"},{"location":"examples/timeseries-phenology/#overview","title":"Overview","text":"<p>This example covers:</p> <ol> <li>Time Series Analysis: STL Decomposition and Smoothing.</li> <li>Similarity &amp; Classification: Comparing pixel trajectories.</li> <li>Phenological Extraction: Determining SOS (Start of Season) and EOS (End of Season).</li> <li>Crop Growth Models: Introduction to yield simulation concepts.</li> </ol>"},{"location":"examples/timeseries-phenology/#step-1-load-dense-time-series-modis","title":"Step 1: Load Dense Time Series (MODIS)","text":"<pre><code>import ee\nimport xarray as xr\nimport xee\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.signal import savgol_filter\nfrom statsmodels.tsa.seasonal import STL\n\n# Initialize Earth Engine\nee.Initialize(project='spatialgeography')\n\n# Agricultural region\nroi = ee.Geometry.Point([76.5, 30.5]).buffer(5000).bounds()\n\n# Load MODIS NDVI\nmodis = ee.ImageCollection(\"MODIS/061/MOD13A1\") \\\n    .filterBounds(roi) \\\n    .filterDate('2020-01-01', '2023-12-31') \\\n    .select('NDVI')\n\nds = xr.open_dataset(modis, engine='ee', geometry=roi, scale=500).compute()\nds['NDVI'] = ds.NDVI * 0.0001 # Correcting scale factor\n</code></pre>"},{"location":"examples/timeseries-phenology/#step-2-time-series-smoothing-savitzky-golay","title":"Step 2: Time Series Smoothing (Savitzky-Golay)","text":"<p>Remote sensing time series often have noise. Smoothing is the first step for phenology.</p> <pre><code>def smooth_time_series(da):\n    return xr.apply_ufunc(\n        savgol_filter, da,\n        kwargs={'window_length': 7, 'polyorder': 2, 'axis': 0},\n        dask='parallelized'\n    )\n\nds['NDVI_smooth'] = smooth_time_series(ds.NDVI)\n\n# Plot a single pixel profile\nds.NDVI.isel(lat=5, lon=5).plot(label='Raw', alpha=0.5)\nds.NDVI_smooth.isel(lat=5, lon=5).plot(label='Smoothed', linewidth=2)\nplt.legend()\nplt.title(\"NDVI Smoothing for Phenology\")\nplt.show()\n</code></pre>"},{"location":"examples/timeseries-phenology/#step-3-phenological-extraction-threshold-method","title":"Step 3: Phenological Extraction (Threshold Method)","text":"<p>Determining the Start of Season (SOS) and End of Season (EOS).</p> <pre><code>def extract_phenology(profile, threshold=0.4):\n    \"\"\"Simple threshold-based SOS extraction.\"\"\"\n    # Find indices where NDVI crosses threshold\n    start_season = np.where(profile &gt; threshold)[0]\n    if len(start_season) &gt; 0:\n        return start_season[0] # Index of SOS\n    return np.nan\n\n# Apply to the spatial dataset\nsos_map = xr.apply_ufunc(\n    extract_phenology, ds.NDVI_smooth,\n    input_core_dims=[['time']],\n    vectorize=True\n)\n\nplt.figure(figsize=(10, 8))\nsos_map.plot(cmap='viridis')\nplt.title(\"Start of Season (SOS) Day of Year\")\nplt.show()\n</code></pre>"},{"location":"examples/timeseries-phenology/#step-4-time-series-similarity-dynamic-time-warping","title":"Step 4: Time Series Similarity (Dynamic Time Warping)","text":"<p>Identifying pixels with similar growth patterns is often better achieved via Dynamic Time Warping (DTW) rather than Euclidean distance, as it accounts for temporal shifts.</p> <pre><code>from tslearn.metrics import dtw\n\n# Compare two pixel profiles\np1 = ds.NDVI_smooth.isel(lat=5, lon=5).values.reshape(-1, 1)\np2 = ds.NDVI_smooth.isel(lat=10, lon=10).values.reshape(-1, 1)\n\ndtw_score = dtw(p1, p2)\nprint(f\"Dynamic Time Warping Distance: {dtw_score:.3f}\")\n</code></pre>"},{"location":"examples/timeseries-phenology/#step-5-applications-in-agriculture","title":"Step 5: Applications in Agriculture","text":"<p>Integrating phenology with growing degree days (GDD) for yield simulation.</p> <pre><code># Conceptual: Yield = f(Cumulative NDVI, Area, GDD)\nyield_proxy = ds.NDVI_smooth.integrate('time')\n\nplt.figure(figsize=(10, 8))\nyield_proxy.plot(cmap='YlGn')\nplt.title(\"Cumulative Productivity (Yield Proxy)\")\nplt.show()\n</code></pre>"},{"location":"examples/timeseries-phenology/#key-takeaways","title":"Key Takeaways","text":"<p>Summary</p> <ul> <li>Smoothing: Essential for removing cloud artifacts before phenology extraction.</li> <li>Methods: Threshold-based methods are simple; derivative-based methods are more precise.</li> <li>Analysis: XArray makes it easy to integrate temporal profiles across large areas.</li> </ul> <p>\u2192 Next: Network and Flow Analysis</p>"},{"location":"examples/vegetation-vcf/","title":"8. Vegetation Continuous Fields (VCF) Analysis","text":""},{"location":"examples/vegetation-vcf/#vegetation-continuous-fields-vcf-analysis","title":"Vegetation Continuous Fields (VCF) Analysis","text":"<p>Analyze long-term vegetation trends using MODIS MOD44B data, XEE, and XArray visualization tools.</p>"},{"location":"examples/vegetation-vcf/#overview","title":"Overview","text":"<p>This example demonstrates how to:</p> <ol> <li>Load Yearly Data: Access the MODIS Vegetation Continuous Fields (VCF) product.</li> <li>Spatial Mapping: Create maps of percent tree cover.</li> <li>Time Series Charts: Generate line graphs of vegetation trends over decades.</li> <li>Statistical Distribution: Analyze the distribution of vegetation classes using histograms.</li> </ol>"},{"location":"examples/vegetation-vcf/#step-1-open-the-dataset-with-xee","title":"Step 1: Open the Dataset with XEE","text":"<p>We'll load the MOD44B collection, which provides yearly estimates of tree cover, non-tree vegetation, and non-vegetated surfaces.</p> <pre><code>import xarray as xr\nimport xee\nimport ee\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Initialize Earth Engine\nee.Initialize(project='spatialgeography')\n\n# Define a Region of Interest (ROI) - a forested area or transition zone\nroi = ee.Geometry.Point([83.277, 17.7009]).buffer(10000).bounds()\n\n# Open the MODIS VCF collection\nds = xr.open_dataset(\n    'ee://MODIS/061/MOD44B',\n    engine='ee',\n    geometry=roi,\n    scale=250, # MODIS native resolution is ~250m\n    start_time='2000-01-01',\n    end_time='2023-12-31'\n)\n\n# Crucial: Sort by time for correct charting\nds = ds.sortby('time')\n\nprint(ds)\n</code></pre>"},{"location":"examples/vegetation-vcf/#step-2-create-spatial-maps","title":"Step 2: Create Spatial Maps","text":"<p>Visualize the state of vegetation in the most recent year compared to the start of the century.</p> <pre><code># Select Percent Tree Cover\ntree_cover = ds.Percent_Tree_Cover\n\n# Create a side-by-side comparison Map\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\n\ntree_cover.isel(time=0).plot(ax=axes[0], cmap='YlGn', vmin=0, vmax=100)\naxes[0].set_title(f'Tree Cover - {tree_cover.time.values[0].year}')\n\ntree_cover.isel(time=-1).plot(ax=axes[1], cmap='YlGn', vmin=0, vmax=100)\naxes[1].set_title(f'Tree Cover - {tree_cover.time.values[-1].year}')\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"examples/vegetation-vcf/#step-3-time-series-analysis-charts","title":"Step 3: Time Series Analysis (Charts)","text":"<p>Plot the average tree cover over time to identify gain or loss trends.</p> <pre><code># Calculate spatial mean for each year\ntree_cover_ts = tree_cover.mean(dim=['lat', 'lon'])\n\n# Create a professional line chart\nplt.figure(figsize=(12, 5))\ntree_cover_ts.plot(marker='o', linestyle='-', color='forestgreen', linewidth=2)\n\n# Styling\nplt.title('23-Year Trend: Average Percent Tree Cover', fontsize=14)\nplt.ylabel('Percent Cover (%)')\nplt.xlabel('Year')\nplt.grid(True, alpha=0.3)\nplt.show()\n</code></pre>"},{"location":"examples/vegetation-vcf/#step-4-statistical-graphs-histograms","title":"Step 4: Statistical Graphs (Histograms)","text":"<p>Analyze the distribution of pixels to understand the landscape composition.</p> <pre><code># Select the latest year's data\nlatest_data = ds.isel(time=-1)\n\nplt.figure(figsize=(10, 6))\nsns.histplot(latest_data.Percent_Tree_Cover.values.flatten(), color=\"green\", label=\"Tree Cover\", kde=True)\nsns.histplot(latest_data.Percent_Non_Tree_Vegetation.values.flatten(), color=\"orange\", label=\"Non-Tree Veg\", kde=True)\nsns.histplot(latest_data.Percent_Non_Vegetated.values.flatten(), color=\"gray\", label=\"Non-Vegetated\", kde=True)\n\nplt.title('Landscape Composition Distribution (Latest Year)')\nplt.xlabel('Percent Coverage')\nplt.ylabel('Frequency')\nplt.legend()\nplt.show()\n</code></pre>"},{"location":"examples/vegetation-vcf/#step-5-multi-band-comparison-bar-chart","title":"Step 5: Multi-Band Comparison (Bar Chart)","text":"<p>Compare the mean coverage of different classes.</p> <pre><code># Calculate mean of all major classes\nmeans = {\n    'Tree Cover': ds.Percent_Tree_Cover.mean().values,\n    'Non-Tree Veg': ds.Percent_Non_Tree_Vegetation.mean().values,\n    'Non-Vegetated': ds.Percent_Non_Vegetated.mean().values\n}\n\nplt.figure(figsize=(8, 5))\nplt.bar(means.keys(), means.values(), color=['forestgreen', 'orange', 'gray'])\nplt.title('Mean Landscape Composition (2000-2023)')\nplt.ylabel('Average Percent Cover')\nplt.show()\n</code></pre>"},{"location":"examples/vegetation-vcf/#key-takeaways","title":"Key Takeaways","text":"<p>Visualization Mastery</p> <ul> <li>Spatial Maps: Use <code>.plot()</code> with specified time indices to see changes over space.</li> <li>Charts: Use <code>.mean(dim=['lat', 'lon'])</code> to reduce spatial data into a time series line graph.</li> <li>Graphs: Leverage <code>seaborn</code> or <code>matplotlib</code> directly on <code>.values.flatten()</code> for distribution analysis.</li> <li>VCF Data: MOD44B is excellent for studying long-term environmental change without the noise of seasonal NDVI.</li> </ul> <p>\u2192 Next: Deep Learning Architectures</p>"},{"location":"examples/water-quality/","title":"Water Quality Monitoring","text":""},{"location":"examples/water-quality/#water-quality-monitoring-with-xee","title":"Water Quality Monitoring with XEE","text":"<p>Monitor water quality parameters using satellite imagery and Earth Engine data accessed through XEE.</p>"},{"location":"examples/water-quality/#overview","title":"Overview","text":"<p>This example demonstrates:</p> <ul> <li>Water body detection and masking</li> <li>Turbidity estimation</li> <li>Chlorophyll-a concentration</li> <li>Temporal water quality trends</li> <li>Multi-lake comparison</li> </ul> <p>Dataset: Sentinel-2 for water quality parameters</p>"},{"location":"examples/water-quality/#step-1-initialize-and-define-study-area","title":"Step 1: Initialize and Define Study Area","text":"<pre><code>import ee\nimport xarray as xr\nimport xee\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom matplotlib.colors import LinearSegmentedColormap\n\n# Initialize Earth Engine\nee.Initialize(project='spatialgeography')\n\n# Define lake region (example: Dal Lake, Kashmir)\nlake_center = ee.Geometry.Point([74.8723, 34.1083])\nlake_roi = lake_center.buffer(2000)  # 2km buffer\n\n# Time period\nstart_date = '2023-01-01'\nend_date = '2023-12-31'\n\nprint(\"Study area defined\")\n</code></pre>"},{"location":"examples/water-quality/#step-2-load-and-process-sentinel-2-data","title":"Step 2: Load and Process Sentinel-2 Data","text":"<pre><code>def mask_s2_clouds(image):\n    \"\"\"Mask clouds and cirrus.\"\"\"\n    qa = image.select('QA60')\n    cloud_mask = qa.bitwiseAnd(1 &lt;&lt; 10).eq(0).And(\n                 qa.bitwiseAnd(1 &lt;&lt; 11).eq(0))\n    return image.updateMask(cloud_mask)\n\n# Load Sentinel-2 collection\ncollection = ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED') \\\n    .filterBounds(lake_roi) \\\n    .filterDate(start_date, end_date) \\\n    .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 20)) \\\n    .map(mask_s2_clouds)\n\nprint(f\"Found {collection.size().getInfo()} images\")\n\n# Create median composite\ncomposite = collection.median().clip(lake_roi)\n\n# Load with XEE\nds = xr.open_dataset(\n    composite,\n    engine='ee',\n    geometry=lake_roi,\n    scale=20,\n    crs='EPSG:4326'\n).compute()\n\nprint(\"Data loaded\")\n</code></pre>"},{"location":"examples/water-quality/#step-3-water-body-detection","title":"Step 3: Water Body Detection","text":"<pre><code># Calculate NDWI (Normalized Difference Water Index)\nndwi = (ds.B3 - ds.B8) / (ds.B3 + ds.B8)\n\n# Calculate MNDWI (Modified NDWI)\nmndwi = (ds.B3 - ds.B11) / (ds.B3 + ds.B11)\n\n# Water mask (MNDWI &gt; 0 indicates water)\nwater_mask = mndwi &gt; 0\n\n# Apply water mask to dataset\nds_water = ds.where(water_mask)\n\n# Visualize water detection\nfig, axes = plt.subplots(1, 3, figsize=(18, 6))\n\n# RGB\nrgb = np.stack([\n    ds.B4.values / 3000,\n    ds.B3.values / 3000,\n    ds.B2.values / 3000\n], axis=-1)\nrgb = np.clip(rgb, 0, 1)\naxes[0].imshow(rgb)\naxes[0].set_title('True Color')\naxes[0].axis('off')\n\n# MNDWI\nmndwi.plot(ax=axes[1], cmap='RdYlBu', vmin=-1, vmax=1)\naxes[1].set_title('MNDWI (Water Index)')\naxes[1].set_axis_off()\n\n# Water mask\nwater_mask.plot(ax=axes[2], cmap='Blues')\naxes[2].set_title('Water Mask')\naxes[2].set_axis_off()\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"examples/water-quality/#step-4-calculate-water-quality-parameters","title":"Step 4: Calculate Water Quality Parameters","text":"<pre><code># Turbidity estimation using Red/Blue ratio\n# Higher values indicate more turbidity\nturbidity_index = ds_water.B4 / ds_water.B2\n\n# Chlorophyll-a estimation using band ratios\n# Based on OC2 algorithm (simplified)\nchl_a = ds_water.B3 / ds_water.B4\n\n# Suspended sediment index\nssi = ds_water.B4 + ds_water.B3\n\n# Total Suspended Matter (TSM) - empirical relationship\ntsm = 3.0 * ds_water.B4 - 0.5  # Simplified model\n\nprint(\"Water quality parameters calculated\")\n</code></pre>"},{"location":"examples/water-quality/#step-5-visualize-water-quality","title":"Step 5: Visualize Water Quality","text":"<pre><code>fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n\n# Turbidity\nim1 = turbidity_index.plot(ax=axes[0, 0], cmap='YlOrBr', vmin=0, vmax=2)\naxes[0, 0].set_title('Turbidity Index (Red/Blue Ratio)')\naxes[0, 0].set_axis_off()\n\n# Chlorophyll-a\nim2 = chl_a.plot(ax=axes[0, 1], cmap='YlGn', vmin=0, vmax=3)\naxes[0, 1].set_title('Chlorophyll-a Proxy')\naxes[0, 1].set_axis_off()\n\n# Suspended Sediment\nim3 = ssi.plot(ax=axes[1, 0], cmap='copper', vmin=0, vmax=2000)\naxes[1, 0].set_title('Suspended Sediment Index')\naxes[1, 0].set_axis_off()\n\n# TSM\nim4 = tsm.plot(ax=axes[1, 1], cmap='RdYlBu_r', vmin=0, vmax=100)\naxes[1, 1].set_title('Total Suspended Matter (mg/L)')\naxes[1, 1].set_axis_off()\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"examples/water-quality/#step-6-temporal-analysis","title":"Step 6: Temporal Analysis","text":"<pre><code># Load time series for temporal analysis\ndef calculate_water_quality(image):\n    \"\"\"Calculate water quality for a single image.\"\"\"\n    # Mask water\n    mndwi = image.normalizedDifference(['B3', 'B11'])\n    water = image.updateMask(mndwi.gt(0))\n\n    # Calculate parameters\n    turbidity = water.select('B4').divide(water.select('B2'))\n    chl_a = water.select('B3').divide(water.select('B4'))\n\n    return image.addBands([\n        turbidity.rename('turbidity'),\n        chl_a.rename('chl_a')\n    ])\n\n# Apply to collection\nwq_collection = collection.map(calculate_water_quality)\n\n# Load time series\nds_ts = xr.open_dataset(\n    wq_collection,\n    engine='ee',\n    geometry=lake_roi,\n    scale=100,  # Lower resolution for faster processing\n    crs='EPSG:4326'\n)\n\nds_ts = ds_ts.sortby('time').compute()\n\n# Calculate mean values over lake\nturbidity_ts = ds_ts.turbidity.mean(dim=['lon', 'lat'])\nchl_a_ts = ds_ts.chl_a.mean(dim=['lon', 'lat'])\n\n# Plot time series\nfig, axes = plt.subplots(2, 1, figsize=(14, 10))\n\n# Turbidity over time\nturbidity_ts.plot(ax=axes[0], marker='o', linestyle='-', color='brown')\naxes[0].set_title('Lake Turbidity Over Time')\naxes[0].set_ylabel('Turbidity Index')\naxes[0].grid(True, alpha=0.3)\n\n# Chlorophyll-a over time\nchl_a_ts.plot(ax=axes[1], marker='o', linestyle='-', color='green')\naxes[1].set_title('Chlorophyll-a Proxy Over Time')\naxes[1].set_ylabel('Chl-a Index')\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"examples/water-quality/#step-7-water-quality-classification","title":"Step 7: Water Quality Classification","text":"<pre><code># Classify water quality based on turbidity and chlorophyll\ndef classify_water_quality(turbidity, chl_a):\n    \"\"\"Classify water quality into categories.\"\"\"\n    quality = np.zeros_like(turbidity.values)\n\n    # Good: Low turbidity, low chlorophyll\n    good = (turbidity &lt; 1.0) &amp; (chl_a &lt; 1.5)\n    quality[good.values] = 1\n\n    # Moderate: Medium turbidity or chlorophyll\n    moderate = ((turbidity &gt;= 1.0) &amp; (turbidity &lt; 1.5)) | \\\n               ((chl_a &gt;= 1.5) &amp; (chl_a &lt; 2.0))\n    quality[moderate.values] = 2\n\n    # Poor: High turbidity or chlorophyll\n    poor = (turbidity &gt;= 1.5) | (chl_a &gt;= 2.0)\n    quality[poor.values] = 3\n\n    return quality\n\n# Classify\nwq_class = classify_water_quality(turbidity_index, chl_a)\n\n# Visualize classification\nfrom matplotlib.colors import ListedColormap\n\ncolors = ['white', 'green', 'yellow', 'red']\nlabels = ['No Data', 'Good', 'Moderate', 'Poor']\ncmap = ListedColormap(colors)\n\nfig, ax = plt.subplots(figsize=(10, 8))\nim = ax.imshow(wq_class, cmap=cmap, vmin=0, vmax=3)\nax.set_title('Water Quality Classification')\nax.set_axis_off()\n\n# Add legend\nfrom matplotlib.patches import Patch\nlegend_elements = [Patch(facecolor=colors[i], label=labels[i]) \n                   for i in range(len(labels))]\nax.legend(handles=legend_elements, loc='upper right')\n\nplt.tight_layout()\nplt.show()\n\n# Calculate statistics\ntotal_pixels = np.sum(wq_class &gt; 0)\nfor i, label in enumerate(labels[1:], 1):\n    count = np.sum(wq_class == i)\n    percentage = (count / total_pixels) * 100 if total_pixels &gt; 0 else 0\n    print(f\"{label}: {percentage:.1f}%\")\n</code></pre>"},{"location":"examples/water-quality/#step-8-multi-lake-comparison","title":"Step 8: Multi-Lake Comparison","text":"<pre><code># Define multiple lakes\nlakes = [\n    {'name': 'Lake A', 'lon': 74.87, 'lat': 34.11, 'buffer': 2000},\n    {'name': 'Lake B', 'lon': 74.90, 'lat': 34.08, 'buffer': 1500},\n    {'name': 'Lake C', 'lon': 74.85, 'lat': 34.13, 'buffer': 1000}\n]\n\n# Compare water quality across lakes\ncomparison_results = []\n\nfor lake in lakes:\n    roi = ee.Geometry.Point([lake['lon'], lake['lat']]).buffer(lake['buffer'])\n\n    # Get composite\n    lake_composite = collection.median().clip(roi)\n\n    # Load data\n    lake_ds = xr.open_dataset(\n        lake_composite,\n        engine='ee',\n        geometry=roi,\n        scale=50,\n        crs='EPSG:4326'\n    ).compute()\n\n    # Calculate indices\n    lake_mndwi = (lake_ds.B3 - lake_ds.B11) / (lake_ds.B3 + lake_ds.B11)\n    lake_water = lake_ds.where(lake_mndwi &gt; 0)\n\n    lake_turbidity = (lake_water.B4 / lake_water.B2).mean().values.item()\n    lake_chl_a = (lake_water.B3 / lake_water.B4).mean().values.item()\n\n    comparison_results.append({\n        'Lake': lake['name'],\n        'Turbidity': lake_turbidity,\n        'Chl-a': lake_chl_a\n    })\n\n# Create comparison DataFrame\ncomparison_df = pd.DataFrame(comparison_results)\nprint(\"\\nLake Comparison:\")\nprint(comparison_df)\n\n# Visualize comparison\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Turbidity comparison\naxes[0].bar(comparison_df['Lake'], comparison_df['Turbidity'], color='brown', alpha=0.7)\naxes[0].set_ylabel('Turbidity Index')\naxes[0].set_title('Turbidity Comparison')\naxes[0].grid(True, alpha=0.3, axis='y')\n\n# Chlorophyll comparison\naxes[1].bar(comparison_df['Lake'], comparison_df['Chl-a'], color='green', alpha=0.7)\naxes[1].set_ylabel('Chl-a Index')\naxes[1].set_title('Chlorophyll-a Comparison')\naxes[1].grid(True, alpha=0.3, axis='y')\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"examples/water-quality/#step-9-export-results","title":"Step 9: Export Results","text":"<pre><code># Create comprehensive water quality dataset\nwq_results = xr.Dataset({\n    'turbidity': (['y', 'x'], turbidity_index.values),\n    'chl_a': (['y', 'x'], chl_a.values),\n    'tsm': (['y', 'x'], tsm.values),\n    'water_mask': (['y', 'x'], water_mask.values.astype(int)),\n    'quality_class': (['y', 'x'], wq_class)\n}, coords={\n    'y': ds.lat.values,\n    'x': ds.lon.values\n})\n\n# Add CRS\nwq_results = wq_results.rio.write_crs('EPSG:4326')\n\n# Save as NetCDF\nwq_results.to_netcdf('water_quality_results.nc')\nprint(\"Results saved to water_quality_results.nc\")\n\n# Export individual parameters as GeoTIFF\nturbidity_da = xr.DataArray(\n    turbidity_index.values,\n    dims=['y', 'x'],\n    coords={'y': ds.lat.values, 'x': ds.lon.values}\n)\nturbidity_da = turbidity_da.rio.write_crs('EPSG:4326')\nturbidity_da.rio.to_raster('turbidity.tif')\n\n# Save time series\nts_df = pd.DataFrame({\n    'date': pd.to_datetime(ds_ts.time.values),\n    'turbidity': turbidity_ts.values,\n    'chl_a': chl_a_ts.values\n})\nts_df.to_csv('water_quality_timeseries.csv', index=False)\n\n# Save comparison\ncomparison_df.to_csv('lake_comparison.csv', index=False)\n\nprint(\"All results exported successfully\")\n</code></pre>"},{"location":"examples/water-quality/#key-takeaways","title":"Key Takeaways","text":"<p>What You Learned</p> <ul> <li>Water body detection using MNDWI</li> <li>Turbidity estimation from band ratios</li> <li>Chlorophyll-a proxy calculation</li> <li>Total Suspended Matter estimation</li> <li>Temporal water quality monitoring</li> <li>Water quality classification</li> <li>Multi-lake comparison</li> <li>Comprehensive result export</li> </ul>"},{"location":"examples/water-quality/#additional-resources","title":"Additional Resources","text":"<ul> <li>Water Quality Remote Sensing</li> <li>Sentinel-2 for Water Quality</li> <li>Earth Engine Water Detection</li> </ul>"},{"location":"fundamentals/stac-dask/","title":"STAC and Dask","text":""},{"location":"fundamentals/stac-dask/#stac-and-dask-basics","title":"STAC and Dask Basics","text":""},{"location":"fundamentals/stac-dask/#overview","title":"Overview","text":"<p>In this section, we'll learn the basics of querying cloud-hosted data via STAC and leverage parallel computing via Dask.</p> <p>We will learn how to:</p> <ul> <li>Query a catalog of Sentinel-2 images</li> <li>Find the least-cloudy scene over a chosen area</li> <li>Visualize the scene</li> <li>Download it as a GeoTIFF file</li> </ul>"},{"location":"fundamentals/stac-dask/#setup-and-data-download","title":"Setup and Data Download","text":"<p>Install required packages:</p> <pre><code>%%capture\nif 'google.colab' in str(get_ipython()):\n    !pip install pystac-client odc-stac rioxarray dask jupyter-server-proxy\n</code></pre> <p>Import libraries:</p> <pre><code>import os\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport pystac_client\nfrom odc import stac\nimport xarray as xr\nimport rioxarray as rxr\n</code></pre>"},{"location":"fundamentals/stac-dask/#dask","title":"Dask","text":"<p>Dask is a Python library to run your computation in parallel across many machines. Dask has built-in support for key geospatial packages like XArray and Pandas, allowing you to scale your computation easily.</p> <p>Key Features:</p> <ul> <li>Run code in parallel on your laptop, cloud machine, or cluster</li> <li>Seamless integration with XArray and Pandas</li> <li>Lazy evaluation for efficient computation</li> <li>Interactive dashboard for monitoring</li> </ul>"},{"location":"fundamentals/stac-dask/#starting-a-dask-client","title":"Starting a Dask Client","text":"<pre><code>from dask.distributed import Client\n\nclient = Client()  # set up local cluster on the machine\nclient\n</code></pre>"},{"location":"fundamentals/stac-dask/#viewing-dask-dashboard-in-colab","title":"Viewing Dask Dashboard in Colab","text":"<p>If running in Colab, create a proxy URL to view the dashboard:</p> <pre><code>if 'google.colab' in str(get_ipython()):\n    from google.colab import output\n    port_to_expose = 8787  # Default port for Dask dashboard\n    print(output.eval_js(f'google.colab.kernel.proxyPort({port_to_expose})'))\n</code></pre>"},{"location":"fundamentals/stac-dask/#spatio-temporal-asset-catalog-stac","title":"Spatio Temporal Asset Catalog (STAC)","text":"<p>STAC is an open standard for specifying and querying geospatial data. Data providers can share catalogs of:</p> <ul> <li>Satellite imagery</li> <li>Climate datasets</li> <li>LIDAR data</li> <li>Vector data</li> </ul> <p>All STAC catalogs can be queried to find matching assets by time, location, or metadata.</p>"},{"location":"fundamentals/stac-dask/#stac-components","title":"STAC Components","text":"<p>Item: A single spatiotemporal asset (e.g., one satellite scene)</p> <pre><code>{\n  \"type\": \"Feature\",\n  \"stac_version\": \"1.0.0\",\n  \"id\": \"S2A_MSIL2A_20230115\",\n  \"properties\": {\n    \"datetime\": \"2023-01-15T10:30:00Z\",\n    \"eo:cloud_cover\": 15.5\n  },\n  \"geometry\": {...},\n  \"assets\": {\n    \"red\": {\"href\": \"s3://...\"},\n    \"nir\": {\"href\": \"s3://...\"}\n  }\n}\n</code></pre> <p>Collection: A group of related items</p> <p>Catalog: A collection of collections</p> <p>API: RESTful interface for searching</p>"},{"location":"fundamentals/stac-dask/#browse-available-catalogs","title":"Browse Available Catalogs","text":"<p>Visit https://stacindex.org/ to explore available STAC catalogs.</p>"},{"location":"fundamentals/stac-dask/#connecting-to-a-stac-catalog","title":"Connecting to a STAC Catalog","text":"<p>Let's use Earth Search by Element 84 to access Sentinel-2 data on AWS:</p> <pre><code>catalog = pystac_client.Client.open(\n    'https://earth-search.aws.element84.com/v1')\n</code></pre>"},{"location":"fundamentals/stac-dask/#defining-search-parameters","title":"Defining Search Parameters","text":"<pre><code>latitude = 27.163\nlongitude = 82.608\nyear = 2023\n\n# Define bounding box around the point\nkm2deg = 1.0 / 111\nx, y = (longitude, latitude)\nr = 1 * km2deg  # radius in degrees\nbbox = (x - r, y - r, x + r, y + r)\n</code></pre>"},{"location":"fundamentals/stac-dask/#basic-search","title":"Basic Search","text":"<pre><code>search = catalog.search(\n    collections=['sentinel-2-c1-l2a'],\n    bbox=bbox,\n    datetime=f'{year}'\n)\nitems = search.item_collection()\nitems\n</code></pre>"},{"location":"fundamentals/stac-dask/#filtering-by-metadata","title":"Filtering by Metadata","text":"<p>Apply additional filters for cloud cover and nodata pixels:</p> <pre><code>search = catalog.search(\n    collections=['sentinel-2-c1-l2a'],\n    bbox=bbox,\n    datetime=f'{year}',\n    query={\n        'eo:cloud_cover': {'lt': 30},\n        's2:nodata_pixel_percentage': {'lt': 10}\n    }\n)\nitems = search.item_collection()\n</code></pre>"},{"location":"fundamentals/stac-dask/#sorting-results","title":"Sorting Results","text":"<p>Sort by cloud cover to get the clearest scenes first:</p> <pre><code>search = catalog.search(\n    collections=['sentinel-2-c1-l2a'],\n    bbox=bbox,\n    datetime=f'{year}',\n    query={\n        'eo:cloud_cover': {'lt': 30},\n        's2:nodata_pixel_percentage': {'lt': 10}\n    },\n    sortby=[{\n        'field': 'properties.eo:cloud_cover',\n        'direction': 'asc'\n    }]\n)\nitems = search.item_collection()\nitems\n</code></pre>"},{"location":"fundamentals/stac-dask/#load-stac-images-to-xarray","title":"Load STAC Images to XArray","text":"<p>Load matching images as an XArray Dataset:</p> <pre><code>ds = stac.load(\n    items,\n    bands=['red', 'green', 'blue', 'nir'],\n    resolution=10,\n    chunks={},  # &lt;-- use Dask\n    groupby='solar_day',\n    preserve_original_order=True\n)\nds\n</code></pre>"},{"location":"fundamentals/stac-dask/#check-dataset-size","title":"Check Dataset Size","text":"<pre><code>print(f'DataSet size: {ds.nbytes/1e6:.2f} MB.')\n</code></pre>"},{"location":"fundamentals/stac-dask/#select-a-single-scene","title":"Select a Single Scene","text":"<p>Get the timestamp of the least cloudy scene:</p> <pre><code>timestamp = pd.to_datetime(items[0].properties['datetime']).tz_convert(None)\nscene = ds.sel(time=timestamp)\nscene\n</code></pre> <p>Check scene size:</p> <pre><code>print(f'Scene size: {scene.nbytes/1e6:.2f} MB.')\n</code></pre>"},{"location":"fundamentals/stac-dask/#load-data-into-memory","title":"Load Data into Memory","text":"<p>Use Dask to parallelize data loading:</p> <pre><code>%%time\nscene = scene.compute()\n</code></pre> <p>Watch the Dask dashboard to see the parallel processing in action!</p>"},{"location":"fundamentals/stac-dask/#handle-nodata-values","title":"Handle NoData Values","text":"<p>Sentinel-2 scenes have NoData value of 0:</p> <pre><code>scene = scene.where(scene != 0)\nscene\n</code></pre>"},{"location":"fundamentals/stac-dask/#apply-scale-and-offset","title":"Apply Scale and Offset","text":"<p>Convert raw pixel values to reflectances:</p> <pre><code>scale = 0.0001\noffset = -0.1\nscene = scene * scale + offset\n</code></pre> <p>Scale and Offset Values</p> <p>For Sentinel-2 scenes captured after Jan 25, 2022:</p> <pre><code>Scale: 0.0001\nOffset: -0.1\n</code></pre> <p>These values are in the <code>raster:bands</code> metadata for each band.</p>"},{"location":"fundamentals/stac-dask/#visualize-the-scene","title":"Visualize the Scene","text":"<p>Convert Dataset to DataArray:</p> <pre><code>scene_da = scene.to_array('band')\nscene_da\n</code></pre>"},{"location":"fundamentals/stac-dask/#check-spatial-metadata","title":"Check Spatial Metadata","text":"<pre><code>print('CRS:', scene_da.rio.crs)\nprint('Resolution:', scene_da.rio.resolution())\n</code></pre>"},{"location":"fundamentals/stac-dask/#create-preview","title":"Create Preview","text":"<p>Resample to lower resolution for visualization:</p> <pre><code>preview = scene_da.rio.reproject(\n    scene_da.rio.crs, resolution=300\n)\n\nfig, ax = plt.subplots(1, 1)\nfig.set_size_inches(5, 5)\npreview.sel(band=['red', 'green', 'blue']).plot.imshow(\n    ax=ax,\n    robust=True)\nax.set_title('RGB Visualization')\nax.set_axis_off()\nax.set_aspect('equal')\nplt.show()\n</code></pre>"},{"location":"fundamentals/stac-dask/#false-color-composite","title":"False Color Composite","text":"<pre><code>fig, ax = plt.subplots(1, 1, figsize=(8, 8))\npreview.sel(band=['nir', 'red', 'green']).plot.imshow(\n    ax=ax,\n    robust=True)\nax.set_title('False Color (NIR-R-G)')\nax.set_axis_off()\nax.set_aspect('equal')\nplt.show()\n</code></pre>"},{"location":"fundamentals/stac-dask/#save-results","title":"Save Results","text":""},{"location":"fundamentals/stac-dask/#save-as-netcdf","title":"Save as NetCDF","text":"<pre><code>output_file = 'scene.nc'\nscene.to_netcdf(output_file)\n</code></pre>"},{"location":"fundamentals/stac-dask/#save-as-geotiff","title":"Save as GeoTIFF","text":"<pre><code># Save RGB composite\nrgb_file = 'rgb_composite.tif'\nscene_da.sel(band=['red', 'green', 'blue']).rio.to_raster(rgb_file)\n\n# Save single band\nnir_file = 'nir_band.tif'\nscene.nir.rio.to_raster(nir_file)\n</code></pre>"},{"location":"fundamentals/stac-dask/#save-to-google-drive","title":"Save to Google Drive","text":"<pre><code>if 'google.colab' in str(get_ipython()):\n    from google.colab import drive\n    drive.mount('/content/drive')\n\n    drive_path = '/content/drive/MyDrive/remote-sensing-outputs'\n    if not os.path.exists(drive_path):\n        os.makedirs(drive_path)\n\n    output_path = os.path.join(drive_path, 'scene.nc')\n    scene.to_netcdf(output_path)\n</code></pre>"},{"location":"fundamentals/stac-dask/#advanced-stac-queries","title":"Advanced STAC Queries","text":""},{"location":"fundamentals/stac-dask/#search-by-geometry","title":"Search by Geometry","text":"<pre><code>from shapely.geometry import Point\n\n# Create a point geometry\npoint = Point(longitude, latitude)\nbuffer = point.buffer(0.01)  # ~1km buffer\n\nsearch = catalog.search(\n    collections=['sentinel-2-c1-l2a'],\n    intersects=buffer.__geo_interface__,\n    datetime='2023-01-01/2023-12-31'\n)\n</code></pre>"},{"location":"fundamentals/stac-dask/#multiple-collections","title":"Multiple Collections","text":"<pre><code>search = catalog.search(\n    collections=['sentinel-2-c1-l2a', 'landsat-c2-l2'],\n    bbox=bbox,\n    datetime=f'{year}'\n)\n</code></pre>"},{"location":"fundamentals/stac-dask/#complex-queries","title":"Complex Queries","text":"<pre><code>search = catalog.search(\n    collections=['sentinel-2-c1-l2a'],\n    bbox=bbox,\n    datetime='2023-06-01/2023-08-31',  # Summer months\n    query={\n        'eo:cloud_cover': {'lt': 20},\n        's2:nodata_pixel_percentage': {'lt': 5},\n        'platform': {'in': ['sentinel-2a', 'sentinel-2b']}\n    },\n    limit=50\n)\n</code></pre>"},{"location":"fundamentals/stac-dask/#dask-best-practices","title":"Dask Best Practices","text":""},{"location":"fundamentals/stac-dask/#chunk-size-selection","title":"Chunk Size Selection","text":"<pre><code># Good chunking - balanced chunks\nds = stac.load(\n    items,\n    bands=['red', 'nir'],\n    chunks={'time': 10, 'x': 512, 'y': 512}\n)\n\n# Too small - overhead dominates\nds_bad = stac.load(items, chunks={'time': 1, 'x': 64, 'y': 64})\n\n# Too large - memory issues\nds_bad = stac.load(items, chunks={'time': 100, 'x': 4096, 'y': 4096})\n</code></pre>"},{"location":"fundamentals/stac-dask/#monitor-performance","title":"Monitor Performance","text":"<pre><code># View task graph\nds.red.data.visualize(filename='task_graph.png')\n\n# Check chunk info\nprint(ds.red.data)\n</code></pre>"},{"location":"fundamentals/stac-dask/#persist-results","title":"Persist Results","text":"<pre><code># Persist in memory for repeated access\nscene_persisted = scene.persist()\n\n# Now operations are faster\nresult1 = scene_persisted.mean()\nresult2 = scene_persisted.std()\n</code></pre>"},{"location":"fundamentals/stac-dask/#exercise","title":"Exercise","text":"<p>The <code>items</code> variable contains a list of STAC Items returned by the query. Extract the Sentinel-2 Product ID stored in <code>s2:product_uri</code> property and print a list of all image IDs.</p> <pre><code>for item in items:\n    print(item.properties)\n</code></pre> <p>Solution:</p> <pre><code>product_ids = []\nfor item in items:\n    product_id = item.properties.get('s2:product_uri', 'N/A')\n    product_ids.append(product_id)\n    print(product_id)\n\nprint(f\"\\nTotal images found: {len(product_ids)}\")\n</code></pre>"},{"location":"fundamentals/stac-dask/#key-takeaways","title":"Key Takeaways","text":"<p>What You Learned</p> <ul> <li>STAC provides a standardized way to discover geospatial data</li> <li>Use <code>pystac-client</code> to search STAC catalogs</li> <li>Filter by metadata (cloud cover, date, location)</li> <li>Sort results to find optimal scenes</li> <li>Dask enables parallel data loading and processing</li> <li>Monitor Dask dashboard for performance insights</li> <li>Load STAC items directly into XArray with <code>odc-stac</code></li> <li>Apply scale/offset to convert to physical values</li> </ul>"},{"location":"fundamentals/stac-dask/#next-steps","title":"Next Steps","text":"<p>\u2192 Continue to Working with Zarr</p>"},{"location":"fundamentals/stac-dask/#additional-resources","title":"Additional Resources","text":"<ul> <li>STAC Specification</li> <li>STAC Index</li> <li>pystac-client Documentation</li> <li>odc-stac Documentation</li> <li>Dask Documentation</li> <li>Dask Best Practices</li> </ul>"},{"location":"fundamentals/xarray-basics/","title":"XArray Basics","text":""},{"location":"fundamentals/xarray-basics/#xarray-basics","title":"XArray Basics","text":""},{"location":"fundamentals/xarray-basics/#overview","title":"Overview","text":"<p>XArray has emerged as one of the key Python libraries to work with gridded raster datasets. It can natively handle time-series data, making it ideal for working with Remote Sensing datasets.</p> <p>Key Features:</p> <ul> <li>Builds on NumPy/Pandas for fast arrays and indexing</li> <li>Orders of magnitude faster than other Python libraries like <code>rasterio</code></li> <li>Growing ecosystem: <code>rioxarray</code>, <code>xarray-spatial</code>, <code>XEE</code></li> <li>Seamlessly works with local and cloud-hosted datasets</li> <li>Supports various optimized data formats (NetCDF, Zarr, COG)</li> </ul> <p>In this section, we'll learn XArray basics and create a median composite image from Sentinel-2 time-series data.</p>"},{"location":"fundamentals/xarray-basics/#setup-and-data-download","title":"Setup and Data Download","text":"<p>Install required packages:</p> <pre><code>%%capture\nif 'google.colab' in str(get_ipython()):\n    !pip install pystac-client odc-stac rioxarray dask botocore\n</code></pre> <p>Import libraries:</p> <pre><code>import os\nimport matplotlib.pyplot as plt\nimport pystac_client\nfrom odc.stac import stac_load, configure_s3_access\nimport xarray as xr\nimport rioxarray as rxr\n</code></pre> <p>Create working directories:</p> <pre><code>data_folder = 'data'\noutput_folder = 'output'\n\nif not os.path.exists(data_folder):\n    os.mkdir(data_folder)\nif not os.path.exists(output_folder):\n    os.mkdir(output_folder)\n</code></pre>"},{"location":"fundamentals/xarray-basics/#get-satellite-imagery","title":"Get Satellite Imagery","text":"<p>Define location and time of interest:</p> <pre><code>latitude = 27.163\nlongitude = 82.608\nyear = 2023\n</code></pre> <p>Search for Sentinel-2 imagery using STAC:</p> <pre><code>catalog = pystac_client.Client.open(\n    'https://earth-search.aws.element84.com/v1')\n\n# Configure S3 access for unsigned requests\nconfigure_s3_access(\n    aws_unsigned=True,\n)\n\n# Define bounding box\nkm2deg = 1.0 / 111\nx, y = (longitude, latitude)\nr = 1 * km2deg  # radius in degrees\nbbox = (x - r, y - r, x + r, y + r)\n\n# Search catalog\nsearch = catalog.search(\n    collections=['sentinel-2-c1-l2a'],\n    bbox=bbox,\n    datetime=f'{year}',\n    query={'eo:cloud_cover': {'lt': 30}},\n)\nitems = search.item_collection()\n</code></pre> <p>Load images as XArray Dataset:</p> <pre><code>ds = stac_load(\n    items,\n    bands=['red', 'green', 'blue', 'nir'],\n    resolution=10,\n    bbox=bbox,\n    chunks={},  # &lt;-- use Dask\n    groupby='solar_day',\n)\nds\n</code></pre> <p>Compute the dataset (load into memory):</p> <pre><code>%%time\nds = ds.compute()\n</code></pre>"},{"location":"fundamentals/xarray-basics/#xarray-terminology","title":"XArray Terminology","text":"<p>A Dataset consists of several components:</p>"},{"location":"fundamentals/xarray-basics/#variables","title":"Variables","text":"<p>Similar to bands in a raster dataset. Each variable contains an array of values.</p>"},{"location":"fundamentals/xarray-basics/#dimensions","title":"Dimensions","text":"<p>Similar to array axes (e.g., time, x, y).</p>"},{"location":"fundamentals/xarray-basics/#coordinates","title":"Coordinates","text":"<p>Labels for values in each dimension (e.g., timestamps, latitude, longitude).</p>"},{"location":"fundamentals/xarray-basics/#attributes","title":"Attributes","text":"<p>Metadata associated with the dataset.</p> <p></p>"},{"location":"fundamentals/xarray-basics/#dataarray","title":"DataArray","text":"<p>A Dataset consists of one or more <code>xarray.DataArray</code> objects. Access variables using dot notation:</p> <pre><code>da = ds.red\nda\n</code></pre>"},{"location":"fundamentals/xarray-basics/#selecting-data","title":"Selecting Data","text":"<p>XArray provides powerful selection methods similar to Pandas.</p>"},{"location":"fundamentals/xarray-basics/#index-based-selection-isel","title":"Index-based Selection (isel)","text":"<p>Select by position using <code>isel()</code>:</p> <pre><code># Select last time step\nda.isel(time=-1)\n</code></pre> <p>Get values as NumPy array:</p> <pre><code>da.isel(time=-1).values\n</code></pre> <p>Select across multiple dimensions:</p> <pre><code>da.isel(time=-1, x=-1, y=-1).values\n</code></pre>"},{"location":"fundamentals/xarray-basics/#label-based-selection-sel","title":"Label-based Selection (sel)","text":"<p>View coordinate values:</p> <pre><code>dates = da.time.values\ndates\n</code></pre> <p>Select by coordinate value:</p> <pre><code>da.sel(time='2023-12-16')\n</code></pre>"},{"location":"fundamentals/xarray-basics/#nearest-neighbor-lookup","title":"Nearest Neighbor Lookup","text":"<p>Find closest match when exact value doesn't exist:</p> <pre><code>da.sel(time='2023-01-01', method='nearest')\n</code></pre> <p>Interpolation</p> <p>Use <code>interp()</code> instead of <code>sel()</code> to interpolate values: </p><pre><code>da.interp(time='2023-01-01')\n</code></pre><p></p>"},{"location":"fundamentals/xarray-basics/#range-selection","title":"Range Selection","text":"<p>Select time ranges using <code>slice()</code>:</p> <pre><code># Select all observations in January 2023\nda.sel(time=slice('2023-01-01', '2023-01-31'))\n</code></pre>"},{"location":"fundamentals/xarray-basics/#aggregating-data","title":"Aggregating Data","text":"<p>XArray makes it easy to aggregate data across dimensions.</p>"},{"location":"fundamentals/xarray-basics/#temporal-aggregation","title":"Temporal Aggregation","text":"<p>Create a median composite from all images:</p> <pre><code>median = ds.median(dim='time')\nmedian\n</code></pre>"},{"location":"fundamentals/xarray-basics/#other-aggregation-functions","title":"Other Aggregation Functions","text":"<pre><code># Mean\nmean = ds.mean(dim='time')\n\n# Maximum\nmaximum = ds.max(dim='time')\n\n# Standard deviation\nstd = ds.std(dim='time')\n\n# Sum\ntotal = ds.sum(dim='time')\n</code></pre>"},{"location":"fundamentals/xarray-basics/#groupby-operations","title":"GroupBy Operations","text":"<p>Group by time periods:</p> <pre><code># Monthly median\nmonthly = ds.groupby('time.month').median(dim='time')\n\n# Yearly mean\nyearly = ds.groupby('time.year').mean(dim='time')\n\n# Seasonal aggregation\nseasonal = ds.groupby('time.season').mean(dim='time')\n</code></pre>"},{"location":"fundamentals/xarray-basics/#visualizing-data","title":"Visualizing Data","text":"<p>Convert Dataset to DataArray for plotting:</p> <pre><code>median_da = median.to_array('band')\nmedian_da\n</code></pre>"},{"location":"fundamentals/xarray-basics/#basic-plotting","title":"Basic Plotting","text":"<p>Use <code>robust=True</code> for automatic contrast stretching (2nd and 98th percentiles):</p> <pre><code>fig, ax = plt.subplots(1, 1)\nfig.set_size_inches(5, 5)\nmedian_da.sel(band=['red', 'green', 'blue']).plot.imshow(\n    ax=ax,\n    robust=True)\nax.set_title('RGB Visualization')\nax.set_axis_off()\nax.set_aspect('equal')\nplt.show()\n</code></pre>"},{"location":"fundamentals/xarray-basics/#custom-visualization","title":"Custom Visualization","text":"<pre><code>fig, axes = plt.subplots(2, 2, figsize=(12, 12))\n\n# Red band\nmedian.red.plot(ax=axes[0, 0], cmap='Reds', robust=True)\naxes[0, 0].set_title('Red Band')\n\n# Green band\nmedian.green.plot(ax=axes[0, 1], cmap='Greens', robust=True)\naxes[0, 1].set_title('Green Band')\n\n# Blue band\nmedian.blue.plot(ax=axes[1, 0], cmap='Blues', robust=True)\naxes[1, 0].set_title('Blue Band')\n\n# NIR band\nmedian.nir.plot(ax=axes[1, 1], cmap='YlOrRd', robust=True)\naxes[1, 1].set_title('NIR Band')\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"fundamentals/xarray-basics/#rgb-composite","title":"RGB Composite","text":"<pre><code># Create RGB composite\nrgb = median_da.sel(band=['red', 'green', 'blue'])\n\n# Normalize to 0-1 range\nrgb_norm = (rgb - rgb.min()) / (rgb.max() - rgb.min())\n\n# Plot\nfig, ax = plt.subplots(figsize=(10, 10))\nrgb_norm.plot.imshow(ax=ax)\nax.set_title('True Color Composite')\nax.set_axis_off()\nplt.show()\n</code></pre>"},{"location":"fundamentals/xarray-basics/#mathematical-operations","title":"Mathematical Operations","text":"<p>XArray supports element-wise operations:</p> <pre><code># Calculate NDVI\nndvi = (ds.nir - ds.red) / (ds.nir + ds.red)\n\n# Visualize\nfig, ax = plt.subplots(figsize=(8, 8))\nndvi.isel(time=0).plot(ax=ax, cmap='RdYlGn', vmin=-1, vmax=1)\nax.set_title('NDVI')\nplt.show()\n</code></pre>"},{"location":"fundamentals/xarray-basics/#broadcasting","title":"Broadcasting","text":"<p>Operations automatically broadcast across dimensions:</p> <pre><code># Subtract mean from each time step\nanomaly = ds - ds.mean(dim='time')\n\n# Normalize by standard deviation\nnormalized = (ds - ds.mean(dim='time')) / ds.std(dim='time')\n</code></pre>"},{"location":"fundamentals/xarray-basics/#saving-data","title":"Saving Data","text":""},{"location":"fundamentals/xarray-basics/#netcdf-format","title":"NetCDF Format","text":"<pre><code># Save to NetCDF\noutput_file = os.path.join(output_folder, 'median_composite.nc')\nmedian.to_netcdf(output_file)\n\n# Load back\nloaded = xr.open_dataset(output_file)\n</code></pre>"},{"location":"fundamentals/xarray-basics/#geotiff-format","title":"GeoTIFF Format","text":"<pre><code># Save single band as GeoTIFF\noutput_file = os.path.join(output_folder, 'red_band.tif')\nmedian.red.rio.to_raster(output_file)\n\n# Save RGB composite\nrgb_file = os.path.join(output_folder, 'rgb_composite.tif')\nmedian_da.sel(band=['red', 'green', 'blue']).rio.to_raster(rgb_file)\n</code></pre>"},{"location":"fundamentals/xarray-basics/#zarr-format","title":"Zarr Format","text":"<pre><code># Save to Zarr (cloud-optimized)\nzarr_file = os.path.join(output_folder, 'median.zarr')\nmedian.to_zarr(zarr_file, mode='w')\n\n# Load back\nloaded_zarr = xr.open_zarr(zarr_file)\n</code></pre>"},{"location":"fundamentals/xarray-basics/#advanced-features","title":"Advanced Features","text":""},{"location":"fundamentals/xarray-basics/#lazy-loading","title":"Lazy Loading","text":"<p>XArray supports lazy loading for large datasets:</p> <pre><code># Data is not loaded into memory\nds_lazy = xr.open_dataset('large_file.nc', chunks={'time': 10})\n\n# Computation is lazy\nresult = ds_lazy.mean(dim='time')\n\n# Trigger computation\nresult_computed = result.compute()\n</code></pre>"},{"location":"fundamentals/xarray-basics/#parallel-computing-with-dask","title":"Parallel Computing with Dask","text":"<pre><code>from dask.distributed import Client\n\n# Start Dask client\nclient = Client()\n\n# Load data with chunks\nds_dask = xr.open_dataset('large_file.nc', chunks={'time': 10, 'x': 512, 'y': 512})\n\n# Parallel computation\nresult = ds_dask.mean(dim='time').compute()\n</code></pre>"},{"location":"fundamentals/xarray-basics/#resampling","title":"Resampling","text":"<pre><code># Resample to monthly data\nmonthly = ds.resample(time='1M').mean()\n\n# Resample to weekly data\nweekly = ds.resample(time='1W').median()\n</code></pre>"},{"location":"fundamentals/xarray-basics/#rolling-windows","title":"Rolling Windows","text":"<pre><code># 3-month rolling mean\nrolling_mean = ds.rolling(time=3, center=True).mean()\n\n# 5-day rolling median\nrolling_median = ds.rolling(time=5, center=True).median()\n</code></pre>"},{"location":"fundamentals/xarray-basics/#exercise","title":"Exercise","text":"<p>Display the median composite for the month of May.</p> <p>The snippet below aggregates the time-series to monthly median composites using <code>groupby()</code>:</p> <pre><code>monthly = ds.groupby('time.month').median(dim='time')\nmonthly\n</code></pre> <p>You now have a new dimension named <code>month</code>. Start your exercise by:</p> <ol> <li>Converting the Dataset to a DataArray</li> <li>Extracting data for May (month=5) using <code>sel()</code></li> <li>Plotting the RGB composite</li> </ol> <p>Solution:</p> <pre><code># Convert to DataArray\nmonthly_da = monthly.to_array('band')\n\n# Select May\nmay = monthly_da.sel(month=5)\n\n# Plot RGB\nfig, ax = plt.subplots(figsize=(8, 8))\nmay.sel(band=['red', 'green', 'blue']).plot.imshow(ax=ax, robust=True)\nax.set_title('May Median Composite')\nax.set_axis_off()\nax.set_aspect('equal')\nplt.show()\n</code></pre>"},{"location":"fundamentals/xarray-basics/#key-takeaways","title":"Key Takeaways","text":"<p>What You Learned</p> <ul> <li>XArray provides labeled, multi-dimensional arrays perfect for satellite imagery</li> <li>Use <code>isel()</code> for index-based selection and <code>sel()</code> for label-based selection</li> <li>Aggregation functions like <code>mean()</code>, <code>median()</code>, <code>max()</code> work across dimensions</li> <li><code>groupby()</code> enables temporal aggregations (monthly, yearly, seasonal)</li> <li>Visualization is straightforward with built-in plotting methods</li> <li>XArray integrates seamlessly with Dask for parallel computing</li> <li>Multiple output formats supported: NetCDF, GeoTIFF, Zarr</li> </ul>"},{"location":"fundamentals/xarray-basics/#next-steps","title":"Next Steps","text":"<p>\u2192 Continue to STAC and Dask Basics</p>"},{"location":"fundamentals/xarray-basics/#additional-resources","title":"Additional Resources","text":"<ul> <li>XArray Documentation</li> <li>XArray Tutorial</li> <li>Rioxarray Documentation</li> <li>XArray Plotting</li> <li>Pangeo Gallery</li> </ul>"},{"location":"fundamentals/xee/","title":"XEE for Earth Engine","text":""},{"location":"fundamentals/xee/#xee-for-earth-engine","title":"XEE for Earth Engine","text":""},{"location":"fundamentals/xee/#overview","title":"Overview","text":"<p>XEE (XArray Earth Engine Extension) enables you to use Google Earth Engine datasets with XArray. This integration brings together the massive Earth Engine data catalog with the powerful analysis capabilities of XArray and Dask.</p> <p>Key Features:</p> <ul> <li>Access Earth Engine ImageCollections as XArray datasets</li> <li>Leverage Earth Engine's petabyte-scale data catalog</li> <li>Use XArray's intuitive API for analysis</li> <li>Combine with Dask for parallel processing</li> <li>Seamless integration with existing XArray workflows</li> </ul>"},{"location":"fundamentals/xee/#prerequisites","title":"Prerequisites","text":""},{"location":"fundamentals/xee/#earth-engine-account","title":"Earth Engine Account","text":"<ol> <li>Sign up at earthengine.google.com</li> <li>Wait for approval (usually instant for research/education)</li> </ol>"},{"location":"fundamentals/xee/#authentication","title":"Authentication","text":"<pre><code>import ee\n\n# Authenticate (first time only)\nee.Authenticate()\n\n# Initialize Earth Engine\nee.Initialize(project='spatialgeography')\n</code></pre> <p>For Colab:</p> <pre><code># Authenticate in Colab\nee.Authenticate()\nee.Initialize(project='spatialgeography')\n</code></pre>"},{"location":"fundamentals/xee/#installation","title":"Installation","text":"<pre><code>%%capture\n!pip install xee earthengine-api\n</code></pre>"},{"location":"fundamentals/xee/#basic-usage-opening-different-ee-types","title":"Basic Usage: Opening Different EE Types","text":"<p>XEE allows you to open various Earth Engine objects directly into XArray.</p>"},{"location":"fundamentals/xee/#1-opening-an-eeimage-single-bandmulti-band-image","title":"1. Opening an ee.Image (Single Band/Multi-band Image)","text":"<p>Useful for static datasets like DEMs or single composites.</p> <pre><code>import xarray as xr\nimport xee\nimport ee\n\nee.Initialize(project='spatialgeography')\n\n# Define geometry\nroi = ee.Geometry.Point([83.277, 17.7009]).buffer(10000)\n\n# Open an Earth Engine Image (SRTM DEM)\nds_image = xr.open_dataset(\n    'ee://CGIAR/SRTM90_V4',\n    engine='ee',\n    geometry=roi,\n    scale=30\n)\nprint(ds_image)\n</code></pre>"},{"location":"fundamentals/xee/#2-opening-an-eeimagecollection-time-series","title":"2. Opening an ee.ImageCollection (Time Series)","text":"<p>The primary use case for XEE, allowing you to work with multi-temporal data.</p> <pre><code># Open an Earth Engine ImageCollection (Sentinel-2)\nds_coll = xr.open_dataset(\n    'ee://COPERNICUS/S2_SR_HARMONIZED',\n    engine='ee',\n    geometry=roi,\n    scale=10,\n    start_time='2023-01-01',\n    end_time='2023-03-31',\n    variables=['B4', 'B8'] # Select bands to save memory\n)\nprint(ds_coll)\n</code></pre>"},{"location":"fundamentals/xee/#3-using-eefeaturecollection-spatial-filtering","title":"3. Using ee.FeatureCollection (Spatial Filtering)","text":"<p>While XEE focuses on raster data, you often use <code>FeatureCollections</code> to define the spatial bounds for your XArray.</p> <pre><code># Load administrative boundaries\ndistricts = ee.FeatureCollection('FAO/GAUL/2015/level2')\nvizag = districts.filter(ee.Filter.eq('ADM2_NAME', 'Visakhapatnam'))\n\n# Use the FeatureCollection geometry to clip the XArray\nds_fc = xr.open_dataset(\n    'ee://COPERNICUS/S2_SR_HARMONIZED',\n    engine='ee',\n    geometry=vizag.geometry(),\n    scale=100,\n    start_time='2023-01-01',\n    end_time='2023-01-31'\n)\nprint(ds_fc)\n</code></pre>"},{"location":"fundamentals/xee/#specifying-time-range","title":"Specifying Time Range","text":"<pre><code>ds = xr.open_dataset(\n    'ee://COPERNICUS/S2_SR',\n    engine='ee',\n    geometry=ee.Geometry.Point([82.6, 27.2]).buffer(10000),\n    scale=10,\n    ee_mask_value=-9999,\n    # Time range\n    start_time='2023-01-01',\n    end_time='2023-12-31'\n)\n</code></pre>"},{"location":"fundamentals/xee/#selecting-bands","title":"Selecting Bands","text":"<pre><code>ds = xr.open_dataset(\n    'ee://COPERNICUS/S2_SR',\n    engine='ee',\n    geometry=ee.Geometry.Point([82.6, 27.2]).buffer(10000),\n    scale=10,\n    # Select specific bands\n    variables=['B4', 'B8', 'B11']  # Red, NIR, SWIR\n)\n</code></pre>"},{"location":"fundamentals/xee/#working-with-different-datasets","title":"Working with Different Datasets","text":""},{"location":"fundamentals/xee/#landsat","title":"Landsat","text":"<pre><code># Landsat 8\nds_l8 = xr.open_dataset(\n    'ee://LANDSAT/LC08/C02/T1_L2',\n    engine='ee',\n    geometry=ee.Geometry.Rectangle([lon_min, lat_min, lon_max, lat_max]),\n    scale=30,\n    start_time='2023-01-01',\n    end_time='2023-12-31'\n)\n</code></pre>"},{"location":"fundamentals/xee/#modis","title":"MODIS","text":"<pre><code># MODIS NDVI\nds_modis = xr.open_dataset(\n    'ee://MODIS/006/MOD13A2',\n    engine='ee',\n    geometry=ee.Geometry.Point([lon, lat]).buffer(50000),\n    scale=1000,\n    start_time='2023-01-01',\n    end_time='2023-12-31'\n)\n</code></pre>"},{"location":"fundamentals/xee/#climate-data","title":"Climate Data","text":"<pre><code># ERA5 Climate Reanalysis\nds_era5 = xr.open_dataset(\n    'ee://ECMWF/ERA5/DAILY',\n    engine='ee',\n    geometry=ee.Geometry.Rectangle([lon_min, lat_min, lon_max, lat_max]),\n    scale=27830,  # ~25km\n    start_time='2023-01-01',\n    end_time='2023-12-31'\n)\n</code></pre>"},{"location":"fundamentals/xee/#terrain-data","title":"Terrain Data","text":"<pre><code># SRTM Digital Elevation Model\ndem = xr.open_dataset(\n    'ee://USGS/SRTMGL1_003',\n    engine='ee',\n    geometry=ee.Geometry.Rectangle([lon_min, lat_min, lon_max, lat_max]),\n    scale=30\n)\n</code></pre>"},{"location":"fundamentals/xee/#advanced-features","title":"Advanced Features","text":""},{"location":"fundamentals/xee/#cloud-masking","title":"Cloud Masking","text":"<pre><code>def mask_clouds(image):\n    \"\"\"Mask clouds in Sentinel-2 imagery.\"\"\"\n    qa = image.select('QA60')\n    cloud_mask = qa.bitwiseAnd(1 &lt;&lt; 10).eq(0).And(\n                 qa.bitwiseAnd(1 &lt;&lt; 11).eq(0))\n    return image.updateMask(cloud_mask)\n\n# Apply cloud mask\ncollection = ee.ImageCollection('COPERNICUS/S2_SR') \\\n    .filterBounds(ee.Geometry.Point([lon, lat])) \\\n    .filterDate('2023-01-01', '2023-12-31') \\\n    .map(mask_clouds)\n\n# Open as XArray\nds = xr.open_dataset(\n    collection,\n    engine='ee',\n    geometry=ee.Geometry.Point([lon, lat]).buffer(10000),\n    scale=10\n)\n</code></pre>"},{"location":"fundamentals/xee/#filtering-collections","title":"Filtering Collections","text":"<pre><code># Filter by cloud cover\ncollection = ee.ImageCollection('COPERNICUS/S2_SR') \\\n    .filterBounds(ee.Geometry.Point([lon, lat])) \\\n    .filterDate('2023-01-01', '2023-12-31') \\\n    .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 20))\n\nds = xr.open_dataset(\n    collection,\n    engine='ee',\n    geometry=ee.Geometry.Point([lon, lat]).buffer(10000),\n    scale=10\n)\n</code></pre>"},{"location":"fundamentals/xee/#custom-geometries","title":"Custom Geometries","text":"<pre><code># Polygon geometry\npolygon = ee.Geometry.Polygon([[\n    [lon_min, lat_min],\n    [lon_max, lat_min],\n    [lon_max, lat_max],\n    [lon_min, lat_max],\n    [lon_min, lat_min]\n]])\n\nds = xr.open_dataset(\n    'ee://COPERNICUS/S2_SR',\n    engine='ee',\n    geometry=polygon,\n    scale=10,\n    start_time='2023-01-01',\n    end_time='2023-12-31'\n)\n</code></pre>"},{"location":"fundamentals/xee/#using-feature-collections","title":"Using Feature Collections","text":"<pre><code># Load administrative boundaries\ncountries = ee.FeatureCollection('USDOS/LSIB_SIMPLE/2017')\nindia = countries.filter(ee.Filter.eq('country_na', 'India'))\n\n# Get geometry\ngeometry = india.geometry()\n\n# Open dataset for India\nds = xr.open_dataset(\n    'ee://COPERNICUS/S2_SR',\n    engine='ee',\n    geometry=geometry,\n    scale=100,  # Use coarser resolution for large areas\n    start_time='2023-01-01',\n    end_time='2023-01-31'\n)\n</code></pre>"},{"location":"fundamentals/xee/#combining-with-xarray-operations","title":"Combining with XArray Operations","text":""},{"location":"fundamentals/xee/#calculate-spectral-indices","title":"Calculate Spectral Indices","text":"<pre><code># Load Sentinel-2 data\nds = xr.open_dataset(\n    'ee://COPERNICUS/S2_SR',\n    engine='ee',\n    geometry=ee.Geometry.Point([82.6, 27.2]).buffer(10000),\n    scale=10,\n    start_time='2023-01-01',\n    end_time='2023-12-31',\n    variables=['B4', 'B8']  # Red, NIR\n)\n\n# Calculate NDVI\nndvi = (ds.B8 - ds.B4) / (ds.B8 + ds.B4)\n\n# Visualize\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(10, 8))\nndvi.isel(time=0).plot(ax=ax, cmap='RdYlGn', vmin=-1, vmax=1)\nax.set_title('NDVI from Earth Engine')\nplt.show()\n</code></pre>"},{"location":"fundamentals/xee/#time-series-analysis","title":"Time Series Analysis","text":"<pre><code># Load MODIS NDVI\nds = xr.open_dataset(\n    'ee://MODIS/006/MOD13A2',\n    engine='ee',\n    geometry=ee.Geometry.Point([82.6, 27.2]).buffer(5000),\n    scale=1000,\n    start_time='2020-01-01',\n    end_time='2023-12-31'\n)\n\n# Extract NDVI time series\n# IMPORTANT: Always sort by time before resampling or extracting time series\nndvi_ts = ds.NDVI.sortby('time').mean(dim=['X', 'Y'])\n\n# Plot time series\nfig, ax = plt.subplots(figsize=(12, 6))\nndvi_ts.plot(ax=ax)\nax.set_title('NDVI Time Series')\nax.set_ylabel('NDVI')\nplt.show()\n</code></pre>"},{"location":"fundamentals/xee/#important-tips-for-xee","title":"Important Tips for XEE","text":"<p>Crucial: Always Sort by Time</p> <p>Earth Engine collections are not always returned in chronological order. To avoid <code>ValueError: Index must be monotonic for resampling</code>, always sort your dataset: </p><pre><code>ds = ds.sortby('time')\n</code></pre><p></p> <p>Spatial Alignment &amp; Projections</p> <p>For consistent results and global compatibility, always specify the coordinate system as EPSG:4326 (WGS 84). </p><pre><code>ds = xr.open_dataset(..., crs='EPSG:4326', scale=100)\n</code></pre><p></p> <p>Handling Sparse Data (NaNs)</p> <p>Aggressive cloud masking can remove all data for certain periods. Always check if your dataset contains valid numeric data before plotting: </p><pre><code>if ds.NDVI.notnull().any():\n    ds.NDVI.plot()\nelse:\n    print(\"No valid data found in this period.\")\n</code></pre><p></p>"},{"location":"fundamentals/xee/#spatial-aggregation","title":"Spatial Aggregation","text":"<pre><code># Load temperature data\nds = xr.open_dataset(\n    'ee://ECMWF/ERA5/DAILY',\n    engine='ee',\n    geometry=ee.Geometry.Rectangle([70, 8, 97, 35]),  # India bounds\n    scale=27830,\n    start_time='2023-01-01',\n    end_time='2023-12-31',\n    variables=['mean_2m_air_temperature']\n)\n\n# Calculate spatial mean\ntemp_mean = ds.mean_2m_air_temperature.mean(dim=['lon', 'lat'])\n\n# Convert from Kelvin to Celsius\ntemp_celsius = temp_mean - 273.15\n\n# Plot\nfig, ax = plt.subplots(figsize=(12, 6))\ntemp_celsius.plot(ax=ax)\nax.set_title('Mean Temperature over India')\nax.set_ylabel('Temperature (\u00b0C)')\nplt.show()\n</code></pre>"},{"location":"fundamentals/xee/#integration-with-dask","title":"Integration with Dask","text":"<pre><code>from dask.distributed import Client\n\n# Start Dask client\nclient = Client()\n\n# Open large dataset with chunking\nds = xr.open_dataset(\n    'ee://COPERNICUS/S2_SR',\n    engine='ee',\n    geometry=ee.Geometry.Rectangle([lon_min, lat_min, lon_max, lat_max]),\n    scale=10,\n    start_time='2023-01-01',\n    end_time='2023-12-31',\n    chunks={'time': 10, 'X': 512, 'Y': 512}\n)\n\n# Parallel computation\nresult = ds.mean(dim='time').compute()\n</code></pre>"},{"location":"fundamentals/xee/#real-world-example-land-cover-change","title":"Real-World Example: Land Cover Change","text":"<pre><code>import ee\nimport xarray as xr\nimport matplotlib.pyplot as plt\n\nee.Initialize(project='spatialgeography')\n\n# Define area of interest\naoi = ee.Geometry.Point([82.6, 27.2]).buffer(20000)\n\n# Load Sentinel-2 for two time periods\nds_2020 = xr.open_dataset(\n    'ee://COPERNICUS/S2_SR',\n    engine='ee',\n    geometry=aoi,\n    scale=20,\n    start_time='2020-01-01',\n    end_time='2020-12-31',\n    variables=['B4', 'B8']\n)\n\nds_2023 = xr.open_dataset(\n    'ee://COPERNICUS/S2_SR',\n    engine='ee',\n    geometry=aoi,\n    scale=20,\n    start_time='2023-01-01',\n    end_time='2023-12-31',\n    variables=['B4', 'B8']\n)\n\n# Calculate median NDVI for each period\nndvi_2020 = ((ds_2020.B8 - ds_2020.B4) / (ds_2020.B8 + ds_2020.B4)).median(dim='time')\nndvi_2023 = ((ds_2023.B8 - ds_2023.B4) / (ds_2023.B8 + ds_2023.B4)).median(dim='time')\n\n# Calculate change\nndvi_change = ndvi_2023 - ndvi_2020\n\n# Visualize\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\nndvi_2020.plot(ax=axes[0], cmap='RdYlGn', vmin=-1, vmax=1)\naxes[0].set_title('NDVI 2020')\n\nndvi_2023.plot(ax=axes[1], cmap='RdYlGn', vmin=-1, vmax=1)\naxes[1].set_title('NDVI 2023')\n\nndvi_change.plot(ax=axes[2], cmap='RdBu', vmin=-0.5, vmax=0.5)\naxes[2].set_title('NDVI Change (2023-2020)')\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"fundamentals/xee/#performance-tips","title":"Performance Tips","text":""},{"location":"fundamentals/xee/#1-use-appropriate-scale","title":"1. Use Appropriate Scale","text":"<pre><code># Too fine - slow and large\nds = xr.open_dataset(..., scale=10)  # 10m\n\n# Appropriate for analysis\nds = xr.open_dataset(..., scale=30)  # 30m\n\n# Coarse for overview\nds = xr.open_dataset(..., scale=100)  # 100m\n</code></pre>"},{"location":"fundamentals/xee/#2-limit-spatial-extent","title":"2. Limit Spatial Extent","text":"<pre><code># Use smallest necessary geometry\ngeometry = ee.Geometry.Point([lon, lat]).buffer(5000)  # 5km buffer\n</code></pre>"},{"location":"fundamentals/xee/#3-filter-before-loading","title":"3. Filter Before Loading","text":"<pre><code># Filter in Earth Engine (server-side)\ncollection = ee.ImageCollection('COPERNICUS/S2_SR') \\\n    .filterBounds(geometry) \\\n    .filterDate('2023-01-01', '2023-12-31') \\\n    .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 20))\n\nds = xr.open_dataset(collection, engine='ee', ...)\n</code></pre>"},{"location":"fundamentals/xee/#4-select-only-needed-bands","title":"4. Select Only Needed Bands","text":"<pre><code># Don't load all bands\nds = xr.open_dataset(..., variables=['B4', 'B8'])  # Only Red and NIR\n</code></pre>"},{"location":"fundamentals/xee/#troubleshooting","title":"Troubleshooting","text":""},{"location":"fundamentals/xee/#authentication-issues","title":"Authentication Issues","text":"<pre><code># Re-authenticate\nee.Authenticate(force=True)\nee.Initialize(project='spatialgeography')\n</code></pre>"},{"location":"fundamentals/xee/#memory-errors","title":"Memory Errors","text":"<pre><code># Use coarser resolution\nds = xr.open_dataset(..., scale=100)  # Instead of 10\n\n# Or smaller area\ngeometry = ee.Geometry.Point([lon, lat]).buffer(1000)  # 1km instead of 10km\n</code></pre>"},{"location":"fundamentals/xee/#slow-performance","title":"Slow Performance","text":"<pre><code># Use Dask chunking\nds = xr.open_dataset(..., chunks={'time': 5, 'X': 256, 'Y': 256})\n\n# Reduce spatial extent\n# Increase scale (lower resolution)\n</code></pre>"},{"location":"fundamentals/xee/#key-takeaways","title":"Key Takeaways","text":"<p>What You Learned</p> <ul> <li>XEE enables XArray access to Earth Engine datasets</li> <li>Combine Earth Engine's data catalog with XArray's analysis tools</li> <li>Use Earth Engine for server-side filtering and processing</li> <li>Integrate with Dask for parallel computation</li> <li>Access diverse datasets: Sentinel, Landsat, MODIS, climate data</li> <li>Apply cloud masking and custom filters</li> <li>Perform time series and spatial analysis</li> </ul>"},{"location":"fundamentals/xee/#next-steps","title":"Next Steps","text":"<p>\u2192 Continue to Calculating Spectral Indices</p>"},{"location":"fundamentals/xee/#additional-resources","title":"Additional Resources","text":"<ul> <li>XEE Documentation</li> <li>Earth Engine Data Catalog</li> <li>Earth Engine Python API</li> <li>Earth Engine Guides</li> <li>XArray Earth Engine Examples</li> </ul>"},{"location":"fundamentals/zarr/","title":"Working with Zarr","text":""},{"location":"fundamentals/zarr/#working-with-zarr","title":"Working with Zarr","text":""},{"location":"fundamentals/zarr/#overview","title":"Overview","text":"<p>Zarr is a cloud-optimized format for storing chunked, compressed N-dimensional arrays. It's designed for efficient storage and access of large scientific datasets, making it ideal for remote sensing applications.</p> <p>Key Features:</p> <ul> <li>Cloud-native storage format</li> <li>Chunked and compressed arrays</li> <li>Parallel read/write operations</li> <li>Works seamlessly with Dask and XArray</li> <li>Supports multiple storage backends (local, S3, GCS, Azure)</li> </ul>"},{"location":"fundamentals/zarr/#why-zarr","title":"Why Zarr?","text":""},{"location":"fundamentals/zarr/#traditional-formats-netcdf-hdf5","title":"Traditional Formats (NetCDF, HDF5)","text":"<ul> <li>Designed for local filesystems</li> <li>Poor performance over HTTP</li> <li>Sequential access patterns</li> <li>Difficult to parallelize</li> </ul>"},{"location":"fundamentals/zarr/#zarr-advantages","title":"Zarr Advantages","text":"<ul> <li>\u2705 Optimized for cloud storage</li> <li>\u2705 Parallel read/write</li> <li>\u2705 Efficient partial reads</li> <li>\u2705 Multiple compression algorithms</li> <li>\u2705 Language-agnostic specification</li> </ul>"},{"location":"fundamentals/zarr/#basic-zarr-operations","title":"Basic Zarr Operations","text":""},{"location":"fundamentals/zarr/#creating-a-zarr-array","title":"Creating a Zarr Array","text":"<pre><code>import zarr\nimport numpy as np\n\n# Create a Zarr array\nz = zarr.open('data.zarr', mode='w', shape=(10000, 10000), \n              chunks=(1000, 1000), dtype='f4', \n              compressor=zarr.Blosc(cname='zstd', clevel=3))\n\n# Write data\ndata = np.random.random((10000, 10000))\nz[:] = data\n</code></pre>"},{"location":"fundamentals/zarr/#reading-zarr-data","title":"Reading Zarr Data","text":"<pre><code># Open existing Zarr array\nz = zarr.open('data.zarr', mode='r')\n\n# Read subset\nsubset = z[1000:2000, 1000:2000]\n\n# Read entire array\nall_data = z[:]\n</code></pre>"},{"location":"fundamentals/zarr/#zarr-with-xarray","title":"Zarr with XArray","text":"<pre><code>import xarray as xr\n\n# Create XArray dataset\nds = xr.Dataset({\n    'temperature': (['time', 'y', 'x'], np.random.random((365, 1000, 1000))),\n    'precipitation': (['time', 'y', 'x'], np.random.random((365, 1000, 1000)))\n})\n\n# Save to Zarr\nds.to_zarr('climate_data.zarr', mode='w')\n\n# Load from Zarr\nds_loaded = xr.open_zarr('climate_data.zarr')\n</code></pre>"},{"location":"fundamentals/zarr/#chunking-strategies","title":"Chunking Strategies","text":"<p>Chunking is critical for performance. Choose chunk sizes based on your access patterns.</p>"},{"location":"fundamentals/zarr/#time-series-access","title":"Time-Series Access","text":"<p>If you frequently access time slices:</p> <pre><code># Optimize for time-series access\nds.to_zarr('timeseries.zarr', \n           encoding={\n               'temperature': {'chunks': (1, 1000, 1000)}\n           })\n</code></pre>"},{"location":"fundamentals/zarr/#spatial-access","title":"Spatial Access","text":"<p>If you frequently access spatial subsets:</p> <pre><code># Optimize for spatial access\nds.to_zarr('spatial.zarr',\n           encoding={\n               'temperature': {'chunks': (365, 100, 100)}\n           })\n</code></pre>"},{"location":"fundamentals/zarr/#balanced-chunking","title":"Balanced Chunking","text":"<p>For mixed access patterns:</p> <pre><code># Balanced chunks\nds.to_zarr('balanced.zarr',\n           encoding={\n               'temperature': {'chunks': (10, 512, 512)}\n           })\n</code></pre>"},{"location":"fundamentals/zarr/#chunk-size-guidelines","title":"Chunk Size Guidelines","text":"<p>Optimal Chunk Sizes</p> <ul> <li>Minimum: 1 MB per chunk</li> <li>Maximum: 100 MB per chunk</li> <li>Optimal: 10-50 MB per chunk</li> <li>Rule of thumb: Aim for ~10,000 chunks total</li> </ul> <pre><code># Calculate chunk size\nimport numpy as np\n\ndef calculate_chunk_size(shape, dtype, target_mb=10):\n    \"\"\"Calculate optimal chunk dimensions.\"\"\"\n    itemsize = np.dtype(dtype).itemsize\n    target_bytes = target_mb * 1024 * 1024\n    total_items = target_bytes / itemsize\n\n    # Distribute across dimensions\n    chunk_dim = int(total_items ** (1/len(shape)))\n    chunks = tuple(min(chunk_dim, s) for s in shape)\n\n    return chunks\n\n# Example\nshape = (365, 5000, 5000)\nchunks = calculate_chunk_size(shape, 'float32', target_mb=10)\nprint(f\"Recommended chunks: {chunks}\")\n</code></pre>"},{"location":"fundamentals/zarr/#compression","title":"Compression","text":"<p>Zarr supports multiple compression algorithms:</p>"},{"location":"fundamentals/zarr/#blosc-recommended","title":"Blosc (Recommended)","text":"<pre><code>from zarr import Blosc\n\n# Fast compression\ncompressor = Blosc(cname='lz4', clevel=5, shuffle=Blosc.SHUFFLE)\n\n# Balanced\ncompressor = Blosc(cname='zstd', clevel=3, shuffle=Blosc.SHUFFLE)\n\n# High compression\ncompressor = Blosc(cname='zstd', clevel=9, shuffle=Blosc.BITSHUFFLE)\n\n# Use with XArray\nds.to_zarr('compressed.zarr',\n           encoding={\n               'temperature': {'compressor': compressor}\n           })\n</code></pre>"},{"location":"fundamentals/zarr/#other-compressors","title":"Other Compressors","text":"<pre><code>from numcodecs import Zlib, GZip, BZ2, LZMA\n\n# Zlib (good compression)\ncompressor = Zlib(level=5)\n\n# GZip (compatible)\ncompressor = GZip(level=6)\n\n# LZMA (high compression, slow)\ncompressor = LZMA(preset=6)\n</code></pre>"},{"location":"fundamentals/zarr/#compression-comparison","title":"Compression Comparison","text":"<pre><code>import time\n\ncompressors = {\n    'none': None,\n    'lz4': Blosc(cname='lz4', clevel=5),\n    'zstd': Blosc(cname='zstd', clevel=3),\n    'zlib': Zlib(level=5)\n}\n\ndata = np.random.random((1000, 1000, 100))\n\nfor name, comp in compressors.items():\n    start = time.time()\n    z = zarr.open(f'test_{name}.zarr', mode='w',\n                  shape=data.shape, chunks=(100, 100, 10),\n                  compressor=comp)\n    z[:] = data\n    write_time = time.time() - start\n\n    size = sum(f.stat().st_size for f in Path(f'test_{name}.zarr').rglob('*') if f.is_file())\n\n    print(f\"{name:10s} - Size: {size/1e6:6.2f} MB, Time: {write_time:5.2f}s\")\n</code></pre>"},{"location":"fundamentals/zarr/#cloud-storage","title":"Cloud Storage","text":""},{"location":"fundamentals/zarr/#aws-s3","title":"AWS S3","text":"<pre><code>import s3fs\n\n# Create S3 filesystem\ns3 = s3fs.S3FileSystem(anon=False)\n\n# Write to S3\nstore = s3fs.S3Map(root='s3://my-bucket/data.zarr', s3=s3)\nds.to_zarr(store, mode='w')\n\n# Read from S3\nds_s3 = xr.open_zarr(store)\n</code></pre>"},{"location":"fundamentals/zarr/#google-cloud-storage","title":"Google Cloud Storage","text":"<pre><code>import gcsfs\n\n# Create GCS filesystem\ngcs = gcsfs.GCSFileSystem(token='anon')\n\n# Write to GCS\nstore = gcsfs.GCSMap('gs://my-bucket/data.zarr', gcs=gcs)\nds.to_zarr(store, mode='w')\n\n# Read from GCS\nds_gcs = xr.open_zarr(store)\n</code></pre>"},{"location":"fundamentals/zarr/#azure-blob-storage","title":"Azure Blob Storage","text":"<pre><code>import adlfs\n\n# Create Azure filesystem\nfs = adlfs.AzureBlobFileSystem(account_name='myaccount')\n\n# Write to Azure\nstore = fs.get_mapper('container/data.zarr')\nds.to_zarr(store, mode='w')\n\n# Read from Azure\nds_azure = xr.open_zarr(store)\n</code></pre>"},{"location":"fundamentals/zarr/#appending-data","title":"Appending Data","text":"<p>Zarr supports appending along dimensions:</p> <pre><code># Initial dataset\nds1 = xr.Dataset({\n    'temperature': (['time', 'y', 'x'], np.random.random((10, 100, 100)))\n})\nds1.to_zarr('timeseries.zarr', mode='w')\n\n# Append new time steps\nds2 = xr.Dataset({\n    'temperature': (['time', 'y', 'x'], np.random.random((5, 100, 100)))\n})\nds2.to_zarr('timeseries.zarr', append_dim='time')\n\n# Load combined dataset\nds_combined = xr.open_zarr('timeseries.zarr')\nprint(ds_combined.dims)  # time: 15\n</code></pre>"},{"location":"fundamentals/zarr/#parallel-writing","title":"Parallel Writing","text":"<p>Use Dask for parallel writes:</p> <pre><code>from dask.distributed import Client\n\nclient = Client()\n\n# Create large dataset with Dask\nds_large = xr.Dataset({\n    'data': (['time', 'y', 'x'], \n             da.random.random((1000, 5000, 5000), chunks=(10, 500, 500)))\n})\n\n# Parallel write to Zarr\nds_large.to_zarr('large_data.zarr', \n                 compute=True,\n                 consolidated=True)\n</code></pre>"},{"location":"fundamentals/zarr/#metadata-and-attributes","title":"Metadata and Attributes","text":""},{"location":"fundamentals/zarr/#store-metadata","title":"Store Metadata","text":"<pre><code># Add attributes\nds.attrs['title'] = 'Climate Data'\nds.attrs['source'] = 'Satellite Observations'\nds.attrs['processing_date'] = '2024-01-01'\n\n# Variable attributes\nds['temperature'].attrs['units'] = 'Kelvin'\nds['temperature'].attrs['long_name'] = 'Air Temperature'\n\n# Save with metadata\nds.to_zarr('data_with_metadata.zarr')\n</code></pre>"},{"location":"fundamentals/zarr/#consolidated-metadata","title":"Consolidated Metadata","text":"<p>Improve performance with consolidated metadata:</p> <pre><code># Write with consolidated metadata\nds.to_zarr('data.zarr', consolidated=True)\n\n# Or consolidate existing Zarr\nfrom zarr.convenience import consolidate_metadata\nconsolidate_metadata('data.zarr')\n\n# Read with consolidated metadata (faster)\nds = xr.open_zarr('data.zarr', consolidated=True)\n</code></pre>"},{"location":"fundamentals/zarr/#real-world-example-sentinel-2-time-series","title":"Real-World Example: Sentinel-2 Time Series","text":"<pre><code>import pystac_client\nfrom odc.stac import load as stac_load\n\n# Search for Sentinel-2 data\ncatalog = pystac_client.Client.open(\n    'https://earth-search.aws.element84.com/v1')\n\nsearch = catalog.search(\n    collections=['sentinel-2-c1-l2a'],\n    bbox=[lon_min, lat_min, lon_max, lat_max],\n    datetime='2023-01-01/2023-12-31',\n    query={'eo:cloud_cover': {'lt': 30}}\n)\nitems = search.item_collection()\n\n# Load as XArray with Dask\nds = stac_load(\n    items,\n    bands=['red', 'green', 'blue', 'nir'],\n    resolution=10,\n    chunks={'time': 10, 'x': 512, 'y': 512}\n)\n\n# Calculate NDVI\nndvi = (ds.nir - ds.red) / (ds.nir + ds.red)\n\n# Save to Zarr with compression\nencoding = {\n    'ndvi': {\n        'compressor': zarr.Blosc(cname='zstd', clevel=3),\n        'chunks': (10, 512, 512)\n    }\n}\n\nndvi.to_dataset(name='ndvi').to_zarr(\n    'sentinel2_ndvi_2023.zarr',\n    encoding=encoding,\n    consolidated=True\n)\n\n# Load and analyze\nndvi_loaded = xr.open_zarr('sentinel2_ndvi_2023.zarr')\nmonthly_mean = ndvi_loaded.resample(time='1M').mean()\n</code></pre>"},{"location":"fundamentals/zarr/#performance-tips","title":"Performance Tips","text":""},{"location":"fundamentals/zarr/#1-use-consolidated-metadata","title":"1. Use Consolidated Metadata","text":"<pre><code># Always use consolidated metadata for cloud storage\nds.to_zarr('data.zarr', consolidated=True)\n</code></pre>"},{"location":"fundamentals/zarr/#2-choose-appropriate-chunks","title":"2. Choose Appropriate Chunks","text":"<pre><code># Match chunks to access patterns\n# Time-series: large time chunks\n# Spatial: large spatial chunks\n</code></pre>"},{"location":"fundamentals/zarr/#3-use-compression","title":"3. Use Compression","text":"<pre><code># Blosc with zstd is usually best\ncompressor = zarr.Blosc(cname='zstd', clevel=3)\n</code></pre>"},{"location":"fundamentals/zarr/#4-parallel-io","title":"4. Parallel I/O","text":"<pre><code># Use Dask for parallel operations\nds.to_zarr('data.zarr', compute=True)\n</code></pre>"},{"location":"fundamentals/zarr/#5-avoid-small-chunks","title":"5. Avoid Small Chunks","text":"<pre><code># Bad: too many small chunks\nchunks = (1, 10, 10)  # Only 100 items per chunk\n\n# Good: reasonable chunk size\nchunks = (10, 512, 512)  # ~2.6M items per chunk\n</code></pre>"},{"location":"fundamentals/zarr/#troubleshooting","title":"Troubleshooting","text":""},{"location":"fundamentals/zarr/#issue-slow-reads","title":"Issue: Slow Reads","text":"<pre><code># Solution: Check chunk size and use consolidated metadata\nds = xr.open_zarr('data.zarr', consolidated=True)\nprint(ds.chunks)\n</code></pre>"},{"location":"fundamentals/zarr/#issue-large-file-size","title":"Issue: Large File Size","text":"<pre><code># Solution: Use compression\nds.to_zarr('data.zarr', \n           encoding={'var': {'compressor': zarr.Blosc(cname='zstd', clevel=5)}})\n</code></pre>"},{"location":"fundamentals/zarr/#issue-memory-errors","title":"Issue: Memory Errors","text":"<pre><code># Solution: Use smaller chunks\nds = xr.open_zarr('data.zarr', chunks={'time': 1})\n</code></pre>"},{"location":"fundamentals/zarr/#key-takeaways","title":"Key Takeaways","text":"<p>What You Learned</p> <ul> <li>Zarr is optimized for cloud storage and parallel access</li> <li>Chunking strategy depends on access patterns</li> <li>Compression reduces storage costs</li> <li>Consolidated metadata improves performance</li> <li>Zarr works seamlessly with XArray and Dask</li> <li>Supports appending and parallel writes</li> <li>Multiple cloud storage backends supported</li> </ul>"},{"location":"fundamentals/zarr/#next-steps","title":"Next Steps","text":"<p>\u2192 Continue to XEE for Earth Engine</p>"},{"location":"fundamentals/zarr/#additional-resources","title":"Additional Resources","text":"<ul> <li>Zarr Documentation</li> <li>Zarr Tutorial</li> <li>XArray Zarr Guide</li> <li>Pangeo Zarr Guide</li> <li>Cloud-Optimized Formats</li> </ul>"},{"location":"getting-started/colab-basics/","title":"Google Colab Basics","text":""},{"location":"getting-started/colab-basics/#google-colab-basics","title":"Google Colab Basics","text":"<p>Google Colab is a hosted Jupyter notebook environment that allows anyone to run Python code via a web browser. It provides free computation and data storage that can be utilized by your Python code.</p>"},{"location":"getting-started/colab-basics/#getting-started","title":"Getting Started","text":""},{"location":"getting-started/colab-basics/#creating-your-first-notebook","title":"Creating Your First Notebook","text":"<ol> <li>Visit colab.research.google.com</li> <li>Sign in with your Google account</li> <li>Click File \u2192 New Notebook</li> </ol>"},{"location":"getting-started/colab-basics/#running-code","title":"Running Code","text":"<p>You can click the +Code button to create a new cell and enter a block of code. To run the code, click the Run Code button next to the cell, or press <code>Shift+Enter</code>.</p> <pre><code>print('Hello, Cloud Native Remote Sensing!')\n</code></pre>"},{"location":"getting-started/colab-basics/#package-management","title":"Package Management","text":"<p>Colab comes pre-installed with many Python packages. You can use a package by simply importing it:</p> <pre><code>import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n</code></pre>"},{"location":"getting-started/colab-basics/#installing-additional-packages","title":"Installing Additional Packages","text":"<p>If you want to install packages not included by default, use <code>!pip</code>:</p> <pre><code>!pip install rioxarray odc-stac pystac-client\n</code></pre> <p>Suppress Installation Output</p> <p>Use <code>%%capture</code> to hide installation messages: </p><pre><code>%%capture\n!pip install rioxarray odc-stac pystac-client\n</code></pre><p></p>"},{"location":"getting-started/colab-basics/#checking-installed-packages","title":"Checking Installed Packages","text":"<pre><code># List all packages\n!pip list\n\n# Check specific package version\n!pip show xarray\n</code></pre>"},{"location":"getting-started/colab-basics/#data-management","title":"Data Management","text":"<p>Colab provides 100GB of disk space along with your notebook. This can be used to store your data, intermediate outputs, and results.</p>"},{"location":"getting-started/colab-basics/#creating-directories","title":"Creating Directories","text":"<pre><code>import os\n\ndata_folder = 'data'\noutput_folder = 'output'\n\nif not os.path.exists(data_folder):\n    os.mkdir(data_folder)\nif not os.path.exists(output_folder):\n    os.mkdir(output_folder)\n</code></pre>"},{"location":"getting-started/colab-basics/#downloading-data","title":"Downloading Data","text":"<p>Helper function to download files from URLs:</p> <pre><code>import requests\n\ndef download(url, folder='data'):\n    filename = os.path.join(folder, os.path.basename(url))\n    if not os.path.exists(filename):\n        with requests.get(url, stream=True, allow_redirects=True) as r:\n            with open(filename, 'wb') as f:\n                for chunk in r.iter_content(chunk_size=8192):\n                    f.write(chunk)\n        print('Downloaded', filename)\n    return filename\n\n# Example usage\nurl = 'https://example.com/data.tif'\nfilepath = download(url)\n</code></pre>"},{"location":"getting-started/colab-basics/#reading-data","title":"Reading Data","text":"<pre><code>import geopandas as gpd\nimport rioxarray as rxr\n\n# Read vector data\ngdf = gpd.read_file(filepath)\n\n# Read raster data\nraster = rxr.open_rasterio(filepath)\n</code></pre>"},{"location":"getting-started/colab-basics/#google-drive-integration","title":"Google Drive Integration","text":"<p>Rather than saving to the temporary Colab machine, you can save to your Google Drive for persistent storage.</p>"},{"location":"getting-started/colab-basics/#mounting-google-drive","title":"Mounting Google Drive","text":"<pre><code>from google.colab import drive\ndrive.mount('/content/drive')\n</code></pre> <p>After running this, click the link and authorize access.</p>"},{"location":"getting-started/colab-basics/#working-with-drive-files","title":"Working with Drive Files","text":"<pre><code># Define paths\ndrive_folder_root = 'MyDrive'\noutput_folder = 'remote-sensing-outputs'\ndrive_folder_path = os.path.join('/content/drive', drive_folder_root, output_folder)\n\n# Create folder if it doesn't exist\nif not os.path.exists(drive_folder_path):\n    os.makedirs(drive_folder_path)\n\n# Save file to Drive\noutput_file = 'result.tif'\noutput_path = os.path.join(drive_folder_path, output_file)\nraster.rio.to_raster(output_path)\n</code></pre>"},{"location":"getting-started/colab-basics/#unmounting-drive","title":"Unmounting Drive","text":"<pre><code>drive.flush_and_unmount()\n</code></pre>"},{"location":"getting-started/colab-basics/#runtime-management","title":"Runtime Management","text":""},{"location":"getting-started/colab-basics/#runtime-types","title":"Runtime Types","text":"<p>Colab offers different runtime types:</p> <ul> <li>None: No accelerator (CPU only)</li> <li>GPU: NVIDIA GPU (T4, P100, or V100)</li> <li>TPU: Google TPU</li> </ul> <p>To change runtime:</p> <ol> <li>Click Runtime \u2192 Change runtime type</li> <li>Select Hardware accelerator</li> <li>Click Save</li> </ol>"},{"location":"getting-started/colab-basics/#checking-resources","title":"Checking Resources","text":"<pre><code># Check RAM\n!cat /proc/meminfo | grep MemTotal\n\n# Check CPU\n!cat /proc/cpuinfo | grep \"model name\" | head -1\n\n# Check GPU (if available)\n!nvidia-smi\n</code></pre>"},{"location":"getting-started/colab-basics/#session-limits","title":"Session Limits","text":"<ul> <li>Free tier: 12-hour maximum runtime</li> <li>Colab Pro: 24-hour maximum runtime</li> <li>Sessions disconnect after 90 minutes of inactivity</li> </ul>"},{"location":"getting-started/colab-basics/#keyboard-shortcuts","title":"Keyboard Shortcuts","text":"Action Shortcut Run cell <code>Ctrl+Enter</code> or <code>Shift+Enter</code> Insert cell above <code>Ctrl+M A</code> Insert cell below <code>Ctrl+M B</code> Delete cell <code>Ctrl+M D</code> Convert to code <code>Ctrl+M Y</code> Convert to text <code>Ctrl+M M</code> Show shortcuts <code>Ctrl+M H</code>"},{"location":"getting-started/colab-basics/#magic-commands","title":"Magic Commands","text":"<p>Colab supports IPython magic commands:</p> <pre><code># Time execution\n%%time\nresult = expensive_computation()\n\n# Time multiple runs\n%%timeit\nquick_computation()\n\n# Run shell commands\n!ls -la\n\n# Change directory\n%cd /content/data\n\n# Show current directory\n%pwd\n\n# Load external Python file\n%load script.py\n\n# Run external Python file\n%run script.py\n</code></pre>"},{"location":"getting-started/colab-basics/#visualization","title":"Visualization","text":""},{"location":"getting-started/colab-basics/#matplotlib","title":"Matplotlib","text":"<pre><code>import matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(1, 1, figsize=(10, 8))\nraster.plot(ax=ax, cmap='viridis')\nplt.title('Satellite Image')\nplt.show()\n</code></pre>"},{"location":"getting-started/colab-basics/#interactive-plots","title":"Interactive Plots","text":"<pre><code># Install plotly\n!pip install plotly\n\nimport plotly.express as px\n\n# Create interactive plot\nfig = px.scatter(df, x='lon', y='lat', color='value')\nfig.show()\n</code></pre>"},{"location":"getting-started/colab-basics/#forms-and-widgets","title":"Forms and Widgets","text":"<p>Create interactive forms:</p> <pre><code>#@title Configuration { run: \"auto\" }\nyear = 2023 #@param {type:\"slider\", min:2015, max:2024, step:1}\ncloud_cover = 30 #@param {type:\"slider\", min:0, max:100, step:5}\nregion = \"Global\" #@param [\"Global\", \"North America\", \"Europe\", \"Asia\"]\n\nprint(f\"Year: {year}\")\nprint(f\"Max Cloud Cover: {cloud_cover}%\")\nprint(f\"Region: {region}\")\n</code></pre>"},{"location":"getting-started/colab-basics/#sharing-notebooks","title":"Sharing Notebooks","text":""},{"location":"getting-started/colab-basics/#save-to-github","title":"Save to GitHub","text":"<ol> <li>Click File \u2192 Save a copy in GitHub</li> <li>Authorize GitHub access</li> <li>Select repository and branch</li> <li>Add commit message</li> <li>Click OK</li> </ol>"},{"location":"getting-started/colab-basics/#share-link","title":"Share Link","text":"<ol> <li>Click Share button (top right)</li> <li>Set permissions:</li> <li>Viewer: Can view only</li> <li>Commenter: Can comment</li> <li>Editor: Can edit</li> <li>Copy link</li> </ol>"},{"location":"getting-started/colab-basics/#download-notebook","title":"Download Notebook","text":"<pre><code># Download as .ipynb\n# File \u2192 Download \u2192 Download .ipynb\n\n# Download as .py\n# File \u2192 Download \u2192 Download .py\n</code></pre>"},{"location":"getting-started/colab-basics/#best-practices","title":"Best Practices","text":""},{"location":"getting-started/colab-basics/#1-save-frequently","title":"1. Save Frequently","text":"<p>Colab auto-saves, but manually save important work:</p> <ul> <li><code>Ctrl+S</code> or File \u2192 Save</li> </ul>"},{"location":"getting-started/colab-basics/#2-use-version-control","title":"2. Use Version Control","text":"<pre><code># Save checkpoint\n# File \u2192 Save a copy in Drive\n# File \u2192 Revision history\n</code></pre>"},{"location":"getting-started/colab-basics/#3-organize-code","title":"3. Organize Code","text":"<pre><code># Use markdown cells for documentation\n# Use code cells for executable code\n# Group related operations together\n</code></pre>"},{"location":"getting-started/colab-basics/#4-clear-outputs","title":"4. Clear Outputs","text":"<p>Before sharing:</p> <ul> <li>Edit \u2192 Clear all outputs</li> </ul>"},{"location":"getting-started/colab-basics/#5-restart-runtime","title":"5. Restart Runtime","text":"<p>If experiencing issues:</p> <ul> <li>Runtime \u2192 Restart runtime</li> </ul>"},{"location":"getting-started/colab-basics/#working-with-large-datasets","title":"Working with Large Datasets","text":""},{"location":"getting-started/colab-basics/#streaming-data","title":"Streaming Data","text":"<p>Don't download entire datasets:</p> <pre><code>import xarray as xr\n\n# Stream from cloud storage\nds = xr.open_dataset(\n    'https://example.com/large_dataset.nc',\n    chunks={'time': 10}\n)\n</code></pre>"},{"location":"getting-started/colab-basics/#using-dask","title":"Using Dask","text":"<pre><code>from dask.distributed import Client\n\n# Start local Dask cluster\nclient = Client()\n\n# View dashboard\nclient\n</code></pre>"},{"location":"getting-started/colab-basics/#viewing-dask-dashboard-in-colab","title":"Viewing Dask Dashboard in Colab","text":"<pre><code>from google.colab import output\n\nport_to_expose = 8787  # Dask dashboard port\nprint(output.eval_js(f'google.colab.kernel.proxyPort({port_to_expose})'))\n</code></pre>"},{"location":"getting-started/colab-basics/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/colab-basics/#common-issues","title":"Common Issues","text":"<p>1. Runtime Disconnected</p> <pre><code># Solution: Reconnect\n# Runtime \u2192 Reconnect\n</code></pre> <p>2. Out of Memory</p> <pre><code># Solution: Use smaller chunks or restart runtime\n# Runtime \u2192 Restart runtime\n</code></pre> <p>3. Package Import Errors</p> <pre><code># Solution: Reinstall package\n!pip install --upgrade --force-reinstall package-name\n</code></pre> <p>4. Drive Mount Issues</p> <pre><code># Solution: Unmount and remount\ndrive.flush_and_unmount()\ndrive.mount('/content/drive', force_remount=True)\n</code></pre>"},{"location":"getting-started/colab-basics/#advanced-features","title":"Advanced Features","text":""},{"location":"getting-started/colab-basics/#gpu-acceleration","title":"GPU Acceleration","text":"<pre><code>import tensorflow as tf\n\n# Check GPU availability\nprint(\"GPU Available:\", tf.config.list_physical_devices('GPU'))\n\n# Use GPU for computation\nwith tf.device('/GPU:0'):\n    # Your GPU-accelerated code here\n    pass\n</code></pre>"},{"location":"getting-started/colab-basics/#parallel-processing","title":"Parallel Processing","text":"<pre><code>from joblib import Parallel, delayed\n\ndef process_file(filename):\n    # Processing logic\n    return result\n\n# Process files in parallel\nresults = Parallel(n_jobs=-1)(\n    delayed(process_file)(f) for f in files\n)\n</code></pre>"},{"location":"getting-started/colab-basics/#custom-snippets","title":"Custom Snippets","text":"<p>Create reusable code snippets:</p> <ol> <li>Tools \u2192 Command palette</li> <li>Search \"Insert code snippet\"</li> <li>Create custom snippet</li> </ol>"},{"location":"getting-started/colab-basics/#example-workflow","title":"Example Workflow","text":"<p>Complete example of a typical remote sensing workflow in Colab:</p> <pre><code># 1. Install packages\n%%capture\n!pip install rioxarray pystac-client odc-stac\n\n# 2. Import libraries\nimport xarray as xr\nimport rioxarray as rxr\nfrom pystac_client import Client\nimport matplotlib.pyplot as plt\n\n# 3. Search for data\ncatalog = Client.open('https://earth-search.aws.element84.com/v1')\nsearch = catalog.search(\n    collections=['sentinel-2-l2a'],\n    bbox=[lon_min, lat_min, lon_max, lat_max],\n    datetime='2023-01-01/2023-12-31'\n)\nitems = search.item_collection()\n\n# 4. Load data\nfrom odc.stac import load as stac_load\nds = stac_load(\n    items,\n    bands=['red', 'green', 'blue', 'nir'],\n    resolution=10\n)\n\n# 5. Process\nndvi = (ds.nir - ds.red) / (ds.nir + ds.red)\n\n# 6. Visualize\nfig, ax = plt.subplots(figsize=(10, 8))\nndvi.isel(time=0).plot(ax=ax, cmap='RdYlGn')\nplt.title('NDVI')\nplt.show()\n\n# 7. Save to Drive\nfrom google.colab import drive\ndrive.mount('/content/drive')\noutput_path = '/content/drive/MyDrive/ndvi_result.nc'\nndvi.to_netcdf(output_path)\n</code></pre>"},{"location":"getting-started/colab-basics/#next-steps","title":"Next Steps","text":"<p>Now that you're familiar with Google Colab:</p> <p>\u2192 Continue to XArray Basics</p>"},{"location":"getting-started/colab-basics/#additional-resources","title":"Additional Resources","text":"<ul> <li>Colab FAQ</li> <li>Colab Notebooks Gallery</li> <li>Markdown Guide</li> <li>Data Science Snippets</li> </ul>"},{"location":"getting-started/colab-basics/#quick-reference-card","title":"Quick Reference Card","text":"<pre><code># Essential Colab Commands\n\n# Package management\n!pip install package-name\n!pip list\n!pip show package-name\n\n# File system\n!ls\n!pwd\n!mkdir folder_name\n!rm file_name\n\n# System information\n!cat /proc/meminfo\n!nvidia-smi\n\n# Google Drive\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\n# Magic commands\n%%time\n%%timeit\n%pwd\n%cd directory\n</code></pre>"},{"location":"getting-started/introduction/","title":"Introduction","text":""},{"location":"getting-started/introduction/#introduction-to-cloud-native-remote-sensing","title":"Introduction to Cloud Native Remote Sensing","text":""},{"location":"getting-started/introduction/#overview","title":"Overview","text":"<p>Cloud-native remote sensing represents a paradigm shift in how we work with Earth observation data. This approach leverages modern cloud computing infrastructure and optimized data formats to enable efficient processing of massive satellite imagery datasets.</p>"},{"location":"getting-started/introduction/#the-evolution-of-remote-sensing-workflows","title":"The Evolution of Remote Sensing Workflows","text":""},{"location":"getting-started/introduction/#traditional-approach","title":"Traditional Approach","text":"<pre><code>graph LR\n    A[Data Provider] --&gt; B[Download Data]\n    B --&gt; C[Local Storage]\n    C --&gt; D[Local Processing]\n    D --&gt; E[Results]</code></pre> <p>Challenges:</p> <ul> <li>Large download times (hours to days)</li> <li>Significant storage requirements (terabytes)</li> <li>Limited by local compute resources</li> <li>Difficult to share and reproduce</li> </ul>"},{"location":"getting-started/introduction/#cloud-native-approach","title":"Cloud-Native Approach","text":"<pre><code>graph LR\n    A[Cloud Data Catalog] --&gt; B[Query via STAC]\n    B --&gt; C[Stream Data]\n    C --&gt; D[Cloud Processing]\n    D --&gt; E[Results]</code></pre> <p>Benefits:</p> <ul> <li>Instant data access</li> <li>Minimal storage needs</li> <li>Scalable compute</li> <li>Easy sharing and collaboration</li> </ul>"},{"location":"getting-started/introduction/#core-concepts","title":"Core Concepts","text":""},{"location":"getting-started/introduction/#1-analysis-ready-data-ard","title":"1. Analysis-Ready Data (ARD)","text":"<p>Analysis-ready data is satellite imagery that has been processed to a minimum set of requirements and organized so it can be analyzed immediately:</p> <ul> <li>Geometric Correction: Orthorectified and georeferenced</li> <li>Radiometric Calibration: Converted to surface reflectance</li> <li>Cloud Masking: Quality flags included</li> <li>Tiled and Indexed: Organized for efficient access</li> </ul>"},{"location":"getting-started/introduction/#2-cloud-optimized-formats","title":"2. Cloud-Optimized Formats","text":""},{"location":"getting-started/introduction/#cloud-optimized-geotiff-cog","title":"Cloud Optimized GeoTIFF (COG)","text":"<ul> <li>Regular GeoTIFF with internal tiling</li> <li>Supports HTTP range requests</li> <li>Enables partial file reading</li> </ul>"},{"location":"getting-started/introduction/#zarr","title":"Zarr","text":"<ul> <li>Chunked, compressed N-dimensional arrays</li> <li>Cloud-native storage format</li> <li>Parallel read/write operations</li> <li>Works seamlessly with Dask</li> </ul>"},{"location":"getting-started/introduction/#3-stac-spatiotemporal-asset-catalog","title":"3. STAC (SpatioTemporal Asset Catalog)","text":"<p>STAC provides a common language to describe geospatial information:</p> <p>Core Components:</p> <ul> <li>Items: Individual assets (e.g., a single satellite scene)</li> <li>Catalogs: Collections of items</li> <li>Collections: Groups of related items</li> <li>API: RESTful interface for searching</li> </ul>"},{"location":"getting-started/introduction/#4-lazy-evaluation","title":"4. Lazy Evaluation","text":"<p>Process data only when needed:</p> <pre><code># This doesn't load data into memory\nds = xr.open_dataset('large_file.nc', chunks={'time': 10})\n\n# Computation is lazy\nresult = ds.mean(dim='time')\n\n# Data is loaded and processed only when needed\nresult.compute()\n</code></pre>"},{"location":"getting-started/introduction/#key-technologies-stack","title":"Key Technologies Stack","text":""},{"location":"getting-started/introduction/#data-discovery","title":"Data Discovery","text":"<ul> <li>STAC: Find and access data</li> <li>pystac-client: Python client for STAC APIs</li> </ul>"},{"location":"getting-started/introduction/#data-access","title":"Data Access","text":"<ul> <li>XArray: Multi-dimensional labeled arrays</li> <li>rioxarray: Geospatial extensions for XArray</li> <li>XEE: Earth Engine integration</li> </ul>"},{"location":"getting-started/introduction/#computation","title":"Computation","text":"<ul> <li>Dask: Parallel computing</li> <li>NumPy: Array operations</li> <li>Pandas: Tabular data</li> </ul>"},{"location":"getting-started/introduction/#storage","title":"Storage","text":"<ul> <li>Zarr: Cloud-optimized arrays</li> <li>COG: Cloud-optimized GeoTIFF</li> <li>NetCDF: Self-describing data format</li> </ul>"},{"location":"getting-started/introduction/#visualization","title":"Visualization","text":"<ul> <li>Matplotlib: Static plots</li> <li>Hvplot: Interactive visualizations</li> <li>Folium: Interactive maps</li> </ul>"},{"location":"getting-started/introduction/#common-use-cases","title":"Common Use Cases","text":""},{"location":"getting-started/introduction/#1-time-series-analysis","title":"1. Time Series Analysis","text":"<p>Monitor changes over time across large areas:</p> <pre><code># Load multi-year Sentinel-2 data\nds = xr.open_dataset('s2_timeseries.zarr', chunks='auto')\n\n# Calculate monthly NDVI\nndvi = (ds.nir - ds.red) / (ds.nir + ds.red)\nmonthly_ndvi = ndvi.resample(time='1M').mean()\n\n# Compute trends\ntrend = monthly_ndvi.polyfit(dim='time', deg=1)\n</code></pre>"},{"location":"getting-started/introduction/#2-large-area-mapping","title":"2. Large Area Mapping","text":"<p>Process continental or global datasets:</p> <pre><code># Process data in parallel using Dask\nfrom dask.distributed import Client\n\nclient = Client()  # Start local cluster\n\n# Load data with chunking\nds = xr.open_zarr('global_landcover.zarr')\n\n# Process in parallel\nresult = ds.groupby('time.year').mean().compute()\n</code></pre>"},{"location":"getting-started/introduction/#3-multi-sensor-fusion","title":"3. Multi-sensor Fusion","text":"<p>Combine data from different satellites:</p> <pre><code># Load Sentinel-2 optical data\ns2 = xr.open_dataset('sentinel2.nc')\n\n# Load Sentinel-1 radar data\ns1 = xr.open_dataset('sentinel1.nc')\n\n# Align and combine\ncombined = xr.merge([s2, s1], join='inner')\n</code></pre>"},{"location":"getting-started/introduction/#advantages-of-cloud-native-approach","title":"Advantages of Cloud-Native Approach","text":""},{"location":"getting-started/introduction/#performance","title":"Performance","text":"<ul> <li>Parallel Processing: Utilize multiple cores/machines</li> <li>Lazy Evaluation: Process only what's needed</li> <li>Optimized I/O: Read only required chunks</li> </ul>"},{"location":"getting-started/introduction/#scalability","title":"Scalability","text":"<ul> <li>Elastic Compute: Scale up/down as needed</li> <li>Distributed Processing: Handle datasets larger than memory</li> <li>Cloud Storage: Unlimited storage capacity</li> </ul>"},{"location":"getting-started/introduction/#collaboration","title":"Collaboration","text":"<ul> <li>Reproducible: Share code and environment</li> <li>Accessible: Run anywhere with internet</li> <li>Version Control: Track changes with Git</li> </ul>"},{"location":"getting-started/introduction/#cost-effectiveness","title":"Cost-Effectiveness","text":"<ul> <li>Pay-per-use: Only pay for what you use</li> <li>No Infrastructure: No need for local servers</li> <li>Shared Resources: Leverage public datasets</li> </ul>"},{"location":"getting-started/introduction/#getting-started-checklist","title":"Getting Started Checklist","text":"<ul> <li>[ ] Understand basic Python programming</li> <li>[ ] Familiarize yourself with NumPy and Pandas</li> <li>[ ] Learn basic remote sensing concepts</li> <li>[ ] Set up Google Colab account (free)</li> <li>[ ] Review coordinate systems and projections</li> <li>[ ] Understand raster data structures</li> </ul>"},{"location":"getting-started/introduction/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this course, you will be able to:</p> <ol> <li>Discover satellite imagery using STAC catalogs</li> <li>Access cloud-hosted datasets efficiently</li> <li>Process large-scale Earth observation data</li> <li>Analyze time series and multi-temporal datasets</li> <li>Visualize results effectively</li> <li>Scale your analysis from local to cloud</li> <li>Optimize workflows for performance</li> <li>Share reproducible analysis</li> </ol>"},{"location":"getting-started/introduction/#next-steps","title":"Next Steps","text":"<p>Now that you understand the fundamentals, let's set up your environment:</p> <p>\u2192 Continue to Setup Environment</p>"},{"location":"getting-started/introduction/#additional-resources","title":"Additional Resources","text":"<ul> <li>XArray Documentation</li> <li>STAC Specification</li> <li>Dask Documentation</li> <li>Zarr Documentation</li> <li>Pangeo Community</li> </ul>"},{"location":"getting-started/introduction/#key-takeaways","title":"Key Takeaways","text":"<p>Remember</p> <ul> <li>Cloud-native workflows enable analysis at unprecedented scales</li> <li>STAC standardizes data discovery across providers</li> <li>XArray provides intuitive interfaces for N-dimensional data</li> <li>Dask enables parallel and distributed computing</li> <li>Zarr optimizes data storage and access in the cloud</li> <li>Lazy evaluation minimizes data transfer and computation</li> </ul>"},{"location":"getting-started/setup/","title":"Setup Environment","text":""},{"location":"getting-started/setup/#setup-environment","title":"Setup Environment","text":"<p>This guide will help you set up your environment for cloud-native remote sensing with Python. We'll cover both Google Colab (recommended for beginners) and local installation.</p>"},{"location":"getting-started/setup/#option-1-google-colab-recommended","title":"Option 1: Google Colab (Recommended)","text":"<p>Google Colab is a free Jupyter notebook environment that runs in the cloud. It's perfect for this course because:</p> <ul> <li>\u2705 No installation required</li> <li>\u2705 Free GPU/TPU access</li> <li>\u2705 Pre-installed common libraries</li> <li>\u2705 Easy sharing and collaboration</li> <li>\u2705 Persistent storage with Google Drive</li> </ul>"},{"location":"getting-started/setup/#getting-started-with-colab","title":"Getting Started with Colab","text":"<ol> <li>Access Google Colab</li> <li>Visit colab.research.google.com</li> <li> <p>Sign in with your Google account</p> </li> <li> <p>Create a New Notebook</p> </li> <li>Click \"New Notebook\" or File \u2192 New Notebook</li> <li> <p>Rename your notebook (File \u2192 Rename)</p> </li> <li> <p>Install Required Packages</p> </li> </ol> <pre><code>%%capture\n!pip install xarray rioxarray pystac-client odc-stac dask zarr xee earthengine-api\n</code></pre>"},{"location":"getting-started/setup/#colab-pro-optional","title":"Colab Pro (Optional)","text":"<p>For intensive work, consider Colab Pro ($9.99/month):</p> <ul> <li>Longer runtimes</li> <li>More memory</li> <li>Faster GPUs</li> <li>Background execution</li> </ul>"},{"location":"getting-started/setup/#option-2-local-installation","title":"Option 2: Local Installation","text":""},{"location":"getting-started/setup/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.9 or higher</li> <li>pip or conda package manager</li> <li>8GB+ RAM recommended</li> <li>Git (for version control)</li> </ul>"},{"location":"getting-started/setup/#using-conda-recommended","title":"Using Conda (Recommended)","text":"<pre><code># Create a new environment\nconda create -n remote-sensing python=3.10\n\n# Activate the environment\nconda activate remote-sensing\n\n# Install packages\nconda install -c conda-forge xarray dask netCDF4 bottleneck numpy pandas matplotlib\n\n# Install additional packages with pip\npip install rioxarray pystac-client odc-stac zarr xee earthengine-api\n</code></pre>"},{"location":"getting-started/setup/#using-pip","title":"Using pip","text":"<pre><code># Create virtual environment\npython -m venv remote-sensing-env\n\n# Activate (Windows)\nremote-sensing-env\\Scripts\\activate\n\n# Activate (Linux/Mac)\nsource remote-sensing-env/bin/activate\n\n# Install packages\npip install xarray[complete] rioxarray pystac-client odc-stac dask[complete] zarr xee earthengine-api matplotlib jupyter\n</code></pre>"},{"location":"getting-started/setup/#required-packages","title":"Required Packages","text":""},{"location":"getting-started/setup/#core-libraries","title":"Core Libraries","text":"Package Purpose Installation xarray Multi-dimensional arrays <code>pip install xarray</code> rioxarray Geospatial extensions <code>pip install rioxarray</code> dask Parallel computing <code>pip install dask[complete]</code> zarr Cloud-optimized storage <code>pip install zarr</code>"},{"location":"getting-started/setup/#data-access","title":"Data Access","text":"Package Purpose Installation pystac-client STAC API client <code>pip install pystac-client</code> odc-stac Load STAC to XArray <code>pip install odc-stac</code> xee Earth Engine integration <code>pip install xee</code> earthengine-api Earth Engine Python API <code>pip install earthengine-api</code>"},{"location":"getting-started/setup/#visualization","title":"Visualization","text":"Package Purpose Installation matplotlib Static plots <code>pip install matplotlib</code> hvplot Interactive plots <code>pip install hvplot</code> folium Interactive maps <code>pip install folium</code>"},{"location":"getting-started/setup/#optional-but-useful","title":"Optional but Useful","text":"<pre><code>pip install geopandas shapely fiona pyproj jupyter jupyterlab ipywidgets\n</code></pre>"},{"location":"getting-started/setup/#verification","title":"Verification","text":"<p>Test your installation with this script:</p> <pre><code>import sys\nprint(f\"Python version: {sys.version}\")\n\n# Test imports\ntry:\n    import xarray as xr\n    print(f\"\u2713 xarray {xr.__version__}\")\nexcept ImportError:\n    print(\"\u2717 xarray not found\")\n\ntry:\n    import rioxarray\n    print(f\"\u2713 rioxarray installed\")\nexcept ImportError:\n    print(\"\u2717 rioxarray not found\")\n\ntry:\n    import pystac_client\n    print(f\"\u2713 pystac-client installed\")\nexcept ImportError:\n    print(\"\u2717 pystac-client not found\")\n\ntry:\n    import dask\n    print(f\"\u2713 dask {dask.__version__}\")\nexcept ImportError:\n    print(\"\u2717 dask not found\")\n\ntry:\n    import zarr\n    print(f\"\u2713 zarr {zarr.__version__}\")\nexcept ImportError:\n    print(\"\u2717 zarr not found\")\n\ntry:\n    import xee\n    print(f\"\u2713 xee installed\")\nexcept ImportError:\n    print(\"\u2717 xee not found\")\n\nprint(\"\\n\u2705 All core packages installed successfully!\")\n</code></pre>"},{"location":"getting-started/setup/#setting-up-earth-engine-optional","title":"Setting Up Earth Engine (Optional)","text":"<p>If you want to use Google Earth Engine:</p> <ol> <li>Create an Earth Engine Account</li> <li>Visit earthengine.google.com</li> <li> <p>Sign up for access</p> </li> <li> <p>Authenticate</p> </li> </ol> <pre><code>import ee\n\n# Authenticate (first time only)\nee.Authenticate()\n\n# Initialize\nee.Initialize(project='spatialgeography')\n</code></pre>"},{"location":"getting-started/setup/#ide-recommendations","title":"IDE Recommendations","text":""},{"location":"getting-started/setup/#jupyter-lab","title":"Jupyter Lab","text":"<p>Best for interactive data exploration:</p> <pre><code>pip install jupyterlab\njupyter lab\n</code></pre>"},{"location":"getting-started/setup/#vs-code","title":"VS Code","text":"<p>Great for development with extensions:</p> <ul> <li>Python</li> <li>Jupyter</li> <li>Remote Development</li> <li>GitLens</li> </ul>"},{"location":"getting-started/setup/#pycharm","title":"PyCharm","text":"<p>Full-featured IDE for Python development</p>"},{"location":"getting-started/setup/#cloud-platforms","title":"Cloud Platforms","text":""},{"location":"getting-started/setup/#aws","title":"AWS","text":"<ul> <li>EC2 for compute</li> <li>S3 for storage</li> <li>SageMaker for ML</li> </ul>"},{"location":"getting-started/setup/#google-cloud","title":"Google Cloud","text":"<ul> <li>Compute Engine</li> <li>Cloud Storage</li> <li>Earth Engine</li> </ul>"},{"location":"getting-started/setup/#microsoft-azure","title":"Microsoft Azure","text":"<ul> <li>Virtual Machines</li> <li>Blob Storage</li> <li>Planetary Computer</li> </ul>"},{"location":"getting-started/setup/#directory-structure","title":"Directory Structure","text":"<p>Organize your projects:</p> <pre><code>remote-sensing-project/\n\u251c\u2500\u2500 data/              # Local data cache\n\u251c\u2500\u2500 notebooks/         # Jupyter notebooks\n\u251c\u2500\u2500 scripts/           # Python scripts\n\u251c\u2500\u2500 outputs/           # Results and figures\n\u251c\u2500\u2500 environment.yml    # Conda environment\n\u2514\u2500\u2500 requirements.txt   # Pip requirements\n</code></pre>"},{"location":"getting-started/setup/#best-practices","title":"Best Practices","text":""},{"location":"getting-started/setup/#1-use-virtual-environments","title":"1. Use Virtual Environments","text":"<p>Always isolate your project dependencies:</p> <pre><code>conda create -n project-name python=3.10\n</code></pre>"},{"location":"getting-started/setup/#2-pin-package-versions","title":"2. Pin Package Versions","text":"<p>Create reproducible environments:</p> <pre><code># requirements.txt\nxarray==2023.12.0\ndask==2023.12.0\nzarr==2.16.1\n</code></pre>"},{"location":"getting-started/setup/#3-use-git-for-version-control","title":"3. Use Git for Version Control","text":"<pre><code>git init\ngit add .\ngit commit -m \"Initial commit\"\n</code></pre>"},{"location":"getting-started/setup/#4-document-your-environment","title":"4. Document Your Environment","text":"<pre><code># Export conda environment\nconda env export &gt; environment.yml\n\n# Export pip requirements\npip freeze &gt; requirements.txt\n</code></pre>"},{"location":"getting-started/setup/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/setup/#common-issues","title":"Common Issues","text":"<p>1. Import Errors</p> <pre><code># Solution: Reinstall package\npip install --upgrade --force-reinstall package-name\n</code></pre> <p>2. Memory Errors</p> <pre><code># Solution: Use Dask chunking\nds = xr.open_dataset('file.nc', chunks={'time': 10})\n</code></pre> <p>3. GDAL/Rasterio Issues</p> <pre><code># Solution: Use conda for GDAL\nconda install -c conda-forge gdal rasterio\n</code></pre>"},{"location":"getting-started/setup/#performance-tips","title":"Performance Tips","text":""},{"location":"getting-started/setup/#1-configure-dask","title":"1. Configure Dask","text":"<pre><code>import dask\ndask.config.set({'array.slicing.split_large_chunks': True})\n</code></pre>"},{"location":"getting-started/setup/#2-set-chunk-sizes","title":"2. Set Chunk Sizes","text":"<pre><code># Good chunking\nds = xr.open_zarr('data.zarr', chunks={'time': 10, 'x': 512, 'y': 512})\n\n# Avoid too small or too large chunks\n</code></pre>"},{"location":"getting-started/setup/#3-use-local-dask-cluster","title":"3. Use Local Dask Cluster","text":"<pre><code>from dask.distributed import Client, LocalCluster\n\ncluster = LocalCluster(n_workers=4, threads_per_worker=2)\nclient = Client(cluster)\n</code></pre>"},{"location":"getting-started/setup/#next-steps","title":"Next Steps","text":"<p>Now that your environment is set up:</p> <p>\u2192 Continue to Google Colab Basics</p>"},{"location":"getting-started/setup/#additional-resources","title":"Additional Resources","text":"<ul> <li>Conda Cheat Sheet</li> <li>Pip User Guide</li> <li>Jupyter Documentation</li> <li>Dask Best Practices</li> </ul>"},{"location":"getting-started/setup/#quick-reference","title":"Quick Reference","text":""},{"location":"getting-started/setup/#activate-environment","title":"Activate Environment","text":"<pre><code># Conda\nconda activate remote-sensing\n\n# Pip/venv\nsource remote-sensing-env/bin/activate  # Linux/Mac\nremote-sensing-env\\Scripts\\activate     # Windows\n</code></pre>"},{"location":"getting-started/setup/#update-packages","title":"Update Packages","text":"<pre><code># Conda\nconda update --all\n\n# Pip\npip install --upgrade package-name\n</code></pre>"},{"location":"getting-started/setup/#list-installed-packages","title":"List Installed Packages","text":"<pre><code># Conda\nconda list\n\n# Pip\npip list\n</code></pre>"},{"location":"processing/aggregation/","title":"Data Aggregation","text":""},{"location":"processing/aggregation/#data-aggregation","title":"Data Aggregation","text":"<p>Aggregating remote sensing data across spatial and temporal dimensions is essential for regional analysis, climate studies, and change detection.</p>"},{"location":"processing/aggregation/#overview","title":"Overview","text":"<p>Learn how to:</p> <ul> <li>Aggregate data spatially (zonal statistics)</li> <li>Aggregate data temporally (composites)</li> <li>Combine spatial and temporal aggregation</li> <li>Use groupby operations for categorical analysis</li> </ul>"},{"location":"processing/aggregation/#spatial-aggregation","title":"Spatial Aggregation","text":""},{"location":"processing/aggregation/#zonal-statistics","title":"Zonal Statistics","text":"<p>Calculate statistics within zones:</p> <pre><code>import xarray as xr\nimport geopandas as gpd\n\n# Load data\nds = xr.open_zarr('ndvi_data.zarr')\n\n# Load zones (e.g., administrative boundaries)\nzones = gpd.read_file('districts.geojson')\n\n# For each zone, calculate mean NDVI\nresults = []\nfor idx, zone in zones.iterrows():\n    # Clip to zone\n    clipped = ds.rio.clip([zone.geometry], zones.crs)\n\n    # Calculate mean\n    mean_ndvi = clipped.NDVI.mean(dim=['x', 'y']).values\n\n    results.append({\n        'zone_id': zone['id'],\n        'zone_name': zone['name'],\n        'mean_ndvi': mean_ndvi\n    })\n\n# Convert to DataFrame\nimport pandas as pd\nstats_df = pd.DataFrame(results)\nprint(stats_df)\n</code></pre>"},{"location":"processing/aggregation/#grid-based-aggregation","title":"Grid-based Aggregation","text":"<p>Aggregate to coarser resolution:</p> <pre><code># Coarsen by factor of 10\ncoarse = ds.coarsen(x=10, y=10, boundary='trim').mean()\n\n# Visualize comparison\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\nds.NDVI.isel(time=0).plot(ax=axes[0])\naxes[0].set_title('Original (10m)')\n\ncoarse.NDVI.isel(time=0).plot(ax=axes[1])\naxes[1].set_title('Aggregated (100m)')\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"processing/aggregation/#temporal-aggregation","title":"Temporal Aggregation","text":""},{"location":"processing/aggregation/#composites","title":"Composites","text":"<p>Create temporal composites:</p> <pre><code># Monthly median composite\nmonthly_composite = ds.resample(time='1M').median()\n\n# Annual maximum composite\nannual_max = ds.resample(time='1Y').max()\n\n# Seasonal composites\nseasonal = ds.groupby('time.season').mean()\n\n# Visualize seasonal patterns\nfig, axes = plt.subplots(2, 2, figsize=(12, 12))\nseasons = ['DJF', 'MAM', 'JJA', 'SON']\n\nfor idx, season in enumerate(seasons):\n    ax = axes[idx // 2, idx % 2]\n    if season in seasonal.season:\n        seasonal.sel(season=season).NDVI.plot(ax=ax, cmap='RdYlGn')\n        ax.set_title(f'{season} Mean NDVI')\n        ax.set_axis_off()\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"processing/aggregation/#groupby-operations","title":"GroupBy Operations","text":""},{"location":"processing/aggregation/#group-by-categories","title":"Group by Categories","text":"<pre><code># Group by month\nmonthly_stats = ds.groupby('time.month').mean()\n\n# Plot monthly climatology\nmonthly_stats.NDVI.mean(dim=['x', 'y']).plot(marker='o')\nplt.title('Monthly NDVI Climatology')\nplt.xlabel('Month')\nplt.ylabel('Mean NDVI')\nplt.grid(True, alpha=0.3)\nplt.show()\n</code></pre>"},{"location":"processing/aggregation/#custom-grouping","title":"Custom Grouping","text":"<pre><code># Define seasons manually\ndef get_season(month):\n    if month in [12, 1, 2]:\n        return 'Winter'\n    elif month in [3, 4, 5]:\n        return 'Spring'\n    elif month in [6, 7, 8]:\n        return 'Summer'\n    else:\n        return 'Fall'\n\n# Apply custom grouping\nseasons = xr.DataArray(\n    [get_season(m) for m in ds.time.dt.month.values],\n    dims='time',\n    coords={'time': ds.time}\n)\n\nseasonal_custom = ds.groupby(seasons).mean()\n</code></pre>"},{"location":"processing/aggregation/#combined-aggregation","title":"Combined Aggregation","text":""},{"location":"processing/aggregation/#spatio-temporal-aggregation","title":"Spatio-temporal Aggregation","text":"<pre><code># Calculate regional monthly means\nregional_monthly = ds.sel(\n    x=slice(82.0, 83.0),\n    y=slice(26.5, 27.5)\n).resample(time='1M').mean().mean(dim=['x', 'y'])\n\n# Plot\nregional_monthly.NDVI.plot(marker='o')\nplt.title('Regional Monthly Mean NDVI')\nplt.ylabel('NDVI')\nplt.grid(True, alpha=0.3)\nplt.show()\n</code></pre>"},{"location":"processing/aggregation/#performance-tips","title":"Performance Tips","text":""},{"location":"processing/aggregation/#use-dask-for-large-datasets","title":"Use Dask for Large Datasets","text":"<pre><code>from dask.distributed import Client\n\nclient = Client()\n\n# Load with chunks\nds = xr.open_zarr('large_data.zarr', chunks={'time': 10, 'x': 512, 'y': 512})\n\n# Aggregate (lazy)\nmonthly = ds.resample(time='1M').mean()\n\n# Compute\nresult = monthly.compute()\n</code></pre>"},{"location":"processing/aggregation/#next-steps","title":"Next Steps","text":"<p>\u2192 Continue to Advanced Topics</p>"},{"location":"processing/aggregation/#additional-resources","title":"Additional Resources","text":"<ul> <li>XArray GroupBy</li> <li>Rasterio Zonal Stats</li> </ul>"},{"location":"processing/cloud-masking/","title":"Cloud Masking","text":""},{"location":"processing/cloud-masking/#cloud-masking","title":"Cloud Masking","text":""},{"location":"processing/cloud-masking/#overview","title":"Overview","text":"<p>Clouds are a major challenge in optical remote sensing. They obscure the Earth's surface and can introduce errors in analysis. Proper cloud masking is essential for accurate results.</p> <p>In this section, we'll learn how to:</p> <ul> <li>Use quality assessment (QA) bands for cloud detection</li> <li>Create cloud masks using bit manipulation</li> <li>Apply masks to remove cloudy pixels</li> <li>Combine cloud and shadow masks</li> <li>Handle partially cloudy scenes</li> </ul>"},{"location":"processing/cloud-masking/#understanding-qa-bands","title":"Understanding QA Bands","text":"<p>Most satellite products include Quality Assessment (QA) bands that contain pixel-level quality information encoded as bit flags.</p>"},{"location":"processing/cloud-masking/#sentinel-2-qa60-band","title":"Sentinel-2 QA60 Band","text":"<p>The QA60 band contains cloud information:</p> <ul> <li>Bit 10: Opaque clouds</li> <li>Bit 11: Cirrus clouds</li> </ul>"},{"location":"processing/cloud-masking/#landsat-qa_pixel-band","title":"Landsat QA_PIXEL Band","text":"<p>Contains multiple quality flags:</p> <ul> <li>Bit 3: Cloud</li> <li>Bit 4: Cloud shadow</li> <li>Bit 1: Dilated cloud</li> </ul>"},{"location":"processing/cloud-masking/#bit-manipulation","title":"Bit Manipulation","text":"<p>Quality flags are stored as binary values. We use bitwise operations to extract specific flags.</p>"},{"location":"processing/cloud-masking/#binary-representation","title":"Binary Representation","text":"<pre><code># Example: Number 1024 in binary\nbinary = bin(1024)\nprint(binary)  # '0b10000000000'\n\n# Bit 10 is set (counting from right, starting at 0)\n</code></pre>"},{"location":"processing/cloud-masking/#bitwise-and-operation","title":"Bitwise AND Operation","text":"<pre><code># Check if bit 10 is set\nvalue = 1024\nbit_10_mask = 1 &lt;&lt; 10  # Shift 1 left by 10 positions = 1024\nresult = value &amp; bit_10_mask\nprint(result)  # 1024 (bit is set)\n\n# Check if bit 11 is set\nbit_11_mask = 1 &lt;&lt; 11  # 2048\nresult = value &amp; bit_11_mask\nprint(result)  # 0 (bit is not set)\n</code></pre>"},{"location":"processing/cloud-masking/#setup","title":"Setup","text":"<pre><code>%%capture\nif 'google.colab' in str(get_ipython()):\n    !pip install pystac-client odc-stac rioxarray dask\n</code></pre> <pre><code>import os\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport pystac_client\nfrom odc import stac\nimport xarray as xr\nimport rioxarray as rxr\n</code></pre>"},{"location":"processing/cloud-masking/#get-sentinel-2-scene","title":"Get Sentinel-2 Scene","text":"<pre><code>latitude = 27.163\nlongitude = 82.608\nyear = 2023\n\n# Define bounding box\nkm2deg = 1.0 / 111\nx, y = (longitude, latitude)\nr = 1 * km2deg\nbbox = (x - r, y - r, x + r, y + r)\n\n# Query STAC Catalog\ncatalog = pystac_client.Client.open(\n    'https://earth-search.aws.element84.com/v1')\n\n# Search for cloudy scenes to demonstrate masking\nsearch = catalog.search(\n    collections=['sentinel-2-c1-l2a'],\n    bbox=bbox,\n    datetime=f'{year}',\n    query={\n        'eo:cloud_cover': {'gt': 20, 'lt': 60}  # Moderately cloudy\n    },\n    sortby=[{\n        'field': 'properties.eo:cloud_cover',\n        'direction': 'asc'\n    }]\n)\nitems = search.item_collection()\n\n# Load data including QA band\nds = stac.load(\n    items,\n    bands=['red', 'green', 'blue', 'nir', 'qa60'],  # Include QA band\n    resolution=20,\n    chunks={},\n    groupby='solar_day',\n    preserve_original_order=True\n)\n\n# Select a scene\ntimestamp = pd.to_datetime(items[0].properties['datetime']).tz_convert(None)\nscene = ds.sel(time=timestamp).compute()\n\n# Apply scale and offset (not needed for QA band)\nscale = 0.0001\noffset = -0.1\nscene_scaled = scene[['red', 'green', 'blue', 'nir']].where(\n    scene[['red', 'green', 'blue', 'nir']] != 0) * scale + offset\n\n# Add QA band back\nscene_scaled['qa60'] = scene.qa60\nscene = scene_scaled\n</code></pre>"},{"location":"processing/cloud-masking/#visualize-the-scene","title":"Visualize the Scene","text":"<pre><code>scene_da = scene[['red', 'green', 'blue']].to_array('band')\n\nfig, ax = plt.subplots(figsize=(10, 8))\nscene_da.plot.imshow(ax=ax, robust=True)\nax.set_title('Original Scene (with clouds)')\nax.set_axis_off()\nplt.show()\n</code></pre>"},{"location":"processing/cloud-masking/#create-a-cloud-mask","title":"Create a Cloud Mask","text":""},{"location":"processing/cloud-masking/#extract-cloud-bits","title":"Extract Cloud Bits","text":"<pre><code># Get QA band\nqa = scene.qa60\n\n# Create masks for opaque and cirrus clouds\nopaque_clouds = (qa &amp; (1 &lt;&lt; 10)) != 0\ncirrus_clouds = (qa &amp; (1 &lt;&lt; 11)) != 0\n\n# Combine cloud masks\ncloud_mask = opaque_clouds | cirrus_clouds\n\n# Visualize cloud mask\nfig, ax = plt.subplots(figsize=(10, 8))\ncloud_mask.plot(ax=ax, cmap='RdYlGn_r', add_colorbar=False)\nax.set_title('Cloud Mask (True = Cloud)')\nax.set_axis_off()\nplt.show()\n</code></pre>"},{"location":"processing/cloud-masking/#apply-cloud-mask","title":"Apply Cloud Mask","text":"<pre><code># Invert mask (True = clear, False = cloud)\nclear_mask = ~cloud_mask\n\n# Apply mask to scene\nscene_masked = scene.where(clear_mask)\n\n# Visualize masked scene\nscene_masked_da = scene_masked[['red', 'green', 'blue']].to_array('band')\n\nfig, ax = plt.subplots(figsize=(10, 8))\nscene_masked_da.plot.imshow(ax=ax, robust=True)\nax.set_title('Cloud-Masked Scene')\nax.set_axis_off()\nplt.show()\n</code></pre>"},{"location":"processing/cloud-masking/#beforeafter-comparison","title":"Before/After Comparison","text":"<pre><code>fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n\n# Original\nscene_da.plot.imshow(ax=axes[0], robust=True)\naxes[0].set_title('Original (with clouds)')\naxes[0].set_axis_off()\n\n# Cloud mask\ncloud_mask.plot(ax=axes[1], cmap='RdYlGn_r', add_colorbar=False)\naxes[1].set_title('Cloud Mask')\naxes[1].set_axis_off()\n\n# Masked\nscene_masked_da.plot.imshow(ax=axes[2], robust=True)\naxes[2].set_title('Cloud-Masked')\naxes[2].set_axis_off()\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"processing/cloud-masking/#cloud-statistics","title":"Cloud Statistics","text":"<pre><code># Calculate cloud coverage\ntotal_pixels = cloud_mask.size\ncloud_pixels = cloud_mask.sum().values\ncloud_percentage = (cloud_pixels / total_pixels) * 100\n\nprint(f\"Total pixels: {total_pixels:,}\")\nprint(f\"Cloud pixels: {cloud_pixels:,}\")\nprint(f\"Cloud coverage: {cloud_percentage:.2f}%\")\n\n# Compare with metadata\nmetadata_cloud_cover = items[0].properties.get('eo:cloud_cover', 'N/A')\nprint(f\"Metadata cloud cover: {metadata_cloud_cover}%\")\n</code></pre>"},{"location":"processing/cloud-masking/#advanced-cloud-masking","title":"Advanced Cloud Masking","text":""},{"location":"processing/cloud-masking/#morphological-operations","title":"Morphological Operations","text":"<p>Dilate cloud mask to include cloud edges:</p> <pre><code>from scipy import ndimage\n\n# Convert to numpy array\ncloud_array = cloud_mask.values\n\n# Dilate cloud mask (expand clouds)\nstructure = np.ones((5, 5))  # 5x5 kernel\ndilated_clouds = ndimage.binary_dilation(cloud_array, structure=structure)\n\n# Convert back to DataArray\ndilated_mask = xr.DataArray(\n    dilated_clouds,\n    coords=cloud_mask.coords,\n    dims=cloud_mask.dims\n)\n\n# Apply dilated mask\nscene_dilated_masked = scene.where(~dilated_mask)\n\n# Visualize\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\n\ncloud_mask.plot(ax=axes[0], cmap='RdYlGn_r', add_colorbar=False)\naxes[0].set_title('Original Cloud Mask')\naxes[0].set_axis_off()\n\ndilated_mask.plot(ax=axes[1], cmap='RdYlGn_r', add_colorbar=False)\naxes[1].set_title('Dilated Cloud Mask')\naxes[1].set_axis_off()\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"processing/cloud-masking/#cloud-shadow-detection","title":"Cloud Shadow Detection","text":"<p>Estimate cloud shadows using geometry:</p> <pre><code># Simple cloud shadow detection using NIR threshold\nnir_threshold = 0.15\npotential_shadows = scene.nir &lt; nir_threshold\n\n# Combine with cloud mask (shadows are near clouds)\n# Shift cloud mask to approximate shadow location\nshadow_offset = 10  # pixels\ncloud_shifted = np.roll(cloud_array, shadow_offset, axis=0)\ncloud_shifted = np.roll(cloud_shifted, shadow_offset, axis=1)\n\n# Shadows are dark areas near clouds\nshadow_mask = potential_shadows.values &amp; cloud_shifted &amp; ~cloud_array\n\n# Convert to DataArray\nshadow_mask_da = xr.DataArray(\n    shadow_mask,\n    coords=cloud_mask.coords,\n    dims=cloud_mask.dims\n)\n\n# Visualize\nfig, ax = plt.subplots(figsize=(10, 8))\nshadow_mask_da.plot(ax=ax, cmap='gray', add_colorbar=False)\nax.set_title('Estimated Cloud Shadows')\nax.set_axis_off()\nplt.show()\n</code></pre>"},{"location":"processing/cloud-masking/#combined-cloud-and-shadow-mask","title":"Combined Cloud and Shadow Mask","text":"<pre><code># Combine cloud and shadow masks\ncombined_mask = cloud_mask | shadow_mask_da\n\n# Apply combined mask\nscene_fully_masked = scene.where(~combined_mask)\n\n# Visualize\nscene_fully_masked_da = scene_fully_masked[['red', 'green', 'blue']].to_array('band')\n\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\n\nscene_masked_da.plot.imshow(ax=axes[0], robust=True)\naxes[0].set_title('Cloud-Masked Only')\naxes[0].set_axis_off()\n\nscene_fully_masked_da.plot.imshow(ax=axes[1], robust=True)\naxes[1].set_title('Cloud and Shadow Masked')\naxes[1].set_axis_off()\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"processing/cloud-masking/#time-series-cloud-masking","title":"Time Series Cloud Masking","text":"<pre><code># Load full time series\nds_full = stac.load(\n    items,\n    bands=['red', 'nir', 'qa60'],\n    resolution=20,\n    chunks={'time': 10}\n).compute()\n\n# Apply scale/offset\nds_scaled = ds_full[['red', 'nir']].where(ds_full[['red', 'nir']] != 0) * scale + offset\nds_scaled['qa60'] = ds_full.qa60\n\n# Create cloud masks for all time steps\nqa_full = ds_scaled.qa60\ncloud_masks = (qa_full &amp; (1 &lt;&lt; 10)) != 0 | (qa_full &amp; (1 &lt;&lt; 11)) != 0\n\n# Apply masks\nds_masked = ds_scaled.where(~cloud_masks)\n\n# Calculate NDVI\nndvi = (ds_masked.nir - ds_masked.red) / (ds_masked.nir + ds_masked.red)\n\n# Calculate temporal mean (ignoring masked pixels)\nndvi_mean = ndvi.mean(dim='time')\n\n# Visualize\nfig, ax = plt.subplots(figsize=(10, 8))\nndvi_mean.plot(ax=ax, cmap='RdYlGn', vmin=-1, vmax=1)\nax.set_title('Mean NDVI (Cloud-Masked Time Series)')\nax.set_axis_off()\nplt.show()\n</code></pre>"},{"location":"processing/cloud-masking/#quality-metrics","title":"Quality Metrics","text":"<pre><code># Calculate valid pixel percentage for each time step\nvalid_pixels = (~cloud_masks).sum(dim=['x', 'y'])\ntotal_pixels = cloud_masks.sizes['x'] * cloud_masks.sizes['y']\nvalid_percentage = (valid_pixels / total_pixels) * 100\n\n# Plot\nfig, ax = plt.subplots(figsize=(12, 6))\nvalid_percentage.plot(ax=ax, marker='o')\nax.set_title('Valid (Cloud-Free) Pixel Percentage Over Time')\nax.set_ylabel('Valid Pixels (%)')\nax.grid(True, alpha=0.3)\nax.axhline(y=50, color='r', linestyle='--', label='50% threshold')\nax.legend()\nplt.show()\n</code></pre>"},{"location":"processing/cloud-masking/#exercise","title":"Exercise","text":"<p>Create a function that takes a Sentinel-2 scene and returns a cloud-free composite by:</p> <ol> <li>Creating a cloud mask</li> <li>Dilating the mask by 3 pixels</li> <li>Applying the mask to the scene</li> <li>Calculating the percentage of valid pixels</li> </ol> <p>Solution:</p> <pre><code>def create_cloud_free_scene(scene, dilation_size=3):\n    \"\"\"\n    Create cloud-free scene from Sentinel-2 data.\n\n    Parameters:\n    -----------\n    scene : xarray.Dataset\n        Sentinel-2 scene with qa60 band\n    dilation_size : int\n        Size of dilation kernel\n\n    Returns:\n    --------\n    masked_scene : xarray.Dataset\n        Cloud-masked scene\n    valid_percentage : float\n        Percentage of valid pixels\n    \"\"\"\n    # Extract QA band\n    qa = scene.qa60\n\n    # Create cloud mask\n    cloud_mask = (qa &amp; (1 &lt;&lt; 10)) != 0 | (qa &amp; (1 &lt;&lt; 11)) != 0\n\n    # Dilate mask\n    structure = np.ones((dilation_size, dilation_size))\n    dilated = ndimage.binary_dilation(cloud_mask.values, structure=structure)\n    dilated_mask = xr.DataArray(dilated, coords=cloud_mask.coords, dims=cloud_mask.dims)\n\n    # Apply mask\n    masked_scene = scene.where(~dilated_mask)\n\n    # Calculate valid percentage\n    valid_pixels = (~dilated_mask).sum().values\n    total_pixels = dilated_mask.size\n    valid_percentage = (valid_pixels / total_pixels) * 100\n\n    return masked_scene, valid_percentage\n\n# Use function\nmasked, valid_pct = create_cloud_free_scene(scene, dilation_size=3)\nprint(f\"Valid pixels: {valid_pct:.2f}%\")\n</code></pre>"},{"location":"processing/cloud-masking/#key-takeaways","title":"Key Takeaways","text":"<p>What You Learned</p> <ul> <li>QA bands contain pixel-level quality information</li> <li>Bitwise operations extract specific quality flags</li> <li>Cloud masks identify and remove cloudy pixels</li> <li>Morphological operations refine masks</li> <li>Cloud shadows can be detected and masked</li> <li>Time series masking improves composite quality</li> <li>Valid pixel percentage indicates data quality</li> </ul>"},{"location":"processing/cloud-masking/#next-steps","title":"Next Steps","text":"<p>\u2192 Continue to Time Series Extraction</p>"},{"location":"processing/cloud-masking/#additional-resources","title":"Additional Resources","text":"<ul> <li>Sentinel-2 Cloud Masks</li> <li>Landsat QA Bands</li> <li>Cloud Masking Algorithms</li> <li>S2cloudless</li> </ul>"},{"location":"processing/spectral-indices/","title":"Calculating Spectral Indices","text":""},{"location":"processing/spectral-indices/#calculating-spectral-indices","title":"Calculating Spectral Indices","text":""},{"location":"processing/spectral-indices/#overview","title":"Overview","text":"<p>Spectral indices are core to many remote sensing analyses. They combine different spectral bands to highlight specific features or properties of the Earth's surface.</p> <p>In this section, we'll learn how to:</p> <ul> <li>Calculate common spectral indices (NDVI, NDWI, SAVI)</li> <li>Visualize index results</li> <li>Save computed indices</li> <li>Create custom indices</li> </ul>"},{"location":"processing/spectral-indices/#common-spectral-indices","title":"Common Spectral Indices","text":""},{"location":"processing/spectral-indices/#ndvi-normalized-difference-vegetation-index","title":"NDVI (Normalized Difference Vegetation Index)","text":"<p>Measures vegetation health and density:</p> \\[NDVI = \\frac{NIR - Red}{NIR + Red}\\] <ul> <li>Range: -1 to +1</li> <li>Interpretation:</li> <li>&lt; 0: Water, snow, clouds</li> <li>0-0.2: Bare soil, rock</li> <li>0.2-0.5: Sparse vegetation</li> <li>0.5-0.8: Moderate vegetation</li> <li> <p>0.8: Dense vegetation</p> </li> </ul>"},{"location":"processing/spectral-indices/#ndwi-normalized-difference-water-index","title":"NDWI (Normalized Difference Water Index)","text":"<p>Detects water bodies and moisture:</p> \\[NDWI = \\frac{Green - NIR}{Green + NIR}\\] <ul> <li>Range: -1 to +1</li> <li>Interpretation:</li> <li> <p>0: Water</p> </li> <li>&lt; 0: Non-water</li> </ul>"},{"location":"processing/spectral-indices/#mndwi-modified-ndwi","title":"MNDWI (Modified NDWI)","text":"<p>Better for urban areas:</p> \\[MNDWI = \\frac{Green - SWIR}{Green + SWIR}\\]"},{"location":"processing/spectral-indices/#savi-soil-adjusted-vegetation-index","title":"SAVI (Soil Adjusted Vegetation Index)","text":"<p>Minimizes soil brightness influence:</p> \\[SAVI = \\frac{(NIR - Red) \\times (1 + L)}{NIR + Red + L}\\] <p>Where L = 0.5 (soil brightness correction factor)</p>"},{"location":"processing/spectral-indices/#evi-enhanced-vegetation-index","title":"EVI (Enhanced Vegetation Index)","text":"<p>Improved sensitivity in high biomass regions:</p> \\[EVI = 2.5 \\times \\frac{NIR - Red}{NIR + 6 \\times Red - 7.5 \\times Blue + 1}\\]"},{"location":"processing/spectral-indices/#ndbi-normalized-difference-built-up-index","title":"NDBI (Normalized Difference Built-up Index)","text":"<p>Identifies built-up areas:</p> \\[NDBI = \\frac{SWIR - NIR}{SWIR + NIR}\\]"},{"location":"processing/spectral-indices/#setup","title":"Setup","text":"<pre><code>%%capture\nif 'google.colab' in str(get_ipython()):\n    !pip install pystac-client odc-stac rioxarray dask jupyter-server-proxy\n</code></pre> <pre><code>import os\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport pystac_client\nfrom odc import stac\nimport xarray as xr\nimport rioxarray as rxr\nfrom dask.distributed import Client\n\nclient = Client()\nclient\n</code></pre>"},{"location":"processing/spectral-indices/#get-sentinel-2-scene","title":"Get Sentinel-2 Scene","text":"<pre><code>latitude = 27.163\nlongitude = 82.608\nyear = 2023\n\n# Define bounding box\nkm2deg = 1.0 / 111\nx, y = (longitude, latitude)\nr = 1 * km2deg\nbbox = (x - r, y - r, x + r, y + r)\n\n# Query STAC Catalog\ncatalog = pystac_client.Client.open(\n    'https://earth-search.aws.element84.com/v1')\n\nsearch = catalog.search(\n    collections=['sentinel-2-c1-l2a'],\n    bbox=bbox,\n    datetime=f'{year}',\n    query={\n        'eo:cloud_cover': {'lt': 30},\n        's2:nodata_pixel_percentage': {'lt': 10}\n    },\n    sortby=[{\n        'field': 'properties.eo:cloud_cover',\n        'direction': 'asc'\n    }]\n)\nitems = search.item_collection()\n\n# Load data\nds = stac.load(\n    items,\n    bands=['blue', 'green', 'red', 'nir', 'swir16'],\n    resolution=10,\n    chunks={},\n    groupby='solar_day',\n    preserve_original_order=True\n)\n\n# Select least cloudy scene\ntimestamp = pd.to_datetime(items[0].properties['datetime']).tz_convert(None)\nscene = ds.sel(time=timestamp).compute()\n\n# Apply scale and offset\nscale = 0.0001\noffset = -0.1\nscene = scene.where(scene != 0) * scale + offset\n</code></pre>"},{"location":"processing/spectral-indices/#visualize-the-scene","title":"Visualize the Scene","text":"<pre><code>scene_da = scene.to_array('band')\n\n# Create RGB preview\npreview = scene_da.rio.reproject(scene_da.rio.crs, resolution=300)\n\nfig, ax = plt.subplots(1, 1, figsize=(8, 8))\npreview.sel(band=['red', 'green', 'blue']).plot.imshow(\n    ax=ax, robust=True)\nax.set_title('True Color Composite')\nax.set_axis_off()\nax.set_aspect('equal')\nplt.show()\n</code></pre>"},{"location":"processing/spectral-indices/#calculate-spectral-indices","title":"Calculate Spectral Indices","text":""},{"location":"processing/spectral-indices/#ndvi","title":"NDVI","text":"<pre><code># Calculate NDVI\nndvi = (scene.nir - scene.red) / (scene.nir + scene.red)\n\n# Visualize\nfig, ax = plt.subplots(figsize=(10, 8))\nndvi.plot(ax=ax, cmap='RdYlGn', vmin=-1, vmax=1, cbar_kwargs={'label': 'NDVI'})\nax.set_title('NDVI - Normalized Difference Vegetation Index')\nax.set_axis_off()\nplt.show()\n</code></pre>"},{"location":"processing/spectral-indices/#ndwi","title":"NDWI","text":"<pre><code># Calculate NDWI\nndwi = (scene.green - scene.nir) / (scene.green + scene.nir)\n\n# Visualize\nfig, ax = plt.subplots(figsize=(10, 8))\nndwi.plot(ax=ax, cmap='Blues', vmin=-1, vmax=1, cbar_kwargs={'label': 'NDWI'})\nax.set_title('NDWI - Normalized Difference Water Index')\nax.set_axis_off()\nplt.show()\n</code></pre>"},{"location":"processing/spectral-indices/#mndwi","title":"MNDWI","text":"<pre><code># Calculate MNDWI\nmndwi = (scene.green - scene.swir16) / (scene.green + scene.swir16)\n\n# Visualize\nfig, ax = plt.subplots(figsize=(10, 8))\nmndwi.plot(ax=ax, cmap='Blues', vmin=-1, vmax=1, cbar_kwargs={'label': 'MNDWI'})\nax.set_title('MNDWI - Modified Normalized Difference Water Index')\nax.set_axis_off()\nplt.show()\n</code></pre>"},{"location":"processing/spectral-indices/#savi","title":"SAVI","text":"<pre><code># Calculate SAVI\nL = 0.5  # Soil brightness correction factor\nsavi = ((scene.nir - scene.red) * (1 + L)) / (scene.nir + scene.red + L)\n\n# Visualize\nfig, ax = plt.subplots(figsize=(10, 8))\nsavi.plot(ax=ax, cmap='RdYlGn', vmin=-1, vmax=1, cbar_kwargs={'label': 'SAVI'})\nax.set_title('SAVI - Soil Adjusted Vegetation Index')\nax.set_axis_off()\nplt.show()\n</code></pre>"},{"location":"processing/spectral-indices/#evi","title":"EVI","text":"<pre><code># Calculate EVI\nevi = 2.5 * ((scene.nir - scene.red) / \n             (scene.nir + 6 * scene.red - 7.5 * scene.blue + 1))\n\n# Visualize\nfig, ax = plt.subplots(figsize=(10, 8))\nevi.plot(ax=ax, cmap='RdYlGn', vmin=-1, vmax=1, cbar_kwargs={'label': 'EVI'})\nax.set_title('EVI - Enhanced Vegetation Index')\nax.set_axis_off()\nplt.show()\n</code></pre>"},{"location":"processing/spectral-indices/#ndbi","title":"NDBI","text":"<pre><code># Calculate NDBI\nndbi = (scene.swir16 - scene.nir) / (scene.swir16 + scene.nir)\n\n# Visualize\nfig, ax = plt.subplots(figsize=(10, 8))\nndbi.plot(ax=ax, cmap='YlOrRd', vmin=-1, vmax=1, cbar_kwargs={'label': 'NDBI'})\nax.set_title('NDBI - Normalized Difference Built-up Index')\nax.set_axis_off()\nplt.show()\n</code></pre>"},{"location":"processing/spectral-indices/#multi-index-comparison","title":"Multi-Index Comparison","text":"<pre><code># Create subplot for multiple indices\nfig, axes = plt.subplots(2, 3, figsize=(15, 10))\n\n# NDVI\nndvi.plot(ax=axes[0, 0], cmap='RdYlGn', vmin=-1, vmax=1, add_colorbar=False)\naxes[0, 0].set_title('NDVI (Vegetation)')\naxes[0, 0].set_axis_off()\n\n# NDWI\nndwi.plot(ax=axes[0, 1], cmap='Blues', vmin=-1, vmax=1, add_colorbar=False)\naxes[0, 1].set_title('NDWI (Water)')\naxes[0, 1].set_axis_off()\n\n# MNDWI\nmndwi.plot(ax=axes[0, 2], cmap='Blues', vmin=-1, vmax=1, add_colorbar=False)\naxes[0, 2].set_title('MNDWI (Water - Modified)')\naxes[0, 2].set_axis_off()\n\n# SAVI\nsavi.plot(ax=axes[1, 0], cmap='RdYlGn', vmin=-1, vmax=1, add_colorbar=False)\naxes[1, 0].set_title('SAVI (Vegetation - Soil Adjusted)')\naxes[1, 0].set_axis_off()\n\n# EVI\nevi.plot(ax=axes[1, 1], cmap='RdYlGn', vmin=-1, vmax=1, add_colorbar=False)\naxes[1, 1].set_title('EVI (Enhanced Vegetation)')\naxes[1, 1].set_axis_off()\n\n# NDBI\nndbi.plot(ax=axes[1, 2], cmap='YlOrRd', vmin=-1, vmax=1, add_colorbar=False)\naxes[1, 2].set_title('NDBI (Built-up)')\naxes[1, 2].set_axis_off()\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"processing/spectral-indices/#save-computed-indices","title":"Save Computed Indices","text":""},{"location":"processing/spectral-indices/#save-as-netcdf","title":"Save as NetCDF","text":"<pre><code># Combine indices into a dataset\nindices = xr.Dataset({\n    'ndvi': ndvi,\n    'ndwi': ndwi,\n    'mndwi': mndwi,\n    'savi': savi,\n    'evi': evi,\n    'ndbi': ndbi\n})\n\n# Add metadata\nindices.attrs['title'] = 'Spectral Indices'\nindices.attrs['source'] = 'Sentinel-2'\nindices.attrs['date'] = str(timestamp)\n\n# Save to NetCDF\noutput_file = 'spectral_indices.nc'\nindices.to_netcdf(output_file)\n</code></pre>"},{"location":"processing/spectral-indices/#save-as-geotiff","title":"Save as GeoTIFF","text":"<pre><code># Save individual indices\nndvi.rio.to_raster('ndvi.tif')\nndwi.rio.to_raster('ndwi.tif')\nmndwi.rio.to_raster('mndwi.tif')\n\n# Save multi-band GeoTIFF\nindices_da = indices.to_array('index')\nindices_da.rio.to_raster('all_indices.tif')\n</code></pre>"},{"location":"processing/spectral-indices/#save-to-zarr","title":"Save to Zarr","text":"<pre><code># Save to Zarr (cloud-optimized)\nindices.to_zarr('spectral_indices.zarr', mode='w', consolidated=True)\n</code></pre>"},{"location":"processing/spectral-indices/#custom-index-functions","title":"Custom Index Functions","text":"<p>Create reusable functions for indices:</p> <pre><code>def calculate_ndvi(nir, red):\n    \"\"\"Calculate NDVI.\"\"\"\n    return (nir - red) / (nir + red)\n\ndef calculate_ndwi(green, nir):\n    \"\"\"Calculate NDWI.\"\"\"\n    return (green - nir) / (green + nir)\n\ndef calculate_savi(nir, red, L=0.5):\n    \"\"\"Calculate SAVI.\"\"\"\n    return ((nir - red) * (1 + L)) / (nir + red + L)\n\ndef calculate_evi(nir, red, blue, G=2.5, C1=6, C2=7.5, L=1):\n    \"\"\"Calculate EVI.\"\"\"\n    return G * ((nir - red) / (nir + C1 * red - C2 * blue + L))\n\n# Use functions\nndvi = calculate_ndvi(scene.nir, scene.red)\nndwi = calculate_ndwi(scene.green, scene.nir)\nsavi = calculate_savi(scene.nir, scene.red)\nevi = calculate_evi(scene.nir, scene.red, scene.blue)\n</code></pre>"},{"location":"processing/spectral-indices/#time-series-of-indices","title":"Time Series of Indices","text":"<pre><code># Load full time series\nds_full = stac.load(\n    items,\n    bands=['red', 'nir'],\n    resolution=20,  # Coarser for faster processing\n    chunks={'time': 10}\n).compute()\n\n# Apply scale/offset\nds_full = ds_full.where(ds_full != 0) * scale + offset\n\n# Calculate NDVI time series\nndvi_ts = (ds_full.nir - ds_full.red) / (ds_full.nir + ds_full.red)\n\n# Calculate spatial mean\nndvi_mean = ndvi_ts.mean(dim=['x', 'y'])\n\n# Plot time series\nfig, ax = plt.subplots(figsize=(12, 6))\nndvi_mean.plot(ax=ax, marker='o')\nax.set_title('NDVI Time Series')\nax.set_ylabel('Mean NDVI')\nax.grid(True, alpha=0.3)\nplt.show()\n</code></pre>"},{"location":"processing/spectral-indices/#thresholding-and-classification","title":"Thresholding and Classification","text":"<pre><code># Classify NDVI into categories\nndvi_classified = xr.where(ndvi &lt; 0, 0,  # Water/Non-vegetation\n                  xr.where(ndvi &lt; 0.2, 1,  # Bare soil\n                  xr.where(ndvi &lt; 0.5, 2,  # Sparse vegetation\n                  xr.where(ndvi &lt; 0.8, 3,  # Moderate vegetation\n                           4))))  # Dense vegetation\n\n# Define colors for each class\ncolors = ['blue', 'brown', 'yellow', 'lightgreen', 'darkgreen']\nlabels = ['Water', 'Bare Soil', 'Sparse Veg', 'Moderate Veg', 'Dense Veg']\n\n# Visualize\nfig, ax = plt.subplots(figsize=(10, 8))\nim = ax.imshow(ndvi_classified, cmap=plt.cm.colors.ListedColormap(colors))\nax.set_title('NDVI Classification')\nax.set_axis_off()\n\n# Add legend\nfrom matplotlib.patches import Patch\nlegend_elements = [Patch(facecolor=colors[i], label=labels[i]) \n                   for i in range(len(colors))]\nax.legend(handles=legend_elements, loc='upper right')\nplt.show()\n</code></pre>"},{"location":"processing/spectral-indices/#exercise","title":"Exercise","text":"<p>Calculate the Normalized Difference Snow Index (NDSI) using the formula:</p> \\[NDSI = \\frac{Green - SWIR}{Green + SWIR}\\] <p>Then create a binary snow mask where NDSI &gt; 0.4 indicates snow.</p> <p>Solution:</p> <pre><code># Calculate NDSI\nndsi = (scene.green - scene.swir16) / (scene.green + scene.swir16)\n\n# Create snow mask\nsnow_mask = ndsi &gt; 0.4\n\n# Visualize\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\n\n# NDSI\nndsi.plot(ax=axes[0], cmap='Blues', vmin=-1, vmax=1)\naxes[0].set_title('NDSI - Normalized Difference Snow Index')\naxes[0].set_axis_off()\n\n# Snow mask\nsnow_mask.plot(ax=axes[1], cmap='Blues', add_colorbar=False)\naxes[1].set_title('Snow Mask (NDSI &gt; 0.4)')\naxes[1].set_axis_off()\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"processing/spectral-indices/#key-takeaways","title":"Key Takeaways","text":"<p>What You Learned</p> <ul> <li>Spectral indices combine bands to highlight specific features</li> <li>NDVI measures vegetation health and density</li> <li>NDWI and MNDWI detect water bodies</li> <li>SAVI adjusts for soil brightness</li> <li>EVI provides enhanced sensitivity in dense vegetation</li> <li>NDBI identifies built-up areas</li> <li>XArray makes index calculation straightforward</li> <li>Indices can be saved in multiple formats</li> <li>Time series analysis reveals temporal patterns</li> <li>Thresholding enables simple classification</li> </ul>"},{"location":"processing/spectral-indices/#next-steps","title":"Next Steps","text":"<p>\u2192 Continue to Cloud Masking</p>"},{"location":"processing/spectral-indices/#additional-resources","title":"Additional Resources","text":"<ul> <li>Index Database</li> <li>Awesome Spectral Indices</li> <li>USGS Spectral Indices</li> <li>Sentinel-2 Indices</li> </ul>"},{"location":"processing/time-series/","title":"Time Series Extraction","text":""},{"location":"processing/time-series/#time-series-extraction","title":"Time Series Extraction","text":"<p>Time series analysis is a fundamental technique in remote sensing, allowing us to track changes in vegetation, water bodies, land use, and climate over time.</p>"},{"location":"processing/time-series/#overview","title":"Overview","text":"<p>In this section, you'll learn how to:</p> <ul> <li>Extract time series from satellite imagery</li> <li>Handle irregular time intervals</li> <li>Resample data to regular intervals</li> <li>Calculate temporal statistics</li> <li>Detect trends and anomalies</li> </ul>"},{"location":"processing/time-series/#basic-time-series-extraction","title":"Basic Time Series Extraction","text":""},{"location":"processing/time-series/#point-based-extraction","title":"Point-based Extraction","text":"<p>Extract values at a specific location over time:</p> <pre><code>import xarray as xr\nimport matplotlib.pyplot as plt\n\n# Open time series dataset\nds = xr.open_zarr('ndvi_timeseries.zarr')\n\n# Extract at a point (lon, lat)\npoint_ts = ds.sel(x=82.5, y=27.0, method='nearest')\n\n# Plot time series\npoint_ts.NDVI.plot(marker='o')\nplt.title('NDVI Time Series at Point')\nplt.ylabel('NDVI')\nplt.grid(True, alpha=0.3)\nplt.show()\n</code></pre>"},{"location":"processing/time-series/#regional-extraction","title":"Regional Extraction","text":"<p>Extract mean values over a region:</p> <pre><code># Define bounding box\nlon_min, lon_max = 82.0, 83.0\nlat_min, lat_max = 26.5, 27.5\n\n# Extract region\nregion_ts = ds.sel(\n    x=slice(lon_min, lon_max),\n    y=slice(lat_min, lat_max)\n)\n\n# Calculate spatial mean\nmean_ts = region_ts.NDVI.mean(dim=['x', 'y'])\n\n# Plot\nmean_ts.plot(marker='o')\nplt.title('Mean NDVI over Region')\nplt.ylabel('NDVI')\nplt.show()\n</code></pre>"},{"location":"processing/time-series/#resampling-time-series","title":"Resampling Time Series","text":""},{"location":"processing/time-series/#temporal-aggregation","title":"Temporal Aggregation","text":"<p>Resample to regular intervals:</p> <pre><code># Resample to monthly\nmonthly = ds.NDVI.resample(time='1M').mean()\n\n# Resample to weekly\nweekly = ds.NDVI.resample(time='1W').median()\n\n# Plot comparison\nfig, axes = plt.subplots(2, 1, figsize=(12, 8))\n\nds.NDVI.mean(dim=['x', 'y']).plot(ax=axes[0], label='Daily')\nmonthly.mean(dim=['x', 'y']).plot(ax=axes[1], label='Monthly', marker='o')\n\naxes[0].set_title('Original Daily Data')\naxes[1].set_title('Monthly Aggregation')\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"processing/time-series/#temporal-statistics","title":"Temporal Statistics","text":""},{"location":"processing/time-series/#calculate-statistics","title":"Calculate Statistics","text":"<pre><code># Temporal mean\ntemporal_mean = ds.NDVI.mean(dim='time')\n\n# Temporal standard deviation\ntemporal_std = ds.NDVI.std(dim='time')\n\n# Temporal range\ntemporal_range = ds.NDVI.max(dim='time') - ds.NDVI.min(dim='time')\n\n# Visualize\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\ntemporal_mean.plot(ax=axes[0], cmap='RdYlGn')\naxes[0].set_title('Mean NDVI')\n\ntemporal_std.plot(ax=axes[1], cmap='YlOrRd')\naxes[1].set_title('NDVI Std Dev')\n\ntemporal_range.plot(ax=axes[2], cmap='viridis')\naxes[2].set_title('NDVI Range')\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"processing/time-series/#trend-analysis","title":"Trend Analysis","text":""},{"location":"processing/time-series/#linear-trend","title":"Linear Trend","text":"<pre><code>from scipy import stats\nimport numpy as np\n\ndef calculate_trend(data):\n    \"\"\"Calculate linear trend.\"\"\"\n    x = np.arange(len(data))\n    mask = ~np.isnan(data)\n    if mask.sum() &lt; 3:\n        return np.nan\n    slope, intercept, r_value, p_value, std_err = stats.linregress(x[mask], data[mask])\n    return slope\n\n# Apply to each pixel\ntrend = xr.apply_ufunc(\n    calculate_trend,\n    ds.NDVI,\n    input_core_dims=[['time']],\n    vectorize=True\n)\n\n# Visualize\ntrend.plot(cmap='RdBu_r', center=0)\nplt.title('NDVI Trend (slope per time step)')\nplt.show()\n</code></pre>"},{"location":"processing/time-series/#next-steps","title":"Next Steps","text":"<p>\u2192 Continue to Data Aggregation</p>"},{"location":"processing/time-series/#additional-resources","title":"Additional Resources","text":"<ul> <li>XArray Time Series</li> <li>Pandas Resampling</li> </ul>"},{"location":"reference/dask-practices/","title":"Dask Best Practices","text":""},{"location":"reference/dask-practices/#dask-best-practices","title":"Dask Best Practices","text":""},{"location":"reference/dask-practices/#overview","title":"Overview","text":"<p>Dask is a flexible parallel computing library for Python that scales from laptops to clusters. It's particularly useful for processing large remote sensing datasets that don't fit in memory.</p>"},{"location":"reference/dask-practices/#core-concepts","title":"Core Concepts","text":""},{"location":"reference/dask-practices/#lazy-evaluation","title":"Lazy Evaluation","text":"<p>Dask operations are lazy - they build a task graph without executing until you call <code>.compute()</code>.</p> <pre><code>import dask.array as da\nimport xarray as xr\n\n# This doesn't load data\nds = xr.open_dataset('large_file.nc', chunks={'time': 10})\n\n# This builds a task graph (still lazy)\nresult = ds.mean(dim='time')\n\n# This triggers computation\nresult_computed = result.compute()\n</code></pre>"},{"location":"reference/dask-practices/#task-graph","title":"Task Graph","text":"<p>Dask builds a graph of operations to optimize execution.</p> <pre><code># View task graph\nresult.data.visualize(filename='task_graph.png')\n</code></pre>"},{"location":"reference/dask-practices/#chunking-strategies","title":"Chunking Strategies","text":""},{"location":"reference/dask-practices/#chunk-size-guidelines","title":"Chunk Size Guidelines","text":"<p>Optimal chunk size: 10-100 MB per chunk</p> <pre><code># Calculate chunk size\nimport numpy as np\n\ndef calculate_chunk_size_mb(shape, dtype, chunks):\n    \"\"\"Calculate chunk size in MB.\"\"\"\n    itemsize = np.dtype(dtype).itemsize\n    chunk_items = np.prod([chunks[i] if i &lt; len(chunks) else shape[i] \n                           for i in range(len(shape))])\n    return (chunk_items * itemsize) / 1e6\n\n# Example\nshape = (365, 5000, 5000)\nchunks = (10, 500, 500)\nsize_mb = calculate_chunk_size_mb(shape, 'float32', chunks)\nprint(f\"Chunk size: {size_mb:.2f} MB\")\n</code></pre>"},{"location":"reference/dask-practices/#time-series-data","title":"Time Series Data","text":"<pre><code># Good: Large time chunks for temporal operations\nds = xr.open_dataset('timeseries.nc', chunks={'time': 100, 'x': 512, 'y': 512})\n\n# Bad: Small time chunks\nds = xr.open_dataset('timeseries.nc', chunks={'time': 1, 'x': 512, 'y': 512})\n</code></pre>"},{"location":"reference/dask-practices/#spatial-data","title":"Spatial Data","text":"<pre><code># Good: Balanced spatial chunks\nds = xr.open_dataset('spatial.nc', chunks={'time': 10, 'x': 512, 'y': 512})\n\n# Bad: Unbalanced chunks\nds = xr.open_dataset('spatial.nc', chunks={'time': 10, 'x': 5000, 'y': 10})\n</code></pre>"},{"location":"reference/dask-practices/#auto-chunking","title":"Auto Chunking","text":"<pre><code># Let Dask decide\nds = xr.open_dataset('file.nc', chunks='auto')\n\n# With target chunk size\nds = xr.open_zarr('data.zarr', chunks={'time': 'auto'})\n</code></pre>"},{"location":"reference/dask-practices/#dask-schedulers","title":"Dask Schedulers","text":""},{"location":"reference/dask-practices/#single-machine-threaded","title":"Single Machine (Threaded)","text":"<p>Default scheduler, good for I/O-bound tasks.</p> <pre><code># Automatic (default)\nresult = ds.mean().compute()\n\n# Explicit\nresult = ds.mean().compute(scheduler='threads')\n</code></pre>"},{"location":"reference/dask-practices/#single-machine-processes","title":"Single Machine (Processes)","text":"<p>Better for CPU-bound tasks, avoids GIL.</p> <pre><code>result = ds.mean().compute(scheduler='processes')\n</code></pre>"},{"location":"reference/dask-practices/#distributed","title":"Distributed","text":"<p>Best for large computations, provides dashboard.</p> <pre><code>from dask.distributed import Client, LocalCluster\n\n# Start local cluster\ncluster = LocalCluster(\n    n_workers=4,\n    threads_per_worker=2,\n    memory_limit='4GB'\n)\nclient = Client(cluster)\n\n# Computations use distributed scheduler\nresult = ds.mean().compute()\n\n# Close when done\nclient.close()\ncluster.close()\n</code></pre>"},{"location":"reference/dask-practices/#memory-management","title":"Memory Management","text":""},{"location":"reference/dask-practices/#persist-vs-compute","title":"Persist vs Compute","text":"<pre><code># compute(): Load into memory as numpy/pandas\nresult = ds.mean().compute()\n\n# persist(): Keep as Dask array in distributed memory\nresult = ds.mean().persist()\n\n# Use persist() for intermediate results\nintermediate = ds.resample(time='1M').mean().persist()\nresult1 = intermediate.max()\nresult2 = intermediate.min()\n</code></pre>"},{"location":"reference/dask-practices/#clear-memory","title":"Clear Memory","text":"<pre><code># Delete large objects\ndel large_array\n\n# Clear Dask cache\nfrom dask import config\nconfig.set(scheduler='synchronous')\n</code></pre>"},{"location":"reference/dask-practices/#optimization-techniques","title":"Optimization Techniques","text":""},{"location":"reference/dask-practices/#1-rechunking","title":"1. Rechunking","text":"<pre><code># Rechunk for different access patterns\nds_rechunked = ds.chunk({'time': 1, 'x': 1000, 'y': 1000})\n\n# Optimize chunks\nfrom dask.array import rechunk\noptimized = rechunk(ds.data, chunks=(10, 512, 512))\n</code></pre>"},{"location":"reference/dask-practices/#2-avoid-small-tasks","title":"2. Avoid Small Tasks","text":"<pre><code># Good: Reasonable chunk size\nds = ds.chunk({'time': 10, 'x': 512, 'y': 512})\n\n# Bad: Too many small tasks\nds = ds.chunk({'time': 1, 'x': 10, 'y': 10})\n</code></pre>"},{"location":"reference/dask-practices/#3-use-map_blocks","title":"3. Use map_blocks","text":"<pre><code>def process_block(block):\n    # Custom processing\n    return block * 2 + 10\n\nresult = ds.map_blocks(process_block, dtype=float)\n</code></pre>"},{"location":"reference/dask-practices/#4-avoid-repeated-computation","title":"4. Avoid Repeated Computation","text":"<pre><code># Good: Compute once\nmean = ds.mean(dim='time').persist()\nresult1 = mean + 10\nresult2 = mean * 2\n\n# Bad: Recompute each time\nresult1 = ds.mean(dim='time') + 10\nresult2 = ds.mean(dim='time') * 2\n</code></pre>"},{"location":"reference/dask-practices/#distributed-computing","title":"Distributed Computing","text":""},{"location":"reference/dask-practices/#local-cluster","title":"Local Cluster","text":"<pre><code>from dask.distributed import Client, LocalCluster\n\ncluster = LocalCluster(\n    n_workers=4,\n    threads_per_worker=2,\n    memory_limit='2GB',\n    processes=True\n)\nclient = Client(cluster)\n\n# View dashboard\nprint(client.dashboard_link)\n</code></pre>"},{"location":"reference/dask-practices/#cluster-configuration","title":"Cluster Configuration","text":"<pre><code># Custom worker configuration\ncluster = LocalCluster(\n    n_workers=8,\n    threads_per_worker=1,\n    memory_limit='4GB',\n    processes=True,\n    silence_logs=False\n)\n</code></pre>"},{"location":"reference/dask-practices/#adaptive-scaling","title":"Adaptive Scaling","text":"<pre><code># Auto-scale workers\ncluster.adapt(minimum=2, maximum=10)\n</code></pre>"},{"location":"reference/dask-practices/#monitoring","title":"Monitoring","text":""},{"location":"reference/dask-practices/#dashboard","title":"Dashboard","text":"<pre><code># Access dashboard\nprint(client.dashboard_link)\n# Usually: http://localhost:8787/status\n</code></pre>"},{"location":"reference/dask-practices/#progress-bar","title":"Progress Bar","text":"<pre><code>from dask.diagnostics import ProgressBar\n\nwith ProgressBar():\n    result = ds.mean().compute()\n</code></pre>"},{"location":"reference/dask-practices/#performance-report","title":"Performance Report","text":"<pre><code>from dask.distributed import performance_report\n\nwith performance_report(filename='dask-report.html'):\n    result = ds.mean().compute()\n</code></pre>"},{"location":"reference/dask-practices/#common-patterns","title":"Common Patterns","text":""},{"location":"reference/dask-practices/#time-series-processing","title":"Time Series Processing","text":"<pre><code># Monthly aggregation\nmonthly = ds.resample(time='1M').mean()\n\n# With Dask\nmonthly = ds.chunk({'time': 30}).resample(time='1M').mean().compute()\n</code></pre>"},{"location":"reference/dask-practices/#spatial-operations","title":"Spatial Operations","text":"<pre><code># Spatial mean\nspatial_mean = ds.mean(dim=['x', 'y'])\n\n# With Dask\nspatial_mean = ds.chunk({'x': 512, 'y': 512}).mean(dim=['x', 'y']).compute()\n</code></pre>"},{"location":"reference/dask-practices/#rolling-windows","title":"Rolling Windows","text":"<pre><code># 7-day rolling mean\nrolling = ds.rolling(time=7, center=True).mean()\n\n# With Dask\nrolling = ds.chunk({'time': 30}).rolling(time=7, center=True).mean().compute()\n</code></pre>"},{"location":"reference/dask-practices/#groupby-operations","title":"GroupBy Operations","text":"<pre><code># Monthly climatology\nmonthly_clim = ds.groupby('time.month').mean()\n\n# With Dask\nmonthly_clim = ds.chunk({'time': 30}).groupby('time.month').mean().compute()\n</code></pre>"},{"location":"reference/dask-practices/#troubleshooting","title":"Troubleshooting","text":""},{"location":"reference/dask-practices/#memory-errors","title":"Memory Errors","text":"<pre><code># Solution 1: Smaller chunks\nds = ds.chunk({'time': 5, 'x': 256, 'y': 256})\n\n# Solution 2: More workers with less memory each\ncluster = LocalCluster(n_workers=8, memory_limit='1GB')\n\n# Solution 3: Process in batches\nfor year in range(2020, 2024):\n    subset = ds.sel(time=str(year))\n    result = subset.mean().compute()\n</code></pre>"},{"location":"reference/dask-practices/#slow-performance","title":"Slow Performance","text":"<pre><code># Solution 1: Check chunk size\nprint(ds.chunks)\n\n# Solution 2: Use distributed scheduler\nclient = Client()\n\n# Solution 3: Persist intermediate results\nintermediate = ds.mean(dim='time').persist()\n</code></pre>"},{"location":"reference/dask-practices/#too-many-tasks","title":"Too Many Tasks","text":"<pre><code># Solution: Increase chunk size\nds = ds.chunk({'time': 50, 'x': 1024, 'y': 1024})\n</code></pre>"},{"location":"reference/dask-practices/#best-practices-summary","title":"Best Practices Summary","text":""},{"location":"reference/dask-practices/#do","title":"\u2705 Do","text":"<ol> <li>Use appropriate chunk sizes (10-100 MB)</li> <li>Persist intermediate results used multiple times</li> <li>Use distributed scheduler for large computations</li> <li>Monitor with dashboard</li> <li>Close clients and clusters when done</li> <li>Rechunk for access patterns</li> <li>Use map_blocks for custom functions</li> </ol>"},{"location":"reference/dask-practices/#dont","title":"\u274c Don't","text":"<ol> <li>Create too many small chunks</li> <li>Call compute() repeatedly on same data</li> <li>Mix schedulers in same script</li> <li>Forget to close distributed clients</li> <li>Use tiny chunk sizes (&lt; 1 MB)</li> <li>Use huge chunk sizes (&gt; 1 GB)</li> <li>Ignore memory limits</li> </ol>"},{"location":"reference/dask-practices/#configuration","title":"Configuration","text":""},{"location":"reference/dask-practices/#global-configuration","title":"Global Configuration","text":"<pre><code>import dask\n\n# Set scheduler\ndask.config.set(scheduler='threads')\n\n# Set chunk size\ndask.config.set({'array.chunk-size': '128 MiB'})\n\n# Disable task fusion\ndask.config.set({'optimization.fuse.active': False})\n</code></pre>"},{"location":"reference/dask-practices/#environment-variables","title":"Environment Variables","text":"<pre><code># Set number of threads\nexport OMP_NUM_THREADS=4\n\n# Set Dask config directory\nexport DASK_CONFIG=/path/to/config\n</code></pre>"},{"location":"reference/dask-practices/#performance-tips","title":"Performance Tips","text":""},{"location":"reference/dask-practices/#1-optimize-io","title":"1. Optimize I/O","text":"<pre><code># Good: Read with chunks\nds = xr.open_zarr('data.zarr', chunks='auto')\n\n# Bad: Read without chunks\nds = xr.open_zarr('data.zarr')\n</code></pre>"},{"location":"reference/dask-practices/#2-use-efficient-formats","title":"2. Use Efficient Formats","text":"<pre><code># Good: Zarr (cloud-optimized)\nds.to_zarr('output.zarr')\n\n# OK: NetCDF with compression\nds.to_netcdf('output.nc', encoding={'var': {'zlib': True}})\n\n# Bad: Uncompressed NetCDF\nds.to_netcdf('output.nc')\n</code></pre>"},{"location":"reference/dask-practices/#3-minimize-data-transfer","title":"3. Minimize Data Transfer","text":"<pre><code># Good: Reduce before computing\nresult = ds.mean(dim=['x', 'y']).compute()\n\n# Bad: Compute then reduce\nresult = ds.compute().mean(dim=['x', 'y'])\n</code></pre>"},{"location":"reference/dask-practices/#4-use-appropriate-data-types","title":"4. Use Appropriate Data Types","text":"<pre><code># Good: Use smaller dtypes when possible\nds = ds.astype('float32')\n\n# Bad: Unnecessary precision\nds = ds.astype('float64')\n</code></pre>"},{"location":"reference/dask-practices/#advanced-topics","title":"Advanced Topics","text":""},{"location":"reference/dask-practices/#custom-schedulers","title":"Custom Schedulers","text":"<pre><code>from dask.threaded import get\n\nresult = ds.mean().compute(scheduler=get)\n</code></pre>"},{"location":"reference/dask-practices/#task-priorities","title":"Task Priorities","text":"<pre><code># High priority tasks\nresult = ds.mean().compute(priority=10)\n</code></pre>"},{"location":"reference/dask-practices/#resource-management","title":"Resource Management","text":"<pre><code>cluster = LocalCluster(\n    n_workers=4,\n    resources={'GPU': 1}\n)\n\n# Use resources\nresult = ds.map_blocks(gpu_function, resources={'GPU': 1})\n</code></pre>"},{"location":"reference/dask-practices/#additional-resources","title":"Additional Resources","text":"<ul> <li>Dask Documentation</li> <li>Dask Best Practices</li> <li>Dask Tutorial</li> <li>Dask Examples</li> <li>Pangeo Dask Guide</li> </ul>"},{"location":"reference/dask-practices/#quick-reference","title":"Quick Reference","text":""},{"location":"reference/dask-practices/#common-operations","title":"Common Operations","text":"<pre><code># Load with chunks\nds = xr.open_dataset('file.nc', chunks={'time': 10})\n\n# Compute\nresult = ds.mean().compute()\n\n# Persist\nresult = ds.mean().persist()\n\n# Rechunk\nds = ds.chunk({'time': 20})\n\n# Start client\nfrom dask.distributed import Client\nclient = Client()\n\n# Close client\nclient.close()\n</code></pre>"},{"location":"reference/dask-practices/#chunk-size-calculation","title":"Chunk Size Calculation","text":"<pre><code># Target: 10-100 MB per chunk\n# Formula: chunk_size = (chunk_items * itemsize) / 1e6\n\n# Example for float32 (4 bytes)\n# 10 MB: ~2.5M items\n# 100 MB: ~25M items\n\n# For (time, x, y) = (10, 500, 500)\n# Items = 10 * 500 * 500 = 2.5M\n# Size = 2.5M * 4 bytes = 10 MB \u2713\n</code></pre>"},{"location":"reference/stac-spec/","title":"STAC Specification","text":""},{"location":"reference/stac-spec/#stac-specification-reference","title":"STAC Specification Reference","text":""},{"location":"reference/stac-spec/#overview","title":"Overview","text":"<p>The SpatioTemporal Asset Catalog (STAC) specification provides a common language to describe geospatial information, making it easier to index, discover, and access satellite imagery and other geospatial assets.</p>"},{"location":"reference/stac-spec/#core-components","title":"Core Components","text":""},{"location":"reference/stac-spec/#1-stac-item","title":"1. STAC Item","text":"<p>A STAC Item is a GeoJSON Feature with additional fields that describe a spatiotemporal asset.</p> <p>Structure:</p> <pre><code>{\n  \"type\": \"Feature\",\n  \"stac_version\": \"1.0.0\",\n  \"stac_extensions\": [],\n  \"id\": \"S2A_MSIL2A_20230115T051131_N0509_R019_T44QMG_20230115T073857\",\n  \"bbox\": [82.0, 26.5, 83.0, 27.5],\n  \"geometry\": {\n    \"type\": \"Polygon\",\n    \"coordinates\": [[...]]\n  },\n  \"properties\": {\n    \"datetime\": \"2023-01-15T05:11:31Z\",\n    \"eo:cloud_cover\": 15.5,\n    \"platform\": \"sentinel-2a\",\n    \"instruments\": [\"msi\"]\n  },\n  \"assets\": {\n    \"red\": {\n      \"href\": \"s3://sentinel-s2-l2a/.../B04.jp2\",\n      \"type\": \"image/jp2\",\n      \"eo:bands\": [{\n        \"name\": \"B04\",\n        \"common_name\": \"red\",\n        \"center_wavelength\": 0.665\n      }]\n    },\n    \"nir\": {\n      \"href\": \"s3://sentinel-s2-l2a/.../B08.jp2\",\n      \"type\": \"image/jp2\",\n      \"eo:bands\": [{\n        \"name\": \"B08\",\n        \"common_name\": \"nir\",\n        \"center_wavelength\": 0.842\n      }]\n    }\n  },\n  \"links\": [\n    {\n      \"rel\": \"self\",\n      \"href\": \"https://earth-search.aws.element84.com/v1/collections/sentinel-2-l2a/items/...\"\n    },\n    {\n      \"rel\": \"parent\",\n      \"href\": \"https://earth-search.aws.element84.com/v1/collections/sentinel-2-l2a\"\n    }\n  ]\n}\n</code></pre> <p>Key Fields:</p> Field Type Description <code>id</code> string Unique identifier for the item <code>bbox</code> array Bounding box [west, south, east, north] <code>geometry</code> GeoJSON Spatial extent as GeoJSON geometry <code>properties</code> object Additional metadata (datetime, cloud cover, etc.) <code>assets</code> object Links to actual data files <code>links</code> array Relationships to other STAC objects"},{"location":"reference/stac-spec/#2-stac-collection","title":"2. STAC Collection","text":"<p>A STAC Collection provides metadata about a set of related Items.</p> <p>Structure:</p> <pre><code>{\n  \"type\": \"Collection\",\n  \"stac_version\": \"1.0.0\",\n  \"id\": \"sentinel-2-l2a\",\n  \"title\": \"Sentinel-2 Level-2A\",\n  \"description\": \"Global Sentinel-2 data from the Multispectral Instrument (MSI)\",\n  \"license\": \"proprietary\",\n  \"extent\": {\n    \"spatial\": {\n      \"bbox\": [[-180, -90, 180, 90]]\n    },\n    \"temporal\": {\n      \"interval\": [[\"2015-06-27T00:00:00Z\", null]]\n    }\n  },\n  \"summaries\": {\n    \"platform\": [\"sentinel-2a\", \"sentinel-2b\"],\n    \"instruments\": [\"msi\"],\n    \"eo:cloud_cover\": {\n      \"minimum\": 0,\n      \"maximum\": 100\n    }\n  },\n  \"links\": [...]\n}\n</code></pre> <p>Key Fields:</p> Field Type Description <code>id</code> string Collection identifier <code>title</code> string Human-readable title <code>description</code> string Detailed description <code>extent</code> object Spatial and temporal extent <code>summaries</code> object Summary statistics of properties <code>license</code> string Data license"},{"location":"reference/stac-spec/#3-stac-catalog","title":"3. STAC Catalog","text":"<p>A STAC Catalog is a simple, flexible JSON file of links to Items, Collections, and other Catalogs.</p> <p>Structure:</p> <pre><code>{\n  \"type\": \"Catalog\",\n  \"stac_version\": \"1.0.0\",\n  \"id\": \"earth-search\",\n  \"title\": \"Earth Search\",\n  \"description\": \"A STAC API of public datasets on AWS\",\n  \"links\": [\n    {\n      \"rel\": \"self\",\n      \"href\": \"https://earth-search.aws.element84.com/v1\"\n    },\n    {\n      \"rel\": \"child\",\n      \"href\": \"https://earth-search.aws.element84.com/v1/collections/sentinel-2-l2a\"\n    },\n    {\n      \"rel\": \"child\",\n      \"href\": \"https://earth-search.aws.element84.com/v1/collections/landsat-c2-l2\"\n    }\n  ]\n}\n</code></pre>"},{"location":"reference/stac-spec/#stac-api","title":"STAC API","text":"<p>The STAC API specification defines a RESTful service for searching STAC Items.</p>"},{"location":"reference/stac-spec/#core-endpoints","title":"Core Endpoints","text":""},{"location":"reference/stac-spec/#1-landing-page","title":"1. Landing Page","text":"<pre><code>GET /\n</code></pre> <p>Returns the catalog root with links to collections and search.</p>"},{"location":"reference/stac-spec/#2-collections","title":"2. Collections","text":"<pre><code>GET /collections\nGET /collections/{collectionId}\n</code></pre> <p>List all collections or get a specific collection.</p>"},{"location":"reference/stac-spec/#3-items","title":"3. Items","text":"<pre><code>GET /collections/{collectionId}/items\nGET /collections/{collectionId}/items/{itemId}\n</code></pre> <p>List items in a collection or get a specific item.</p>"},{"location":"reference/stac-spec/#4-search","title":"4. Search","text":"<pre><code>POST /search\nGET /search\n</code></pre> <p>Search for items across collections.</p>"},{"location":"reference/stac-spec/#search-parameters","title":"Search Parameters","text":"Parameter Type Description <code>bbox</code> array Bounding box [west, south, east, north] <code>datetime</code> string Single datetime or range (RFC 3339) <code>intersects</code> GeoJSON GeoJSON geometry to search <code>collections</code> array Collection IDs to search <code>ids</code> array Specific item IDs <code>limit</code> integer Maximum number of results <code>query</code> object Additional property filters <code>sortby</code> array Sort results by fields"},{"location":"reference/stac-spec/#search-examples","title":"Search Examples","text":""},{"location":"reference/stac-spec/#basic-spatial-search","title":"Basic Spatial Search","text":"<pre><code>import pystac_client\n\ncatalog = pystac_client.Client.open(\n    'https://earth-search.aws.element84.com/v1'\n)\n\nsearch = catalog.search(\n    collections=['sentinel-2-l2a'],\n    bbox=[82.0, 26.5, 83.0, 27.5]\n)\n\nitems = search.item_collection()\nprint(f\"Found {len(items)} items\")\n</code></pre>"},{"location":"reference/stac-spec/#temporal-search","title":"Temporal Search","text":"<pre><code>search = catalog.search(\n    collections=['sentinel-2-l2a'],\n    bbox=[82.0, 26.5, 83.0, 27.5],\n    datetime='2023-01-01/2023-12-31'\n)\n</code></pre>"},{"location":"reference/stac-spec/#property-filtering","title":"Property Filtering","text":"<pre><code>search = catalog.search(\n    collections=['sentinel-2-l2a'],\n    bbox=[82.0, 26.5, 83.0, 27.5],\n    datetime='2023-01-01/2023-12-31',\n    query={\n        'eo:cloud_cover': {'lt': 20},\n        's2:nodata_pixel_percentage': {'lt': 10}\n    }\n)\n</code></pre>"},{"location":"reference/stac-spec/#geometry-search","title":"Geometry Search","text":"<pre><code>from shapely.geometry import Point\n\npoint = Point(82.5, 27.0)\nbuffer = point.buffer(0.1)\n\nsearch = catalog.search(\n    collections=['sentinel-2-l2a'],\n    intersects=buffer.__geo_interface__,\n    datetime='2023-01-01/2023-12-31'\n)\n</code></pre>"},{"location":"reference/stac-spec/#sorting-results","title":"Sorting Results","text":"<pre><code>search = catalog.search(\n    collections=['sentinel-2-l2a'],\n    bbox=[82.0, 26.5, 83.0, 27.5],\n    datetime='2023-01-01/2023-12-31',\n    sortby=[\n        {'field': 'properties.eo:cloud_cover', 'direction': 'asc'},\n        {'field': 'properties.datetime', 'direction': 'desc'}\n    ]\n)\n</code></pre>"},{"location":"reference/stac-spec/#stac-extensions","title":"STAC Extensions","text":"<p>STAC Extensions add additional fields and functionality.</p>"},{"location":"reference/stac-spec/#common-extensions","title":"Common Extensions","text":""},{"location":"reference/stac-spec/#1-eo-electro-optical","title":"1. EO (Electro-Optical)","text":"<p>Adds fields for optical satellite data.</p> <p>Fields:</p> <ul> <li><code>eo:cloud_cover</code> - Cloud coverage percentage</li> <li><code>eo:bands</code> - Band information (name, wavelength, etc.)</li> </ul>"},{"location":"reference/stac-spec/#2-sar-synthetic-aperture-radar","title":"2. SAR (Synthetic Aperture Radar)","text":"<p>For radar satellite data.</p> <p>Fields:</p> <ul> <li><code>sar:instrument_mode</code> - Instrument mode (IW, EW, etc.)</li> <li><code>sar:frequency_band</code> - Frequency band (C, X, L, etc.)</li> <li><code>sar:polarizations</code> - Polarization modes (VV, VH, HH, HV)</li> </ul>"},{"location":"reference/stac-spec/#3-projection","title":"3. Projection","text":"<p>Coordinate reference system information.</p> <p>Fields:</p> <ul> <li><code>proj:epsg</code> - EPSG code</li> <li><code>proj:wkt2</code> - WKT2 projection string</li> <li><code>proj:shape</code> - Raster dimensions</li> <li><code>proj:transform</code> - Affine transformation</li> </ul>"},{"location":"reference/stac-spec/#4-scientific","title":"4. Scientific","text":"<p>For scientific datasets.</p> <p>Fields:</p> <ul> <li><code>sci:doi</code> - Digital Object Identifier</li> <li><code>sci:citation</code> - Citation string</li> <li><code>sci:publications</code> - Related publications</li> </ul>"},{"location":"reference/stac-spec/#best-practices","title":"Best Practices","text":""},{"location":"reference/stac-spec/#1-use-appropriate-filters","title":"1. Use Appropriate Filters","text":"<pre><code># Good: Filter server-side\nsearch = catalog.search(\n    collections=['sentinel-2-l2a'],\n    query={'eo:cloud_cover': {'lt': 20}}\n)\n\n# Bad: Filter client-side\nsearch = catalog.search(collections=['sentinel-2-l2a'])\nitems = [item for item in search.items() if item.properties['eo:cloud_cover'] &lt; 20]\n</code></pre>"},{"location":"reference/stac-spec/#2-limit-results","title":"2. Limit Results","text":"<pre><code># Always set a reasonable limit\nsearch = catalog.search(\n    collections=['sentinel-2-l2a'],\n    bbox=bbox,\n    limit=100\n)\n</code></pre>"},{"location":"reference/stac-spec/#3-use-pagination","title":"3. Use Pagination","text":"<pre><code># Handle large result sets\nsearch = catalog.search(\n    collections=['sentinel-2-l2a'],\n    bbox=large_bbox,\n    max_items=1000  # pystac-client handles pagination\n)\n\nfor item in search.items():\n    process(item)\n</code></pre>"},{"location":"reference/stac-spec/#4-check-item-properties","title":"4. Check Item Properties","text":"<pre><code># Always check if property exists\ncloud_cover = item.properties.get('eo:cloud_cover', None)\nif cloud_cover is not None and cloud_cover &lt; 20:\n    process(item)\n</code></pre>"},{"location":"reference/stac-spec/#public-stac-catalogs","title":"Public STAC Catalogs","text":""},{"location":"reference/stac-spec/#earth-search-element84","title":"Earth Search (Element84)","text":"<ul> <li>URL: https://earth-search.aws.element84.com/v1</li> <li>Collections: Sentinel-2, Landsat</li> <li>Coverage: Global</li> <li>Free: Yes</li> </ul>"},{"location":"reference/stac-spec/#microsoft-planetary-computer","title":"Microsoft Planetary Computer","text":"<ul> <li>URL: https://planetarycomputer.microsoft.com/api/stac/v1</li> <li>Collections: 50+ datasets</li> <li>Coverage: Global</li> <li>Free: Yes (registration required)</li> </ul>"},{"location":"reference/stac-spec/#google-earth-engine","title":"Google Earth Engine","text":"<ul> <li>Access: Via XEE</li> <li>Collections: 1000+ datasets</li> <li>Coverage: Global</li> <li>Free: Yes (for research/education)</li> </ul>"},{"location":"reference/stac-spec/#radiant-earth-mlhub","title":"Radiant Earth MLHub","text":"<ul> <li>URL: https://api.radiant.earth/mlhub/v1</li> <li>Collections: Training datasets</li> <li>Coverage: Various</li> <li>Free: Yes</li> </ul>"},{"location":"reference/stac-spec/#tools-and-libraries","title":"Tools and Libraries","text":""},{"location":"reference/stac-spec/#python","title":"Python","text":"<ul> <li>pystac - Create and manipulate STAC objects</li> <li>pystac-client - Search STAC APIs</li> <li>odc-stac - Load STAC items into XArray</li> <li>stackstac - Load STAC items into Dask arrays</li> </ul>"},{"location":"reference/stac-spec/#javascript","title":"JavaScript","text":"<ul> <li>stac-js - STAC utilities</li> <li>@radiantearth/stac-browser - Browse STAC catalogs</li> </ul>"},{"location":"reference/stac-spec/#command-line","title":"Command Line","text":"<ul> <li>stac-cli - Command-line STAC tools</li> </ul>"},{"location":"reference/stac-spec/#additional-resources","title":"Additional Resources","text":"<ul> <li>STAC Specification</li> <li>STAC Index</li> <li>STAC Extensions</li> <li>pystac Documentation</li> <li>pystac-client Documentation</li> <li>STAC Tutorials</li> </ul>"},{"location":"reference/stac-spec/#quick-reference","title":"Quick Reference","text":""},{"location":"reference/stac-spec/#common-query-operators","title":"Common Query Operators","text":"Operator Description Example <code>eq</code> Equal to <code>{'platform': {'eq': 'sentinel-2a'}}</code> <code>lt</code> Less than <code>{'eo:cloud_cover': {'lt': 20}}</code> <code>lte</code> Less than or equal <code>{'eo:cloud_cover': {'lte': 20}}</code> <code>gt</code> Greater than <code>{'eo:cloud_cover': {'gt': 50}}</code> <code>gte</code> Greater than or equal <code>{'eo:cloud_cover': {'gte': 50}}</code> <code>in</code> In list <code>{'platform': {'in': ['sentinel-2a', 'sentinel-2b']}}</code>"},{"location":"reference/stac-spec/#datetime-formats","title":"Datetime Formats","text":"<pre><code># Single datetime\ndatetime='2023-01-15T10:30:00Z'\n\n# Date range\ndatetime='2023-01-01/2023-12-31'\n\n# Open-ended (from date)\ndatetime='2023-01-01/..'\n\n# Open-ended (to date)\ndatetime='../2023-12-31'\n</code></pre>"},{"location":"reference/stac-spec/#link-relations","title":"Link Relations","text":"Relation Description <code>self</code> Link to this object <code>root</code> Link to root catalog <code>parent</code> Link to parent catalog/collection <code>child</code> Link to child catalog/collection <code>item</code> Link to an item <code>collection</code> Link to collection"},{"location":"reference/xarray-api/","title":"XArray API","text":""},{"location":"reference/xarray-api/#xarray-api-reference","title":"XArray API Reference","text":""},{"location":"reference/xarray-api/#overview","title":"Overview","text":"<p>XArray is a powerful Python library for working with labeled multi-dimensional arrays. It's particularly well-suited for remote sensing data with dimensions like time, latitude, and longitude.</p>"},{"location":"reference/xarray-api/#core-data-structures","title":"Core Data Structures","text":""},{"location":"reference/xarray-api/#dataarray","title":"DataArray","text":"<p>A multi-dimensional array with labeled dimensions and coordinates.</p> <p>Creation:</p> <pre><code>import xarray as xr\nimport numpy as np\n\n# From numpy array\ndata = np.random.rand(365, 100, 100)\nda = xr.DataArray(\n    data,\n    dims=['time', 'y', 'x'],\n    coords={\n        'time': pd.date_range('2023-01-01', periods=365),\n        'y': np.arange(100),\n        'x': np.arange(100)\n    },\n    name='temperature'\n)\n</code></pre> <p>Attributes:</p> <ul> <li><code>dims</code> - Dimension names</li> <li><code>coords</code> - Coordinate arrays</li> <li><code>values</code> - Underlying numpy array</li> <li><code>attrs</code> - Metadata dictionary</li> <li><code>name</code> - Variable name</li> </ul>"},{"location":"reference/xarray-api/#dataset","title":"Dataset","text":"<p>A dict-like container of DataArrays sharing dimensions.</p> <p>Creation:</p> <pre><code>ds = xr.Dataset({\n    'temperature': (['time', 'y', 'x'], temp_data),\n    'precipitation': (['time', 'y', 'x'], precip_data)\n},\ncoords={\n    'time': pd.date_range('2023-01-01', periods=365),\n    'y': np.arange(100),\n    'x': np.arange(100)\n})\n</code></pre>"},{"location":"reference/xarray-api/#selection-and-indexing","title":"Selection and Indexing","text":""},{"location":"reference/xarray-api/#position-based-isel","title":"Position-based (isel)","text":"<p>Select by integer position:</p> <pre><code># Select first time step\nda.isel(time=0)\n\n# Select range\nda.isel(time=slice(0, 10))\n\n# Select from multiple dimensions\nda.isel(time=0, x=50, y=50)\n\n# Negative indexing\nda.isel(time=-1)  # Last time step\n</code></pre>"},{"location":"reference/xarray-api/#label-based-sel","title":"Label-based (sel)","text":"<p>Select by coordinate value:</p> <pre><code># Select by exact value\nda.sel(time='2023-01-15')\n\n# Select range\nda.sel(time=slice('2023-01-01', '2023-01-31'))\n\n# Nearest neighbor\nda.sel(time='2023-01-15', method='nearest')\n\n# Multiple dimensions\nda.sel(time='2023-01-15', x=50, y=50)\n</code></pre>"},{"location":"reference/xarray-api/#boolean-indexing-where","title":"Boolean Indexing (where)","text":"<p>Filter based on conditions:</p> <pre><code># Keep values where condition is True\nda.where(da &gt; 0)\n\n# Replace with specific value\nda.where(da &gt; 0, other=0)\n\n# Multiple conditions\nda.where((da &gt; 0) &amp; (da &lt; 100))\n</code></pre>"},{"location":"reference/xarray-api/#computation","title":"Computation","text":""},{"location":"reference/xarray-api/#aggregation","title":"Aggregation","text":"<pre><code># Along all dimensions\nda.mean()\nda.sum()\nda.std()\nda.min()\nda.max()\nda.median()\n\n# Along specific dimension\nda.mean(dim='time')\nda.sum(dim=['x', 'y'])\n\n# Keep attributes\nda.mean(dim='time', keep_attrs=True)\n\n# Skip NaN values\nda.mean(dim='time', skipna=True)\n</code></pre>"},{"location":"reference/xarray-api/#element-wise-operations","title":"Element-wise Operations","text":"<pre><code># Arithmetic\nresult = da + 10\nresult = da * 2\nresult = da / 100\n\n# Mathematical functions\nnp.sqrt(da)\nnp.exp(da)\nnp.log(da)\n\n# Trigonometric\nnp.sin(da)\nnp.cos(da)\n\n# Between DataArrays\nresult = da1 + da2  # Automatically aligns coordinates\n</code></pre>"},{"location":"reference/xarray-api/#groupby-operations","title":"GroupBy Operations","text":"<pre><code># Group by time components\nmonthly = da.groupby('time.month').mean()\nseasonal = da.groupby('time.season').mean()\nyearly = da.groupby('time.year').mean()\n\n# Custom grouping\nbins = [0, 10, 20, 30]\ngrouped = da.groupby_bins('temperature', bins).mean()\n\n# Multiple operations\nstats = da.groupby('time.month').agg(['mean', 'std', 'min', 'max'])\n</code></pre>"},{"location":"reference/xarray-api/#resampling","title":"Resampling","text":"<pre><code># Temporal resampling\nmonthly = da.resample(time='1M').mean()\nweekly = da.resample(time='1W').median()\ndaily = da.resample(time='1D').sum()\n\n# Upsampling with interpolation\nhourly = da.resample(time='1H').interpolate('linear')\n</code></pre>"},{"location":"reference/xarray-api/#rolling-windows","title":"Rolling Windows","text":"<pre><code># Moving average\nrolling_mean = da.rolling(time=7, center=True).mean()\n\n# Multiple dimensions\nrolling_2d = da.rolling(x=3, y=3, center=True).mean()\n\n# Custom function\nrolling_custom = da.rolling(time=7).reduce(np.percentile, q=90)\n</code></pre>"},{"location":"reference/xarray-api/#interpolation","title":"Interpolation","text":"<pre><code># Linear interpolation\ninterp = da.interp(time='2023-01-15T12:00:00')\n\n# Multiple points\nnew_times = pd.date_range('2023-01-01', '2023-12-31', freq='6H')\ninterp = da.interp(time=new_times)\n\n# Different methods\ninterp = da.interp(time=new_times, method='cubic')\ninterp = da.interp(time=new_times, method='nearest')\n\n# Fill NaN\ninterp = da.interpolate_na(dim='time', method='linear')\n</code></pre>"},{"location":"reference/xarray-api/#broadcasting","title":"Broadcasting","text":"<pre><code># Automatic alignment\nda1 = xr.DataArray([1, 2, 3], dims='x', coords={'x': [0, 1, 2]})\nda2 = xr.DataArray([10, 20], dims='y', coords={'y': [0, 1]})\n\n# Result has dimensions (x, y)\nresult = da1 + da2\n</code></pre>"},{"location":"reference/xarray-api/#plotting","title":"Plotting","text":""},{"location":"reference/xarray-api/#basic-plots","title":"Basic Plots","text":"<pre><code># Line plot (1D)\nda.sel(x=50, y=50).plot()\n\n# 2D plot\nda.isel(time=0).plot()\n\n# With options\nda.isel(time=0).plot(\n    cmap='viridis',\n    vmin=0,\n    vmax=100,\n    cbar_kwargs={'label': 'Temperature (\u00b0C)'}\n)\n</code></pre>"},{"location":"reference/xarray-api/#advanced-plots","title":"Advanced Plots","text":"<pre><code># Contour plot\nda.isel(time=0).plot.contour(levels=10)\n\n# Filled contour\nda.isel(time=0).plot.contourf(levels=20, cmap='RdYlBu_r')\n\n# Histogram\nda.plot.hist(bins=50)\n\n# Multiple subplots\nda.isel(time=[0, 10, 20, 30]).plot(col='time', col_wrap=2)\n</code></pre>"},{"location":"reference/xarray-api/#io-operations","title":"I/O Operations","text":""},{"location":"reference/xarray-api/#netcdf","title":"NetCDF","text":"<pre><code># Write\nds.to_netcdf('data.nc')\n\n# Read\nds = xr.open_dataset('data.nc')\n\n# With chunks (lazy loading)\nds = xr.open_dataset('data.nc', chunks={'time': 10})\n\n# Multiple files\nds = xr.open_mfdataset('data_*.nc', combine='by_coords')\n</code></pre>"},{"location":"reference/xarray-api/#zarr","title":"Zarr","text":"<pre><code># Write\nds.to_zarr('data.zarr', mode='w')\n\n# Read\nds = xr.open_zarr('data.zarr')\n\n# Append\nds.to_zarr('data.zarr', append_dim='time')\n</code></pre>"},{"location":"reference/xarray-api/#geotiff-via-rioxarray","title":"GeoTIFF (via rioxarray)","text":"<pre><code>import rioxarray as rxr\n\n# Read\nda = rxr.open_rasterio('image.tif')\n\n# Write\nda.rio.to_raster('output.tif')\n\n# With CRS\nda.rio.write_crs('EPSG:4326', inplace=True)\nda.rio.to_raster('output.tif')\n</code></pre>"},{"location":"reference/xarray-api/#dask-integration","title":"Dask Integration","text":""},{"location":"reference/xarray-api/#chunking","title":"Chunking","text":"<pre><code># Chunk on creation\nds = xr.Dataset({\n    'temperature': (['time', 'y', 'x'], \n                   dask.array.random.random((365, 1000, 1000), \n                   chunks=(10, 100, 100)))\n})\n\n# Rechunk existing data\nds_rechunked = ds.chunk({'time': 30, 'x': 500, 'y': 500})\n\n# Auto chunking\nds = xr.open_dataset('large_file.nc', chunks='auto')\n</code></pre>"},{"location":"reference/xarray-api/#lazy-evaluation","title":"Lazy Evaluation","text":"<pre><code># Operations are lazy\nresult = ds.mean(dim='time')  # Not computed yet\n\n# Trigger computation\nresult_computed = result.compute()\n\n# Persist in memory\nresult_persisted = result.persist()\n</code></pre>"},{"location":"reference/xarray-api/#parallel-operations","title":"Parallel Operations","text":"<pre><code>from dask.distributed import Client\n\nclient = Client()\n\n# Operations run in parallel\nresult = ds.mean(dim='time').compute()\n</code></pre>"},{"location":"reference/xarray-api/#coordinate-operations","title":"Coordinate Operations","text":""},{"location":"reference/xarray-api/#adding-coordinates","title":"Adding Coordinates","text":"<pre><code># Add new coordinate\nds = ds.assign_coords(height=100)\n\n# From existing data\nds = ds.assign_coords(ndvi=(ds.nir - ds.red) / (ds.nir + ds.red))\n</code></pre>"},{"location":"reference/xarray-api/#swapping-dimensions","title":"Swapping Dimensions","text":"<pre><code># Swap dimension with coordinate\nds_swapped = ds.swap_dims({'time': 'day_of_year'})\n</code></pre>"},{"location":"reference/xarray-api/#multi-index","title":"Multi-index","text":"<pre><code># Create multi-index\nds = ds.set_index(location=['lat', 'lon'])\n\n# Unstack\nds_unstacked = ds.unstack('location')\n</code></pre>"},{"location":"reference/xarray-api/#merging-and-concatenating","title":"Merging and Concatenating","text":""},{"location":"reference/xarray-api/#merge","title":"Merge","text":"<pre><code># Merge datasets\nmerged = xr.merge([ds1, ds2])\n\n# With options\nmerged = xr.merge([ds1, ds2], compat='override')\n</code></pre>"},{"location":"reference/xarray-api/#concatenate","title":"Concatenate","text":"<pre><code># Along existing dimension\nconcat = xr.concat([ds1, ds2], dim='time')\n\n# Create new dimension\nconcat = xr.concat([ds1, ds2], dim='model')\n</code></pre>"},{"location":"reference/xarray-api/#combine","title":"Combine","text":"<pre><code># Combine by coordinates\ncombined = xr.combine_by_coords([ds1, ds2, ds3])\n\n# Combine nested\ncombined = xr.combine_nested([[ds1, ds2], [ds3, ds4]], \n                             concat_dim=['x', 'y'])\n</code></pre>"},{"location":"reference/xarray-api/#apply-functions","title":"Apply Functions","text":""},{"location":"reference/xarray-api/#apply_ufunc","title":"apply_ufunc","text":"<pre><code>def custom_function(x, y):\n    return x ** 2 + y\n\nresult = xr.apply_ufunc(\n    custom_function,\n    da1, da2,\n    dask='parallelized',\n    output_dtypes=[float]\n)\n</code></pre>"},{"location":"reference/xarray-api/#map_blocks","title":"map_blocks","text":"<pre><code>def process_block(block):\n    # Custom processing\n    return block * 2\n\nresult = da.map_blocks(process_block)\n</code></pre>"},{"location":"reference/xarray-api/#performance-tips","title":"Performance Tips","text":""},{"location":"reference/xarray-api/#1-use-chunking","title":"1. Use Chunking","text":"<pre><code># Good: Chunked for large data\nds = xr.open_dataset('large.nc', chunks={'time': 10})\n\n# Bad: Load all into memory\nds = xr.open_dataset('large.nc')\n</code></pre>"},{"location":"reference/xarray-api/#2-avoid-loops","title":"2. Avoid Loops","text":"<pre><code># Good: Vectorized\nresult = (ds.nir - ds.red) / (ds.nir + ds.red)\n\n# Bad: Loop over time\nfor t in ds.time:\n    result = (ds.nir.sel(time=t) - ds.red.sel(time=t)) / \\\n             (ds.nir.sel(time=t) + ds.red.sel(time=t))\n</code></pre>"},{"location":"reference/xarray-api/#3-use-appropriate-chunks","title":"3. Use Appropriate Chunks","text":"<pre><code># Good: Balanced chunks (10-100 MB per chunk)\nds = ds.chunk({'time': 10, 'x': 512, 'y': 512})\n\n# Bad: Too small\nds = ds.chunk({'time': 1, 'x': 10, 'y': 10})\n\n# Bad: Too large\nds = ds.chunk({'time': 1000, 'x': 10000, 'y': 10000})\n</code></pre>"},{"location":"reference/xarray-api/#4-persist-intermediate-results","title":"4. Persist Intermediate Results","text":"<pre><code># Good: Persist frequently used results\nintermediate = ds.mean(dim='time').persist()\nresult1 = intermediate + 10\nresult2 = intermediate * 2\n\n# Bad: Recompute each time\nresult1 = ds.mean(dim='time') + 10\nresult2 = ds.mean(dim='time') * 2\n</code></pre>"},{"location":"reference/xarray-api/#common-patterns","title":"Common Patterns","text":""},{"location":"reference/xarray-api/#calculate-ndvi","title":"Calculate NDVI","text":"<pre><code>ndvi = (ds.nir - ds.red) / (ds.nir + ds.red)\nndvi = ndvi.rename('NDVI')\n</code></pre>"},{"location":"reference/xarray-api/#temporal-statistics","title":"Temporal Statistics","text":"<pre><code># Mean over time\ntemporal_mean = ds.mean(dim='time')\n\n# Anomalies\nclimatology = ds.groupby('time.dayofyear').mean()\nanomalies = ds.groupby('time.dayofyear') - climatology\n</code></pre>"},{"location":"reference/xarray-api/#spatial-subsetting","title":"Spatial Subsetting","text":"<pre><code># Bounding box\nsubset = ds.sel(\n    x=slice(xmin, xmax),\n    y=slice(ymin, ymax)\n)\n\n# Point extraction\npoint = ds.sel(x=82.5, y=27.0, method='nearest')\n</code></pre>"},{"location":"reference/xarray-api/#additional-resources","title":"Additional Resources","text":"<ul> <li>XArray Documentation</li> <li>XArray Tutorial</li> <li>XArray GitHub</li> <li>Pangeo Gallery</li> <li>XArray Cheat Sheet</li> </ul>"},{"location":"reference/xee-usage/","title":"XEE Usage","text":""},{"location":"reference/xee-usage/#google-earth-engine-reference","title":"Google Earth Engine Reference","text":""},{"location":"reference/xee-usage/#overview","title":"Overview","text":"<p>Google Earth Engine (GEE) is a cloud-based platform for planetary-scale geospatial analysis. It provides access to petabytes of satellite imagery and geospatial datasets with powerful processing capabilities.</p>"},{"location":"reference/xee-usage/#key-features","title":"Key Features","text":"<ul> <li>Massive Data Catalog: 1000+ public datasets</li> <li>Server-side Processing: Computation happens in Google's cloud</li> <li>Parallel Processing: Automatic parallelization</li> <li>No Download Required: Stream data directly</li> <li>Free for Research: No cost for non-commercial use</li> </ul>"},{"location":"reference/xee-usage/#authentication","title":"Authentication","text":""},{"location":"reference/xee-usage/#first-time-setup","title":"First-Time Setup","text":"<pre><code>import ee\n\n# Authenticate (opens browser)\nee.Authenticate()\n\n# Initialize\nee.Initialize(project='spatialgeography')\n</code></pre>"},{"location":"reference/xee-usage/#project-based-authentication","title":"Project-based Authentication","text":"<pre><code># For newer accounts\nee.Initialize(project='spatialgeography')\n</code></pre>"},{"location":"reference/xee-usage/#service-account","title":"Service Account","text":"<pre><code># For production/automated systems\ncredentials = ee.ServiceAccountCredentials(\n    email='your-service-account@project.iam.gserviceaccount.com',\n    key_file='/path/to/private-key.json'\n)\nee.Initialize(credentials)\n</code></pre>"},{"location":"reference/xee-usage/#core-data-types","title":"Core Data Types","text":""},{"location":"reference/xee-usage/#eeimage","title":"ee.Image","text":"<p>A single raster image with one or more bands.</p> <pre><code># Load image\nimage = ee.Image('COPERNICUS/S2_SR/20230115T051131_20230115T051126_T44QMG')\n\n# Select bands\nrgb = image.select(['B4', 'B3', 'B2'])\n\n# Get info\nprint(image.bandNames().getInfo())\nprint(image.projection().getInfo())\n</code></pre>"},{"location":"reference/xee-usage/#eeimagecollection","title":"ee.ImageCollection","text":"<p>A stack or time series of images.</p> <pre><code># Load collection\ncollection = ee.ImageCollection('COPERNICUS/S2_SR') \\\n    .filterBounds(ee.Geometry.Point([82.5, 27.0])) \\\n    .filterDate('2023-01-01', '2023-12-31') \\\n    .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 20))\n\n# Get size\nprint(collection.size().getInfo())\n\n# Get first image\nfirst = collection.first()\n</code></pre>"},{"location":"reference/xee-usage/#eegeometry","title":"ee.Geometry","text":"<p>Geometric objects (points, lines, polygons).</p> <pre><code># Point\npoint = ee.Geometry.Point([82.5, 27.0])\n\n# Rectangle\nrect = ee.Geometry.Rectangle([82.0, 26.5, 83.0, 27.5])\n\n# Polygon\npolygon = ee.Geometry.Polygon([[\n    [82.0, 27.0],\n    [83.0, 27.0],\n    [83.0, 28.0],\n    [82.0, 28.0],\n    [82.0, 27.0]\n]])\n\n# Buffer\nbuffered = point.buffer(10000)  # 10km buffer\n</code></pre>"},{"location":"reference/xee-usage/#eefeature","title":"ee.Feature","text":"<p>A geometry with properties (attributes).</p> <pre><code># Create feature\nfeature = ee.Feature(\n    ee.Geometry.Point([82.5, 27.0]),\n    {'name': 'Location A', 'value': 100}\n)\n\n# Get property\nname = feature.get('name')\n</code></pre>"},{"location":"reference/xee-usage/#eefeaturecollection","title":"ee.FeatureCollection","text":"<p>A collection of features (vector data).</p> <pre><code># Load feature collection\ncountries = ee.FeatureCollection('USDOS/LSIB_SIMPLE/2017')\n\n# Filter\nindia = countries.filter(ee.Filter.eq('country_na', 'India'))\n\n# Get geometry\ngeometry = india.geometry()\n</code></pre>"},{"location":"reference/xee-usage/#filtering","title":"Filtering","text":""},{"location":"reference/xee-usage/#spatial-filtering","title":"Spatial Filtering","text":"<pre><code># Filter by bounds\nfiltered = collection.filterBounds(geometry)\n\n# Filter by geometry intersection\nfiltered = collection.filter(ee.Filter.intersects('.geo', geometry))\n</code></pre>"},{"location":"reference/xee-usage/#temporal-filtering","title":"Temporal Filtering","text":"<pre><code># Date range\nfiltered = collection.filterDate('2023-01-01', '2023-12-31')\n\n# Specific dates\nfiltered = collection.filter(\n    ee.Filter.inList('system:index', ['20230115', '20230120'])\n)\n</code></pre>"},{"location":"reference/xee-usage/#metadata-filtering","title":"Metadata Filtering","text":"<pre><code># Less than\nfiltered = collection.filter(\n    ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 20)\n)\n\n# Greater than\nfiltered = collection.filter(\n    ee.Filter.gt('SUN_ELEVATION', 30)\n)\n\n# Equals\nfiltered = collection.filter(\n    ee.Filter.eq('SPACECRAFT_NAME', 'Sentinel-2A')\n)\n\n# In list\nfiltered = collection.filter(\n    ee.Filter.inList('MGRS_TILE', ['44QMG', '44QNG'])\n)\n\n# Multiple conditions (AND)\nfiltered = collection.filter(\n    ee.Filter.And(\n        ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 20),\n        ee.Filter.gt('SUN_ELEVATION', 30)\n    )\n)\n\n# Multiple conditions (OR)\nfiltered = collection.filter(\n    ee.Filter.Or(\n        ee.Filter.eq('SPACECRAFT_NAME', 'Sentinel-2A'),\n        ee.Filter.eq('SPACECRAFT_NAME', 'Sentinel-2B')\n    )\n)\n</code></pre>"},{"location":"reference/xee-usage/#mapping-functions","title":"Mapping Functions","text":""},{"location":"reference/xee-usage/#map","title":"map()","text":"<p>Apply function to each image in collection.</p> <pre><code>def add_ndvi(image):\n    ndvi = image.normalizedDifference(['B8', 'B4']).rename('NDVI')\n    return image.addBands(ndvi)\n\ncollection_with_ndvi = collection.map(add_ndvi)\n</code></pre>"},{"location":"reference/xee-usage/#cloud-masking","title":"Cloud Masking","text":"<pre><code>def mask_s2_clouds(image):\n    qa = image.select('QA60')\n    cloud_mask = qa.bitwiseAnd(1 &lt;&lt; 10).eq(0).And(\n                 qa.bitwiseAnd(1 &lt;&lt; 11).eq(0))\n    return image.updateMask(cloud_mask)\n\nmasked = collection.map(mask_s2_clouds)\n</code></pre>"},{"location":"reference/xee-usage/#reducers","title":"Reducers","text":""},{"location":"reference/xee-usage/#temporal-reduction","title":"Temporal Reduction","text":"<pre><code># Median composite\nmedian = collection.median()\n\n# Mean\nmean = collection.mean()\n\n# Max\nmaximum = collection.max()\n\n# Percentile\np90 = collection.reduce(ee.Reducer.percentile([90]))\n</code></pre>"},{"location":"reference/xee-usage/#spatial-reduction","title":"Spatial Reduction","text":"<pre><code># Mean over region\nmean_value = image.reduceRegion(\n    reducer=ee.Reducer.mean(),\n    geometry=geometry,\n    scale=30,\n    maxPixels=1e9\n)\n\n# Multiple statistics\nstats = image.reduceRegion(\n    reducer=ee.Reducer.mean() \\\n        .combine(ee.Reducer.stdDev(), '', True) \\\n        .combine(ee.Reducer.min(), '', True) \\\n        .combine(ee.Reducer.max(), '', True),\n    geometry=geometry,\n    scale=30\n)\n</code></pre>"},{"location":"reference/xee-usage/#zonal-statistics","title":"Zonal Statistics","text":"<pre><code># Statistics by zone\nzonal_stats = image.reduceRegions(\n    collection=zones,\n    reducer=ee.Reducer.mean(),\n    scale=30\n)\n</code></pre>"},{"location":"reference/xee-usage/#image-operations","title":"Image Operations","text":""},{"location":"reference/xee-usage/#band-math","title":"Band Math","text":"<pre><code># NDVI\nndvi = image.normalizedDifference(['B8', 'B4'])\n\n# Custom calculation\nevi = image.expression(\n    '2.5 * ((NIR - RED) / (NIR + 6 * RED - 7.5 * BLUE + 1))',\n    {\n        'NIR': image.select('B8'),\n        'RED': image.select('B4'),\n        'BLUE': image.select('B2')\n    }\n)\n\n# Arithmetic\nresult = image.select('B8').subtract(image.select('B4'))\nresult = image.select('B8').divide(image.select('B4'))\n</code></pre>"},{"location":"reference/xee-usage/#masking","title":"Masking","text":"<pre><code># Mask by value\nmasked = image.updateMask(image.select('NDVI').gt(0.3))\n\n# Mask by another image\nmask = image.select('QA').eq(0)\nmasked = image.updateMask(mask)\n</code></pre>"},{"location":"reference/xee-usage/#clipping","title":"Clipping","text":"<pre><code># Clip to geometry\nclipped = image.clip(geometry)\n\n# Clip collection\nclipped_collection = collection.map(lambda img: img.clip(geometry))\n</code></pre>"},{"location":"reference/xee-usage/#exporting","title":"Exporting","text":""},{"location":"reference/xee-usage/#export-to-drive","title":"Export to Drive","text":"<pre><code># Export image\ntask = ee.batch.Export.image.toDrive(\n    image=image,\n    description='my_export',\n    folder='EarthEngine',\n    fileNamePrefix='sentinel2_image',\n    scale=10,\n    region=geometry,\n    maxPixels=1e9\n)\ntask.start()\n\n# Check status\nprint(task.status())\n</code></pre>"},{"location":"reference/xee-usage/#export-to-asset","title":"Export to Asset","text":"<pre><code>task = ee.batch.Export.image.toAsset(\n    image=image,\n    description='export_to_asset',\n    assetId='users/username/my_image',\n    scale=10,\n    region=geometry\n)\ntask.start()\n</code></pre>"},{"location":"reference/xee-usage/#export-to-cloud-storage","title":"Export to Cloud Storage","text":"<pre><code>task = ee.batch.Export.image.toCloudStorage(\n    image=image,\n    description='export_to_gcs',\n    bucket='my-bucket',\n    fileNamePrefix='sentinel2',\n    scale=10,\n    region=geometry\n)\ntask.start()\n</code></pre>"},{"location":"reference/xee-usage/#export-table","title":"Export Table","text":"<pre><code>task = ee.batch.Export.table.toDrive(\n    collection=feature_collection,\n    description='export_table',\n    fileFormat='CSV'\n)\ntask.start()\n</code></pre>"},{"location":"reference/xee-usage/#common-datasets","title":"Common Datasets","text":""},{"location":"reference/xee-usage/#sentinel-2","title":"Sentinel-2","text":"<pre><code># Surface Reflectance\ns2_sr = ee.ImageCollection('COPERNICUS/S2_SR')\n\n# Top of Atmosphere\ns2_toa = ee.ImageCollection('COPERNICUS/S2')\n\n# Harmonized (2015-present)\ns2_harmonized = ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED')\n</code></pre>"},{"location":"reference/xee-usage/#landsat","title":"Landsat","text":"<pre><code># Landsat 8 Collection 2 Level 2\nl8 = ee.ImageCollection('LANDSAT/LC08/C02/T1_L2')\n\n# Landsat 9\nl9 = ee.ImageCollection('LANDSAT/LC09/C02/T1_L2')\n</code></pre>"},{"location":"reference/xee-usage/#modis","title":"MODIS","text":"<pre><code># NDVI\nmodis_ndvi = ee.ImageCollection('MODIS/006/MOD13A2')\n\n# Land Surface Temperature\nmodis_lst = ee.ImageCollection('MODIS/006/MOD11A2')\n\n# Land Cover\nmodis_lc = ee.ImageCollection('MODIS/006/MCD12Q1')\n</code></pre>"},{"location":"reference/xee-usage/#climate-data","title":"Climate Data","text":"<pre><code># ERA5 Daily\nera5 = ee.ImageCollection('ECMWF/ERA5/DAILY')\n\n# CHIRPS Precipitation\nchirps = ee.ImageCollection('UCSB-CHG/CHIRPS/DAILY')\n\n# TerraClimate\nterraclimate = ee.ImageCollection('IDAHO_EPSCOR/TERRACLIMATE')\n</code></pre>"},{"location":"reference/xee-usage/#terrain","title":"Terrain","text":"<pre><code># SRTM DEM\nsrtm = ee.Image('USGS/SRTMGL1_003')\n\n# ALOS World 3D\nalos = ee.Image('JAXA/ALOS/AW3D30/V3_2')\n</code></pre>"},{"location":"reference/xee-usage/#best-practices","title":"Best Practices","text":""},{"location":"reference/xee-usage/#1-filter-early","title":"1. Filter Early","text":"<pre><code># Good: Filter before processing\nfiltered = collection \\\n    .filterBounds(geometry) \\\n    .filterDate('2023-01-01', '2023-12-31') \\\n    .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 20))\nresult = filtered.median()\n\n# Bad: Process then filter\nresult = collection.median()\nfiltered = result.clip(geometry)\n</code></pre>"},{"location":"reference/xee-usage/#2-use-appropriate-scale","title":"2. Use Appropriate Scale","text":"<pre><code># Good: Match data resolution\nstats = image.reduceRegion(\n    reducer=ee.Reducer.mean(),\n    geometry=geometry,\n    scale=10  # Sentinel-2 resolution\n)\n\n# Bad: Too fine (slow, unnecessary)\nstats = image.reduceRegion(\n    reducer=ee.Reducer.mean(),\n    geometry=geometry,\n    scale=1\n)\n</code></pre>"},{"location":"reference/xee-usage/#3-limit-computation","title":"3. Limit Computation","text":"<pre><code># Good: Use maxPixels\nstats = image.reduceRegion(\n    reducer=ee.Reducer.mean(),\n    geometry=geometry,\n    scale=30,\n    maxPixels=1e9\n)\n\n# Good: Use smaller geometry\nsmall_geometry = geometry.buffer(-1000)\n</code></pre>"},{"location":"reference/xee-usage/#4-use-getinfo-sparingly","title":"4. Use getInfo() Sparingly","text":"<pre><code># Good: Minimize getInfo() calls\nsize = collection.size().getInfo()\nprint(f\"Collection has {size} images\")\n\n# Bad: getInfo() in loop\nfor i in range(collection.size().getInfo()):\n    image = ee.Image(collection.toList(1, i).get(0))\n    print(image.get('system:index').getInfo())\n</code></pre>"},{"location":"reference/xee-usage/#debugging","title":"Debugging","text":""},{"location":"reference/xee-usage/#print-information","title":"Print Information","text":"<pre><code># Print to console\nprint(image.bandNames().getInfo())\nprint(collection.size().getInfo())\nprint(geometry.area().getInfo())\n</code></pre>"},{"location":"reference/xee-usage/#inspect-properties","title":"Inspect Properties","text":"<pre><code># Get all properties\nprops = image.propertyNames().getInfo()\nprint(props)\n\n# Get specific property\ncloud_cover = image.get('CLOUDY_PIXEL_PERCENTAGE').getInfo()\n</code></pre>"},{"location":"reference/xee-usage/#visualize","title":"Visualize","text":"<pre><code># In Jupyter with geemap\nimport geemap\n\nMap = geemap.Map()\nMap.addLayer(image, {'bands': ['B4', 'B3', 'B2'], 'min': 0, 'max': 3000})\nMap.centerObject(geometry, 10)\nMap\n</code></pre>"},{"location":"reference/xee-usage/#computational-limits","title":"Computational Limits","text":""},{"location":"reference/xee-usage/#free-tier-limits","title":"Free Tier Limits","text":"<ul> <li>Concurrent requests: 3,000</li> <li>User memory: 256 MB per request</li> <li>Computation time: 5 minutes per request</li> <li>Export size: 10 GB per file</li> <li>Asset storage: 250 GB</li> </ul>"},{"location":"reference/xee-usage/#optimization-tips","title":"Optimization Tips","text":"<ol> <li>Use smaller geometries</li> <li>Reduce spatial resolution (increase scale)</li> <li>Filter collections before processing</li> <li>Use appropriate reducers</li> <li>Batch exports for large areas</li> </ol>"},{"location":"reference/xee-usage/#common-issues-fixes-xee-specific","title":"Common Issues &amp; Fixes (XEE-specific)","text":""},{"location":"reference/xee-usage/#1-the-monotonicity-error","title":"1. The Monotonicity Error","text":"<p>Error: <code>ValueError: Index must be monotonic for resampling</code> Cause: Earth Engine collections are not always sorted by time when streamed through XEE. Solution: Use <code>.sortby('time')</code> before any resampling or interpolation.</p> <pre><code>ds = xr.open_dataset(collection, engine='ee')\nds = ds.sortby('time')  # Fix\nresampled = ds.resample(time='1M').median()\n</code></pre>"},{"location":"reference/xee-usage/#2-all-nan-data-empty-plot","title":"2. All-NaN Data (Empty Plot)","text":"<p>Error: <code>TypeError: No numeric data to plot.</code> Cause: Aggressive cloud masking or strict cloud filters (e.g., <code>&lt;20%</code>) removed all valid pixels. Solution: Increase the <code>CLOUDY_PIXEL_PERCENTAGE</code> filter or check <code>ds.notnull().any()</code> before plotting.</p>"},{"location":"reference/xee-usage/#3-spatial-dimension-mismatch","title":"3. Spatial Dimension Mismatch","text":"<p>Error: <code>KeyError: 'X'</code> or <code>KeyError: 'Y'</code> Cause: XEE dimension names can vary (<code>lon/lat</code> vs <code>X/Y</code>) based on the CRS used. Solution: Dynamically detect spatial dimensions or explicitly specify EPSG:4326 for consistency.</p> <pre><code>spatial_dims = [d for d in ds.dims if d not in ['time', 'band']]\nds.mean(dim=spatial_dims).plot()\n</code></pre>"},{"location":"reference/xee-usage/#additional-resources","title":"Additional Resources","text":"<p>... (previous content) ...</p>"},{"location":"reference/xee-usage/#quick-reference","title":"Quick Reference","text":""},{"location":"reference/xee-usage/#common-band-names","title":"Common Band Names","text":"<p>Sentinel-2:</p> <ul> <li>B1: Coastal aerosol (443nm)</li> <li>B2: Blue (490nm)</li> <li>B3: Green (560nm)</li> <li>B4: Red (665nm)</li> <li>B8: NIR (842nm)</li> <li>B11: SWIR1 (1610nm)</li> <li>B12: SWIR2 (2190nm)</li> </ul> <p>Landsat 8/9:</p> <ul> <li>B1: Coastal/Aerosol</li> <li>B2: Blue</li> <li>B3: Green</li> <li>B4: Red</li> <li>B5: NIR</li> <li>B6: SWIR1</li> <li>B7: SWIR2</li> </ul>"},{"location":"reference/xee-usage/#common-filters","title":"Common Filters","text":"<pre><code># Date\n.filterDate('2023-01-01', '2023-12-31')\n\n# Bounds\n.filterBounds(geometry)\n\n# Cloud cover\n.filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 20))\n\n# Property equals\n.filter(ee.Filter.eq('SPACECRAFT_NAME', 'Sentinel-2A'))\n</code></pre>"},{"location":"reference/xee-usage/#common-reducers","title":"Common Reducers","text":"<pre><code>ee.Reducer.mean()\nee.Reducer.median()\nee.Reducer.min()\nee.Reducer.max()\nee.Reducer.stdDev()\nee.Reducer.sum()\nee.Reducer.count()\nee.Reducer.percentile([10, 50, 90])\n</code></pre>"},{"location":"reference/zarr-format/","title":"Zarr Format","text":""},{"location":"reference/zarr-format/#zarr-format-reference","title":"Zarr Format Reference","text":""},{"location":"reference/zarr-format/#overview","title":"Overview","text":"<p>Zarr is a format for the storage of chunked, compressed, N-dimensional arrays. It's designed for use in parallel computing and cloud storage environments.</p>"},{"location":"reference/zarr-format/#key-features","title":"Key Features","text":"<ul> <li>Chunked Storage: Data divided into regular chunks</li> <li>Compression: Multiple compression algorithms supported</li> <li>Cloud-Optimized: Efficient for object storage (S3, GCS, Azure)</li> <li>Parallel I/O: Concurrent reads and writes</li> <li>Language-Agnostic: Implementations in Python, Julia, C++, Java</li> <li>Metadata: Flexible JSON metadata storage</li> </ul>"},{"location":"reference/zarr-format/#storage-structure","title":"Storage Structure","text":""},{"location":"reference/zarr-format/#directory-layout","title":"Directory Layout","text":"<pre><code>data.zarr/\n\u251c\u2500\u2500 .zarray              # Array metadata\n\u251c\u2500\u2500 .zattrs              # User attributes\n\u251c\u2500\u2500 .zgroup              # Group metadata (if applicable)\n\u2514\u2500\u2500 0.0.0                # Chunk files\n    \u251c\u2500\u2500 0.0.1\n    \u251c\u2500\u2500 0.1.0\n    \u251c\u2500\u2500 0.1.1\n    \u2514\u2500\u2500 ...\n</code></pre>"},{"location":"reference/zarr-format/#metadata-files","title":"Metadata Files","text":""},{"location":"reference/zarr-format/#zarray","title":".zarray","text":"<pre><code>{\n    \"chunks\": [10, 512, 512],\n    \"compressor\": {\n        \"id\": \"blosc\",\n        \"cname\": \"zstd\",\n        \"clevel\": 3,\n        \"shuffle\": 1\n    },\n    \"dtype\": \"&lt;f4\",\n    \"fill_value\": \"NaN\",\n    \"filters\": null,\n    \"order\": \"C\",\n    \"shape\": [365, 5000, 5000],\n    \"zarr_format\": 2\n}\n</code></pre>"},{"location":"reference/zarr-format/#zattrs","title":".zattrs","text":"<pre><code>{\n    \"title\": \"NDVI Time Series\",\n    \"source\": \"Sentinel-2\",\n    \"units\": \"dimensionless\",\n    \"valid_range\": [-1.0, 1.0]\n}\n</code></pre>"},{"location":"reference/zarr-format/#chunking","title":"Chunking","text":""},{"location":"reference/zarr-format/#chunk-size-selection","title":"Chunk Size Selection","text":"<p>Formula: Aim for 10-100 MB per chunk</p> <pre><code>import numpy as np\n\ndef optimal_chunks(shape, dtype, target_mb=10):\n    \"\"\"Calculate optimal chunk dimensions.\"\"\"\n    itemsize = np.dtype(dtype).itemsize\n    target_items = (target_mb * 1024 * 1024) / itemsize\n\n    # Distribute evenly across dimensions\n    chunk_dim = int(target_items ** (1/len(shape)))\n    chunks = tuple(min(chunk_dim, s) for s in shape)\n\n    return chunks\n\n# Example\nshape = (365, 5000, 5000)\nchunks = optimal_chunks(shape, 'float32', target_mb=10)\nprint(f\"Recommended chunks: {chunks}\")\n</code></pre>"},{"location":"reference/zarr-format/#access-pattern-optimization","title":"Access Pattern Optimization","text":""},{"location":"reference/zarr-format/#time-series-access","title":"Time Series Access","text":"<pre><code># Optimize for temporal access\nchunks = (1, 1000, 1000)  # Small time chunks, large spatial\n</code></pre>"},{"location":"reference/zarr-format/#spatial-access","title":"Spatial Access","text":"<pre><code># Optimize for spatial access\nchunks = (365, 100, 100)  # Large time chunks, small spatial\n</code></pre>"},{"location":"reference/zarr-format/#balanced-access","title":"Balanced Access","text":"<pre><code># Balanced for mixed access\nchunks = (10, 512, 512)\n</code></pre>"},{"location":"reference/zarr-format/#compression","title":"Compression","text":""},{"location":"reference/zarr-format/#blosc-recommended","title":"Blosc (Recommended)","text":"<p>Fast compression with multiple algorithms.</p> <pre><code>from numcodecs import Blosc\n\n# Fast compression (LZ4)\ncompressor = Blosc(cname='lz4', clevel=5, shuffle=Blosc.SHUFFLE)\n\n# Balanced (Zstandard)\ncompressor = Blosc(cname='zstd', clevel=3, shuffle=Blosc.SHUFFLE)\n\n# High compression (Zstandard)\ncompressor = Blosc(cname='zstd', clevel=9, shuffle=Blosc.BITSHUFFLE)\n\n# Use with Zarr\nimport zarr\nz = zarr.open('data.zarr', mode='w', shape=(1000, 1000), \n              chunks=(100, 100), compressor=compressor)\n</code></pre>"},{"location":"reference/zarr-format/#other-compressors","title":"Other Compressors","text":"<pre><code>from numcodecs import Zlib, GZip, BZ2, LZMA\n\n# Zlib (good compression, moderate speed)\ncompressor = Zlib(level=5)\n\n# GZip (widely compatible)\ncompressor = GZip(level=6)\n\n# BZ2 (high compression, slow)\ncompressor = BZ2(level=9)\n\n# LZMA (highest compression, slowest)\ncompressor = LZMA(preset=6)\n</code></pre>"},{"location":"reference/zarr-format/#compression-comparison","title":"Compression Comparison","text":"Compressor Speed Ratio Use Case LZ4 Very Fast Low Real-time processing Zstandard Fast Good General purpose Zlib Medium Good Compatibility BZ2 Slow High Archival LZMA Very Slow Highest Long-term storage"},{"location":"reference/zarr-format/#xarray-integration","title":"XArray Integration","text":""},{"location":"reference/zarr-format/#writing-to-zarr","title":"Writing to Zarr","text":"<pre><code>import xarray as xr\nfrom numcodecs import Blosc\n\n# Create dataset\nds = xr.Dataset({\n    'temperature': (['time', 'y', 'x'], data)\n})\n\n# Configure encoding\nencoding = {\n    'temperature': {\n        'compressor': Blosc(cname='zstd', clevel=3),\n        'chunks': (10, 512, 512)\n    }\n}\n\n# Write to Zarr\nds.to_zarr('data.zarr', encoding=encoding, consolidated=True)\n</code></pre>"},{"location":"reference/zarr-format/#reading-from-zarr","title":"Reading from Zarr","text":"<pre><code># Open Zarr store\nds = xr.open_zarr('data.zarr', consolidated=True)\n\n# With custom chunks\nds = xr.open_zarr('data.zarr', chunks={'time': 5})\n</code></pre>"},{"location":"reference/zarr-format/#appending-data","title":"Appending Data","text":"<pre><code># Initial write\nds1.to_zarr('timeseries.zarr', mode='w')\n\n# Append along time dimension\nds2.to_zarr('timeseries.zarr', append_dim='time')\n</code></pre>"},{"location":"reference/zarr-format/#cloud-storage","title":"Cloud Storage","text":""},{"location":"reference/zarr-format/#aws-s3","title":"AWS S3","text":"<pre><code>import s3fs\nimport xarray as xr\n\n# Create S3 filesystem\ns3 = s3fs.S3FileSystem(anon=False)\n\n# Write to S3\nstore = s3fs.S3Map(root='s3://my-bucket/data.zarr', s3=s3)\nds.to_zarr(store, mode='w', consolidated=True)\n\n# Read from S3\nds = xr.open_zarr(store, consolidated=True)\n</code></pre>"},{"location":"reference/zarr-format/#google-cloud-storage","title":"Google Cloud Storage","text":"<pre><code>import gcsfs\n\n# Create GCS filesystem\ngcs = gcsfs.GCSFileSystem(token='anon')\n\n# Write to GCS\nstore = gcsfs.GCSMap('gs://my-bucket/data.zarr', gcs=gcs)\nds.to_zarr(store, mode='w', consolidated=True)\n\n# Read from GCS\nds = xr.open_zarr(store, consolidated=True)\n</code></pre>"},{"location":"reference/zarr-format/#azure-blob-storage","title":"Azure Blob Storage","text":"<pre><code>import adlfs\n\n# Create Azure filesystem\nfs = adlfs.AzureBlobFileSystem(account_name='myaccount')\n\n# Write to Azure\nstore = fs.get_mapper('container/data.zarr')\nds.to_zarr(store, mode='w', consolidated=True)\n\n# Read from Azure\nds = xr.open_zarr(store, consolidated=True)\n</code></pre>"},{"location":"reference/zarr-format/#consolidated-metadata","title":"Consolidated Metadata","text":"<p>Improves performance by reducing number of reads.</p>"},{"location":"reference/zarr-format/#create-consolidated-metadata","title":"Create Consolidated Metadata","text":"<pre><code># During write\nds.to_zarr('data.zarr', consolidated=True)\n\n# After write\nfrom zarr.convenience import consolidate_metadata\nconsolidate_metadata('data.zarr')\n</code></pre>"},{"location":"reference/zarr-format/#use-consolidated-metadata","title":"Use Consolidated Metadata","text":"<pre><code># Read with consolidated metadata (faster)\nds = xr.open_zarr('data.zarr', consolidated=True)\n\n# Without consolidated metadata (slower)\nds = xr.open_zarr('data.zarr', consolidated=False)\n</code></pre>"},{"location":"reference/zarr-format/#parallel-io","title":"Parallel I/O","text":""},{"location":"reference/zarr-format/#parallel-writing","title":"Parallel Writing","text":"<pre><code>from dask.distributed import Client\nimport dask.array as da\n\nclient = Client()\n\n# Create Dask array\ndata = da.random.random((1000, 5000, 5000), chunks=(10, 500, 500))\n\n# Create dataset\nds = xr.Dataset({'data': (['time', 'y', 'x'], data)})\n\n# Parallel write\nds.to_zarr('large_data.zarr', compute=True, consolidated=True)\n</code></pre>"},{"location":"reference/zarr-format/#parallel-reading","title":"Parallel Reading","text":"<pre><code># Open with Dask chunks\nds = xr.open_zarr('large_data.zarr', chunks={'time': 10})\n\n# Operations are parallelized\nresult = ds.mean(dim='time').compute()\n</code></pre>"},{"location":"reference/zarr-format/#filters","title":"Filters","text":"<p>Apply transformations before compression.</p> <pre><code>from numcodecs import Delta, FixedScaleOffset\n\n# Delta encoding (for time series)\nfilters = [Delta(dtype='i4')]\n\n# Scale and offset\nfilters = [FixedScaleOffset(offset=0, scale=0.01, dtype='f4')]\n\n# Use with Zarr\nz = zarr.open('data.zarr', mode='w', shape=(1000,), \n              filters=filters, compressor=compressor)\n</code></pre>"},{"location":"reference/zarr-format/#groups-and-hierarchies","title":"Groups and Hierarchies","text":""},{"location":"reference/zarr-format/#creating-groups","title":"Creating Groups","text":"<pre><code>import zarr\n\n# Create root group\nroot = zarr.open_group('data.zarr', mode='w')\n\n# Create subgroups\ntemp_group = root.create_group('temperature')\nprecip_group = root.create_group('precipitation')\n\n# Create arrays in groups\ntemp_group.create_dataset('daily', shape=(365, 100, 100), chunks=(10, 50, 50))\nprecip_group.create_dataset('daily', shape=(365, 100, 100), chunks=(10, 50, 50))\n</code></pre>"},{"location":"reference/zarr-format/#reading-groups","title":"Reading Groups","text":"<pre><code># Open group\nroot = zarr.open_group('data.zarr', mode='r')\n\n# Access subgroups\ntemp = root['temperature']\nprecip = root['precipitation']\n\n# Access arrays\ndaily_temp = temp['daily']\n</code></pre>"},{"location":"reference/zarr-format/#best-practices","title":"Best Practices","text":""},{"location":"reference/zarr-format/#1-use-consolidated-metadata","title":"1. Use Consolidated Metadata","text":"<pre><code># Always use for cloud storage\nds.to_zarr('s3://bucket/data.zarr', consolidated=True)\n</code></pre>"},{"location":"reference/zarr-format/#2-choose-appropriate-chunks","title":"2. Choose Appropriate Chunks","text":"<pre><code># Match access patterns\n# Time series: large time chunks\nchunks = (100, 512, 512)\n\n# Spatial: large spatial chunks\nchunks = (10, 1024, 1024)\n</code></pre>"},{"location":"reference/zarr-format/#3-use-compression","title":"3. Use Compression","text":"<pre><code># Always compress for cloud storage\ncompressor = Blosc(cname='zstd', clevel=3)\n</code></pre>"},{"location":"reference/zarr-format/#4-avoid-small-chunks","title":"4. Avoid Small Chunks","text":"<pre><code># Good: ~10-100 MB per chunk\nchunks = (10, 512, 512)  # ~10 MB for float32\n\n# Bad: Too small\nchunks = (1, 10, 10)  # ~400 bytes\n</code></pre>"},{"location":"reference/zarr-format/#5-use-appropriate-data-types","title":"5. Use Appropriate Data Types","text":"<pre><code># Good: Use smallest appropriate dtype\nds = ds.astype('float32')\n\n# Bad: Unnecessary precision\nds = ds.astype('float64')\n</code></pre>"},{"location":"reference/zarr-format/#performance-optimization","title":"Performance Optimization","text":""},{"location":"reference/zarr-format/#chunk-size-impact","title":"Chunk Size Impact","text":"<pre><code>import time\n\n# Test different chunk sizes\nchunk_sizes = [(5, 256, 256), (10, 512, 512), (20, 1024, 1024)]\n\nfor chunks in chunk_sizes:\n    start = time.time()\n    ds.chunk(chunks).to_zarr(f'test_{chunks[0]}.zarr', mode='w')\n    write_time = time.time() - start\n\n    start = time.time()\n    loaded = xr.open_zarr(f'test_{chunks[0]}.zarr').compute()\n    read_time = time.time() - start\n\n    print(f\"Chunks {chunks}: Write={write_time:.2f}s, Read={read_time:.2f}s\")\n</code></pre>"},{"location":"reference/zarr-format/#compression-impact","title":"Compression Impact","text":"<pre><code># Test compression algorithms\ncompressors = {\n    'none': None,\n    'lz4': Blosc(cname='lz4', clevel=5),\n    'zstd': Blosc(cname='zstd', clevel=3),\n    'zlib': Zlib(level=5)\n}\n\nfor name, comp in compressors.items():\n    encoding = {'data': {'compressor': comp}}\n    ds.to_zarr(f'test_{name}.zarr', encoding=encoding, mode='w')\n\n    # Check size\n    size = sum(f.stat().st_size for f in Path(f'test_{name}.zarr').rglob('*'))\n    print(f\"{name}: {size/1e6:.2f} MB\")\n</code></pre>"},{"location":"reference/zarr-format/#troubleshooting","title":"Troubleshooting","text":""},{"location":"reference/zarr-format/#issue-slow-reads","title":"Issue: Slow Reads","text":"<pre><code># Solution 1: Use consolidated metadata\nds = xr.open_zarr('data.zarr', consolidated=True)\n\n# Solution 2: Check chunk size\nprint(ds.chunks)\n\n# Solution 3: Use appropriate chunks for access pattern\nds = xr.open_zarr('data.zarr', chunks={'time': 10})\n</code></pre>"},{"location":"reference/zarr-format/#issue-large-file-size","title":"Issue: Large File Size","text":"<pre><code># Solution: Use compression\nencoding = {\n    'var': {'compressor': Blosc(cname='zstd', clevel=5)}\n}\nds.to_zarr('data.zarr', encoding=encoding)\n</code></pre>"},{"location":"reference/zarr-format/#issue-memory-errors","title":"Issue: Memory Errors","text":"<pre><code># Solution: Use smaller chunks\nds = xr.open_zarr('data.zarr', chunks={'time': 1})\n</code></pre>"},{"location":"reference/zarr-format/#additional-resources","title":"Additional Resources","text":"<ul> <li>Zarr Documentation</li> <li>Zarr Tutorial</li> <li>Zarr Specification</li> <li>XArray Zarr Guide</li> <li>Pangeo Zarr Examples</li> </ul>"},{"location":"reference/zarr-format/#quick-reference","title":"Quick Reference","text":""},{"location":"reference/zarr-format/#common-operations","title":"Common Operations","text":"<pre><code>import zarr\nimport xarray as xr\n\n# Create Zarr array\nz = zarr.open('data.zarr', mode='w', shape=(1000, 1000), \n              chunks=(100, 100), dtype='f4')\n\n# Write XArray to Zarr\nds.to_zarr('data.zarr', mode='w', consolidated=True)\n\n# Read Zarr with XArray\nds = xr.open_zarr('data.zarr', consolidated=True)\n\n# Append data\nds.to_zarr('data.zarr', append_dim='time')\n\n# Consolidate metadata\nfrom zarr.convenience import consolidate_metadata\nconsolidate_metadata('data.zarr')\n</code></pre>"},{"location":"reference/zarr-format/#recommended-settings","title":"Recommended Settings","text":"<pre><code># General purpose\ncompressor = Blosc(cname='zstd', clevel=3, shuffle=Blosc.SHUFFLE)\nchunks = (10, 512, 512)  # For (time, y, x)\n\n# High compression (archival)\ncompressor = Blosc(cname='zstd', clevel=9, shuffle=Blosc.BITSHUFFLE)\n\n# Fast I/O (real-time)\ncompressor = Blosc(cname='lz4', clevel=5, shuffle=Blosc.SHUFFLE)\n</code></pre>"},{"location":"resources/datasets/","title":"Datasets","text":""},{"location":"resources/datasets/#datasets","title":"Datasets","text":""},{"location":"resources/datasets/#overview","title":"Overview","text":"<p>This page provides information about publicly available datasets for cloud-native remote sensing analysis.</p>"},{"location":"resources/datasets/#satellite-imagery","title":"Satellite Imagery","text":""},{"location":"resources/datasets/#sentinel-2","title":"Sentinel-2","text":"<p>Provider: European Space Agency (ESA) Resolution: 10m, 20m, 60m Revisit: 5 days Bands: 13 spectral bands Coverage: Global  </p> <p>Access via STAC:</p> <pre><code>catalog = pystac_client.Client.open(\n    'https://earth-search.aws.element84.com/v1')\nsearch = catalog.search(\n    collections=['sentinel-2-c1-l2a'],\n    bbox=bbox,\n    datetime='2023-01-01/2023-12-31'\n)\n</code></pre> <p>Direct Access: AWS Open Data</p>"},{"location":"resources/datasets/#landsat-89","title":"Landsat 8/9","text":"<p>Provider: USGS/NASA Resolution: 30m (15m panchromatic) Revisit: 16 days (8 days combined) Bands: 11 spectral bands Coverage: Global  </p> <p>Access via STAC:</p> <pre><code>search = catalog.search(\n    collections=['landsat-c2-l2'],\n    bbox=bbox,\n    datetime='2023-01-01/2023-12-31'\n)\n</code></pre> <p>Direct Access: AWS Open Data</p>"},{"location":"resources/datasets/#modis","title":"MODIS","text":"<p>Provider: NASA Resolution: 250m, 500m, 1km Revisit: Daily Products: NDVI, LST, LAI, etc. Coverage: Global  </p> <p>Access via Earth Engine:</p> <pre><code>ds = xr.open_dataset(\n    'ee://MODIS/006/MOD13A2',\n    engine='ee',\n    geometry=geometry,\n    scale=1000\n)\n</code></pre>"},{"location":"resources/datasets/#climate-data","title":"Climate Data","text":""},{"location":"resources/datasets/#era5","title":"ERA5","text":"<p>Provider: ECMWF Resolution: ~25km Temporal: Hourly since 1940 Variables: Temperature, precipitation, wind, etc.  </p> <p>Access:</p> <pre><code>ds = xr.open_dataset(\n    'ee://ECMWF/ERA5/DAILY',\n    engine='ee',\n    geometry=geometry\n)\n</code></pre>"},{"location":"resources/datasets/#chirps","title":"CHIRPS","text":"<p>Provider: UCSB/USGS Resolution: 5km Temporal: Daily since 1981 Variable: Precipitation  </p>"},{"location":"resources/datasets/#terrain-data","title":"Terrain Data","text":""},{"location":"resources/datasets/#srtm","title":"SRTM","text":"<p>Provider: NASA/USGS Resolution: 30m, 90m Coverage: 60\u00b0N to 56\u00b0S Product: Digital Elevation Model  </p> <p>Access:</p> <pre><code>dem = xr.open_dataset(\n    'ee://USGS/SRTMGL1_003',\n    engine='ee',\n    geometry=geometry\n)\n</code></pre>"},{"location":"resources/datasets/#alos-world-3d","title":"ALOS World 3D","text":"<p>Provider: JAXA Resolution: 30m Coverage: Global Product: DSM, DTM  </p>"},{"location":"resources/datasets/#land-cover","title":"Land Cover","text":""},{"location":"resources/datasets/#esa-worldcover","title":"ESA WorldCover","text":"<p>Provider: ESA Resolution: 10m Year: 2020, 2021 Classes: 11 land cover types  </p>"},{"location":"resources/datasets/#dynamic-world","title":"Dynamic World","text":"<p>Provider: Google Resolution: 10m Temporal: 2015-present Classes: 9 land cover types  </p>"},{"location":"resources/datasets/#stac-catalogs","title":"STAC Catalogs","text":""},{"location":"resources/datasets/#earth-search-element84","title":"Earth Search (Element84)","text":"<ul> <li>URL: https://earth-search.aws.element84.com/v1</li> <li>Collections: Sentinel-2, Landsat</li> <li>Coverage: Global</li> <li>Free: Yes</li> </ul>"},{"location":"resources/datasets/#microsoft-planetary-computer","title":"Microsoft Planetary Computer","text":"<ul> <li>URL: https://planetarycomputer.microsoft.com/api/stac/v1</li> <li>Collections: 50+ datasets</li> <li>Coverage: Global</li> <li>Free: Yes (registration required)</li> </ul>"},{"location":"resources/datasets/#google-earth-engine","title":"Google Earth Engine","text":"<ul> <li>Collections: 1000+ datasets</li> <li>Access: Via XEE</li> <li>Free: Yes (for research/education)</li> </ul>"},{"location":"resources/datasets/#data-access-patterns","title":"Data Access Patterns","text":""},{"location":"resources/datasets/#streaming-recommended","title":"Streaming (Recommended)","text":"<pre><code># Don't download - stream from cloud\nds = xr.open_dataset('https://...', chunks='auto')\n</code></pre>"},{"location":"resources/datasets/#partial-downloads","title":"Partial Downloads","text":"<pre><code># Download only what you need\nds = xr.open_dataset('https://...').sel(\n    time=slice('2023-01-01', '2023-01-31'),\n    lat=slice(20, 30),\n    lon=slice(70, 80)\n)\n</code></pre>"},{"location":"resources/datasets/#zarr-archives","title":"Zarr Archives","text":"<pre><code># Efficient cloud access\nds = xr.open_zarr('s3://bucket/data.zarr')\n</code></pre>"},{"location":"resources/datasets/#storage-locations","title":"Storage Locations","text":""},{"location":"resources/datasets/#aws-s3","title":"AWS S3","text":"<ul> <li>Sentinel-2: <code>s3://sentinel-s2-l2a/</code></li> <li>Landsat: <code>s3://usgs-landsat/</code></li> </ul>"},{"location":"resources/datasets/#google-cloud-storage","title":"Google Cloud Storage","text":"<ul> <li>Sentinel-2: <code>gs://gcp-public-data-sentinel-2/</code></li> <li>Landsat: <code>gs://gcp-public-data-landsat/</code></li> </ul>"},{"location":"resources/datasets/#azure-blob-storage","title":"Azure Blob Storage","text":"<ul> <li>Sentinel-2: Available via Planetary Computer</li> </ul>"},{"location":"resources/datasets/#best-practices","title":"Best Practices","text":"<ol> <li>Use STAC for data discovery</li> <li>Stream data instead of downloading</li> <li>Apply filters server-side when possible</li> <li>Use appropriate resolution for your analysis</li> <li>Leverage cloud-optimized formats (COG, Zarr)</li> </ol>"},{"location":"resources/datasets/#additional-resources","title":"Additional Resources","text":"<ul> <li>STAC Index</li> <li>AWS Open Data Registry</li> <li>Google Earth Engine Catalog</li> <li>Planetary Computer Catalog</li> </ul>"},{"location":"resources/reading/","title":"Further Reading","text":""},{"location":"resources/reading/#further-reading","title":"Further Reading","text":"<p>Curated resources for deepening your understanding of cloud-native remote sensing.</p>"},{"location":"resources/reading/#books","title":"Books","text":""},{"location":"resources/reading/#remote-sensing","title":"Remote Sensing","text":"<ul> <li>Remote Sensing and Image Interpretation by Lillesand, Kiefer, and Chipman</li> <li>Introduction to Remote Sensing by Campbell and Wynne</li> </ul>"},{"location":"resources/reading/#python-for-geospatial","title":"Python for Geospatial","text":"<ul> <li>Python for Geospatial Data Analysis by Bonny P. McClain</li> <li>Geoprocessing with Python by Chris Garrard</li> </ul>"},{"location":"resources/reading/#online-courses","title":"Online Courses","text":""},{"location":"resources/reading/#pangeo-tutorial","title":"Pangeo Tutorial","text":"<ul> <li>URL: tutorial.pangeo.io</li> <li>Topics: XArray, Dask, cloud-native workflows</li> </ul>"},{"location":"resources/reading/#earth-engine","title":"Earth Engine","text":"<ul> <li>URL: developers.google.com/earth-engine</li> <li>Topics: Earth Engine fundamentals, JavaScript/Python API</li> </ul>"},{"location":"resources/reading/#carpentries-geospatial","title":"Carpentries Geospatial","text":"<ul> <li>URL: datacarpentry.org/geospatial-workshop</li> <li>Topics: Raster and vector data with Python</li> </ul>"},{"location":"resources/reading/#documentation","title":"Documentation","text":""},{"location":"resources/reading/#xarray","title":"XArray","text":"<ul> <li>User Guide</li> <li>Tutorial</li> </ul>"},{"location":"resources/reading/#dask","title":"Dask","text":"<ul> <li>Best Practices</li> <li>Array Documentation</li> </ul>"},{"location":"resources/reading/#stac","title":"STAC","text":"<ul> <li>Specification</li> <li>STAC Index</li> </ul>"},{"location":"resources/reading/#community","title":"Community","text":""},{"location":"resources/reading/#pangeo","title":"Pangeo","text":"<ul> <li>Website: pangeo.io</li> <li>Discourse: discourse.pangeo.io</li> <li>GitHub: github.com/pangeo-data</li> </ul>"},{"location":"resources/reading/#earth-engine-community","title":"Earth Engine Community","text":"<ul> <li>Forum: groups.google.com/g/google-earth-engine-developers</li> <li>GitHub: github.com/google/earthengine-api</li> </ul>"},{"location":"resources/reading/#blogs-and-articles","title":"Blogs and Articles","text":""},{"location":"resources/reading/#pangeo-blog","title":"Pangeo Blog","text":"<ul> <li>medium.com/pangeo</li> </ul>"},{"location":"resources/reading/#element-84-blog","title":"Element 84 Blog","text":"<ul> <li>element84.com/blog</li> </ul>"},{"location":"resources/reading/#research-papers","title":"Research Papers","text":""},{"location":"resources/reading/#cloud-native-geospatial","title":"Cloud-Native Geospatial","text":"<ul> <li>\"Cloud-Native Geospatial Data Management and Analysis\" (Abernathey et al., 2021)</li> <li>\"Analysis Ready Data For Land\" (Lewis et al., 2018)</li> </ul>"},{"location":"resources/reading/#stac_1","title":"STAC","text":"<ul> <li>\"The SpatioTemporal Asset Catalog (STAC) Specification\" (Holmes et al., 2020)</li> </ul>"},{"location":"resources/tools/","title":"Tools and Libraries","text":""},{"location":"resources/tools/#tools-and-libraries","title":"Tools and Libraries","text":"<p>Essential tools and libraries for cloud-native remote sensing with Python.</p>"},{"location":"resources/tools/#core-libraries","title":"Core Libraries","text":""},{"location":"resources/tools/#xarray","title":"XArray","text":"<ul> <li>Purpose: Multi-dimensional labeled arrays</li> <li>Website: xarray.dev</li> <li>Installation: <code>pip install xarray</code></li> </ul>"},{"location":"resources/tools/#dask","title":"Dask","text":"<ul> <li>Purpose: Parallel computing</li> <li>Website: dask.org</li> <li>Installation: <code>pip install dask[complete]</code></li> </ul>"},{"location":"resources/tools/#rioxarray","title":"Rioxarray","text":"<ul> <li>Purpose: Geospatial extensions for XArray</li> <li>Website: corteva.github.io/rioxarray</li> <li>Installation: <code>pip install rioxarray</code></li> </ul>"},{"location":"resources/tools/#data-access","title":"Data Access","text":""},{"location":"resources/tools/#pystac-client","title":"pystac-client","text":"<ul> <li>Purpose: STAC catalog searching</li> <li>Website: pystac-client.readthedocs.io</li> <li>Installation: <code>pip install pystac-client</code></li> </ul>"},{"location":"resources/tools/#odc-stac","title":"odc-stac","text":"<ul> <li>Purpose: Load STAC items into XArray</li> <li>Website: odc-stac.readthedocs.io</li> <li>Installation: <code>pip install odc-stac</code></li> </ul>"},{"location":"resources/tools/#xee","title":"XEE","text":"<ul> <li>Purpose: Earth Engine integration</li> <li>Website: github.com/google/Xee</li> <li>Installation: <code>pip install xee</code></li> </ul>"},{"location":"resources/tools/#visualization","title":"Visualization","text":""},{"location":"resources/tools/#matplotlib","title":"Matplotlib","text":"<ul> <li>Purpose: Static plotting</li> <li>Installation: <code>pip install matplotlib</code></li> </ul>"},{"location":"resources/tools/#hvplot","title":"Hvplot","text":"<ul> <li>Purpose: Interactive visualizations</li> <li>Installation: <code>pip install hvplot</code></li> </ul>"},{"location":"resources/tools/#geemap","title":"Geemap","text":"<ul> <li>Purpose: Interactive Earth Engine maps</li> <li>Installation: <code>pip install geemap</code></li> </ul>"},{"location":"resources/tools/#storage-formats","title":"Storage Formats","text":""},{"location":"resources/tools/#zarr","title":"Zarr","text":"<ul> <li>Purpose: Cloud-optimized arrays</li> <li>Installation: <code>pip install zarr</code></li> </ul>"},{"location":"resources/tools/#netcdf4","title":"NetCDF4","text":"<ul> <li>Purpose: Self-describing data format</li> <li>Installation: <code>pip install netcdf4</code></li> </ul>"},{"location":"resources/tools/#cloud-storage","title":"Cloud Storage","text":""},{"location":"resources/tools/#s3fs","title":"s3fs","text":"<ul> <li>Purpose: AWS S3 filesystem</li> <li>Installation: <code>pip install s3fs</code></li> </ul>"},{"location":"resources/tools/#gcsfs","title":"gcsfs","text":"<ul> <li>Purpose: Google Cloud Storage</li> <li>Installation: <code>pip install gcsfs</code></li> </ul>"},{"location":"resources/tools/#advanced-analysis","title":"Advanced Analysis","text":"Library Purpose Installation spyndex Standard spectral indices <code>pip install spyndex</code> pymcdm Decision making models <code>pip install pymcdm</code> tslearn Time series analysis <code>pip install tslearn</code> libpysal Spatial weights &amp; stats <code>pip install libpysal</code> spopt Spatial optimization <code>pip install spopt</code> scikit-learn Machine learning <code>pip install scikit-learn</code> osmnx OpenStreetMap networks <code>pip install osmnx</code> tensorflow Deep learning <code>pip install tensorflow</code>"},{"location":"resources/tools/#complete-installation","title":"Complete Installation","text":"<pre><code>pip install xarray dask[complete] rioxarray pystac-client odc-stac xee \\\n    matplotlib hvplot geemap zarr netcdf4 s3fs gcsfs earthengine-api \\\n    spyndex pymcdm tslearn libpysal spopt scikit-learn osmnx tensorflow\n</code></pre>"}]}