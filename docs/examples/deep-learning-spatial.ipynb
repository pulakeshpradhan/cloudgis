{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b036db5",
   "metadata": {},
   "source": [
    "# Deep Learning Architectures in Spatial Analysis\n",
    "\n",
    "Leverage Convolutional Neural Networks (CNNs) and Transformers for advanced spatial feature extraction using XEE, TensorFlow, and PyTorch.\n",
    "\n",
    "## Overview\n",
    "\n",
    "This example covers:\n",
    "\n",
    "1. **Convolutional Neural Networks (CNN)**: Implementing a U-Net for semantic segmentation.\n",
    "2. **Vision Transformers (ViT)**: Concept of global attention in remote sensing.\n",
    "3. **Graph Neural Networks (GNN)**: Analyzing irregularly spaced spatial data.\n",
    "4. **Self-Supervised Learning**: Pre-training on massive unlabelled satellite imagery.\n",
    "\n",
    "## Step 1: Data Preparation for Deep Learning\n",
    "\n",
    "Deep learning requires smaller \"patches\" or \"tiles\" rather than massive full-scene images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84467cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee\n",
    "import xarray as xr\n",
    "import xee\n",
    "import numpy as np\n",
    "\n",
    "# Load Sentinel-2\n",
    "roi = ee.Geometry.Point([77.1, 28.7]).buffer(1000).bounds()\n",
    "s2 = ee.ImageCollection(\"COPERNICUS/S2_SR_HARMONIZED\").filterBounds(roi).median().clip(roi)\n",
    "\n",
    "ds = xr.open_dataset(s2, engine='ee', geometry=roi, scale=10).compute()\n",
    "\n",
    "# Stack bands and create patches\n",
    "def make_patches(da, size=64):\n",
    "    # (Simplified patch creation logic)\n",
    "    data = da[['B2', 'B3', 'B4', 'B8']].to_array().values\n",
    "    c, h, w = data.shape\n",
    "    patch = data[:, :size, :size]\n",
    "    return np.expand_dims(np.moveaxis(patch, 0, -1), 0) # (Batch, H, W, C)\n",
    "\n",
    "patch_data = make_patches(ds)\n",
    "print(f\"Input shape: {patch_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857878d5",
   "metadata": {},
   "source": [
    "## Step 2: CNN Architecture (U-Net in TensorFlow/Keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2611c75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "def build_unet(input_shape=(64, 64, 4)):\n",
    "    inputs = layers.Input(input_shape)\n",
    "    \n",
    "    # Downsample\n",
    "    c1 = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    p1 = layers.MaxPooling2D((2, 2))(c1)\n",
    "    \n",
    "    # Bottleneck\n",
    "    c2 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(p1)\n",
    "    \n",
    "    # Upsample\n",
    "    u3 = layers.UpSampling2D((2, 2))(c2)\n",
    "    u3 = layers.concatenate([u3, c1])\n",
    "    outputs = layers.Conv2D(1, (1, 1), activation='sigmoid')(u3)\n",
    "    \n",
    "    return models.Model(inputs, outputs)\n",
    "\n",
    "model = build_unet()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193b6958",
   "metadata": {},
   "source": [
    "## Step 3: Transformers in Vision (Terminology)\n",
    "\n",
    "Unlike CNNs, Transformers use **Self-Attention** to capture long-range dependencies in satellite imagery. This is particularly useful for detecting large-scale land forms or complex urban patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e185f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conceptual Vision Transformer block\n",
    "class AttentionBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.mha = layers.MultiHeadAttention(num_heads=8, key_dim=embed_dim)\n",
    "        self.norm = layers.LayerNormalization()\n",
    "\n",
    "    def call(self, x):\n",
    "        attn_out = self.mha(x, x)\n",
    "        return self.norm(x + attn_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e3dcd4",
   "metadata": {},
   "source": [
    "## Step 4: Graph Neural Networks (GNN)\n",
    "\n",
    "Used when data is not a grid (e.g., weather stations, social sensing data, or object-based image analysis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22570f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conceptual GNN Layer (PyTorch Geometric style)\n",
    "# Each node (pixel/object) aggregates information from its spatial neighbors.\n",
    "# x_new = f(x, neighbors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efeeff05",
   "metadata": {},
   "source": [
    "## Step 5: Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab719e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# model.fit(train_gen, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6848c263",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "!!! success \"Summary\"\n",
    "    - **CNNs**: The standard for pixel-wise classification and object detection.\n",
    "    - **Transformers**: Rising popularity for large-scale \"Foundation Models\".\n",
    "    - **Infrastructure**: Processing DL models requires GPU; it's often best to export XEE data to TFRecord or Zarr for training.\n",
    "\n",
    "\u2192 Back to [Index](../index.ipynb)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}