{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12f6264b",
   "metadata": {},
   "source": [
    "# Working with Zarr\n",
    "\n",
    "## Overview\n",
    "\n",
    "[Zarr](https://zarr.readthedocs.io/) is a cloud-optimized format for storing chunked, compressed N-dimensional arrays. It's designed for efficient storage and access of large scientific datasets, making it ideal for remote sensing applications.\n",
    "\n",
    "**Key Features:**\n",
    "\n",
    "- Cloud-native storage format\n",
    "- Chunked and compressed arrays\n",
    "- Parallel read/write operations\n",
    "- Works seamlessly with Dask and XArray\n",
    "- Supports multiple storage backends (local, S3, GCS, Azure)\n",
    "\n",
    "## Why Zarr?\n",
    "\n",
    "### Traditional Formats (NetCDF, HDF5)\n",
    "\n",
    "- Designed for local filesystems\n",
    "- Poor performance over HTTP\n",
    "- Sequential access patterns\n",
    "- Difficult to parallelize\n",
    "\n",
    "### Zarr Advantages\n",
    "\n",
    "- \u2705 Optimized for cloud storage\n",
    "- \u2705 Parallel read/write\n",
    "- \u2705 Efficient partial reads\n",
    "- \u2705 Multiple compression algorithms\n",
    "- \u2705 Language-agnostic specification\n",
    "\n",
    "## Basic Zarr Operations\n",
    "\n",
    "### Creating a Zarr Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2f5ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zarr\n",
    "import numpy as np\n",
    "\n",
    "# Create a Zarr array\n",
    "z = zarr.open('data.zarr', mode='w', shape=(10000, 10000), \n",
    "              chunks=(1000, 1000), dtype='f4', \n",
    "              compressor=zarr.Blosc(cname='zstd', clevel=3))\n",
    "\n",
    "# Write data\n",
    "data = np.random.random((10000, 10000))\n",
    "z[:] = data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1d1215",
   "metadata": {},
   "source": [
    "### Reading Zarr Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ae864b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open existing Zarr array\n",
    "z = zarr.open('data.zarr', mode='r')\n",
    "\n",
    "# Read subset\n",
    "subset = z[1000:2000, 1000:2000]\n",
    "\n",
    "# Read entire array\n",
    "all_data = z[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9756745",
   "metadata": {},
   "source": [
    "### Zarr with XArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adba46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "\n",
    "# Create XArray dataset\n",
    "ds = xr.Dataset({\n",
    "    'temperature': (['time', 'y', 'x'], np.random.random((365, 1000, 1000))),\n",
    "    'precipitation': (['time', 'y', 'x'], np.random.random((365, 1000, 1000)))\n",
    "})\n",
    "\n",
    "# Save to Zarr\n",
    "ds.to_zarr('climate_data.zarr', mode='w')\n",
    "\n",
    "# Load from Zarr\n",
    "ds_loaded = xr.open_zarr('climate_data.zarr')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ccbee9",
   "metadata": {},
   "source": [
    "## Chunking Strategies\n",
    "\n",
    "Chunking is critical for performance. Choose chunk sizes based on your access patterns.\n",
    "\n",
    "### Time-Series Access\n",
    "\n",
    "If you frequently access time slices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d39781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize for time-series access\n",
    "ds.to_zarr('timeseries.zarr', \n",
    "           encoding={\n",
    "               'temperature': {'chunks': (1, 1000, 1000)}\n",
    "           })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73f056c",
   "metadata": {},
   "source": [
    "### Spatial Access\n",
    "\n",
    "If you frequently access spatial subsets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca232ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize for spatial access\n",
    "ds.to_zarr('spatial.zarr',\n",
    "           encoding={\n",
    "               'temperature': {'chunks': (365, 100, 100)}\n",
    "           })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21b457a",
   "metadata": {},
   "source": [
    "### Balanced Chunking\n",
    "\n",
    "For mixed access patterns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380d258e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balanced chunks\n",
    "ds.to_zarr('balanced.zarr',\n",
    "           encoding={\n",
    "               'temperature': {'chunks': (10, 512, 512)}\n",
    "           })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01150905",
   "metadata": {},
   "source": [
    "### Chunk Size Guidelines\n",
    "\n",
    "!!! tip \"Optimal Chunk Sizes\"\n",
    "    - **Minimum**: 1 MB per chunk\n",
    "    - **Maximum**: 100 MB per chunk\n",
    "    - **Optimal**: 10-50 MB per chunk\n",
    "    - **Rule of thumb**: Aim for ~10,000 chunks total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8e0c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate chunk size\n",
    "import numpy as np\n",
    "\n",
    "def calculate_chunk_size(shape, dtype, target_mb=10):\n",
    "    \"\"\"Calculate optimal chunk dimensions.\"\"\"\n",
    "    itemsize = np.dtype(dtype).itemsize\n",
    "    target_bytes = target_mb * 1024 * 1024\n",
    "    total_items = target_bytes / itemsize\n",
    "    \n",
    "    # Distribute across dimensions\n",
    "    chunk_dim = int(total_items ** (1/len(shape)))\n",
    "    chunks = tuple(min(chunk_dim, s) for s in shape)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Example\n",
    "shape = (365, 5000, 5000)\n",
    "chunks = calculate_chunk_size(shape, 'float32', target_mb=10)\n",
    "print(f\"Recommended chunks: {chunks}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb0137f",
   "metadata": {},
   "source": [
    "## Compression\n",
    "\n",
    "Zarr supports multiple compression algorithms:\n",
    "\n",
    "### Blosc (Recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5804202d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zarr import Blosc\n",
    "\n",
    "# Fast compression\n",
    "compressor = Blosc(cname='lz4', clevel=5, shuffle=Blosc.SHUFFLE)\n",
    "\n",
    "# Balanced\n",
    "compressor = Blosc(cname='zstd', clevel=3, shuffle=Blosc.SHUFFLE)\n",
    "\n",
    "# High compression\n",
    "compressor = Blosc(cname='zstd', clevel=9, shuffle=Blosc.BITSHUFFLE)\n",
    "\n",
    "# Use with XArray\n",
    "ds.to_zarr('compressed.zarr',\n",
    "           encoding={\n",
    "               'temperature': {'compressor': compressor}\n",
    "           })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de451f2",
   "metadata": {},
   "source": [
    "### Other Compressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78519ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numcodecs import Zlib, GZip, BZ2, LZMA\n",
    "\n",
    "# Zlib (good compression)\n",
    "compressor = Zlib(level=5)\n",
    "\n",
    "# GZip (compatible)\n",
    "compressor = GZip(level=6)\n",
    "\n",
    "# LZMA (high compression, slow)\n",
    "compressor = LZMA(preset=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc061429",
   "metadata": {},
   "source": [
    "### Compression Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07099cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "compressors = {\n",
    "    'none': None,\n",
    "    'lz4': Blosc(cname='lz4', clevel=5),\n",
    "    'zstd': Blosc(cname='zstd', clevel=3),\n",
    "    'zlib': Zlib(level=5)\n",
    "}\n",
    "\n",
    "data = np.random.random((1000, 1000, 100))\n",
    "\n",
    "for name, comp in compressors.items():\n",
    "    start = time.time()\n",
    "    z = zarr.open(f'test_{name}.zarr', mode='w',\n",
    "                  shape=data.shape, chunks=(100, 100, 10),\n",
    "                  compressor=comp)\n",
    "    z[:] = data\n",
    "    write_time = time.time() - start\n",
    "    \n",
    "    size = sum(f.stat().st_size for f in Path(f'test_{name}.zarr').rglob('*') if f.is_file())\n",
    "    \n",
    "    print(f\"{name:10s} - Size: {size/1e6:6.2f} MB, Time: {write_time:5.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741566d4",
   "metadata": {},
   "source": [
    "## Cloud Storage\n",
    "\n",
    "### AWS S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7ae63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import s3fs\n",
    "\n",
    "# Create S3 filesystem\n",
    "s3 = s3fs.S3FileSystem(anon=False)\n",
    "\n",
    "# Write to S3\n",
    "store = s3fs.S3Map(root='s3://my-bucket/data.zarr', s3=s3)\n",
    "ds.to_zarr(store, mode='w')\n",
    "\n",
    "# Read from S3\n",
    "ds_s3 = xr.open_zarr(store)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6e2d5d",
   "metadata": {},
   "source": [
    "### Google Cloud Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156a80b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gcsfs\n",
    "\n",
    "# Create GCS filesystem\n",
    "gcs = gcsfs.GCSFileSystem(token='anon')\n",
    "\n",
    "# Write to GCS\n",
    "store = gcsfs.GCSMap('gs://my-bucket/data.zarr', gcs=gcs)\n",
    "ds.to_zarr(store, mode='w')\n",
    "\n",
    "# Read from GCS\n",
    "ds_gcs = xr.open_zarr(store)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2030cc9b",
   "metadata": {},
   "source": [
    "### Azure Blob Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6772ba48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import adlfs\n",
    "\n",
    "# Create Azure filesystem\n",
    "fs = adlfs.AzureBlobFileSystem(account_name='myaccount')\n",
    "\n",
    "# Write to Azure\n",
    "store = fs.get_mapper('container/data.zarr')\n",
    "ds.to_zarr(store, mode='w')\n",
    "\n",
    "# Read from Azure\n",
    "ds_azure = xr.open_zarr(store)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745d6d29",
   "metadata": {},
   "source": [
    "## Appending Data\n",
    "\n",
    "Zarr supports appending along dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34403db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial dataset\n",
    "ds1 = xr.Dataset({\n",
    "    'temperature': (['time', 'y', 'x'], np.random.random((10, 100, 100)))\n",
    "})\n",
    "ds1.to_zarr('timeseries.zarr', mode='w')\n",
    "\n",
    "# Append new time steps\n",
    "ds2 = xr.Dataset({\n",
    "    'temperature': (['time', 'y', 'x'], np.random.random((5, 100, 100)))\n",
    "})\n",
    "ds2.to_zarr('timeseries.zarr', append_dim='time')\n",
    "\n",
    "# Load combined dataset\n",
    "ds_combined = xr.open_zarr('timeseries.zarr')\n",
    "print(ds_combined.dims)  # time: 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a9fc8b",
   "metadata": {},
   "source": [
    "## Parallel Writing\n",
    "\n",
    "Use Dask for parallel writes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52aa108",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "client = Client()\n",
    "\n",
    "# Create large dataset with Dask\n",
    "ds_large = xr.Dataset({\n",
    "    'data': (['time', 'y', 'x'], \n",
    "             da.random.random((1000, 5000, 5000), chunks=(10, 500, 500)))\n",
    "})\n",
    "\n",
    "# Parallel write to Zarr\n",
    "ds_large.to_zarr('large_data.zarr', \n",
    "                 compute=True,\n",
    "                 consolidated=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62d275b",
   "metadata": {},
   "source": [
    "## Metadata and Attributes\n",
    "\n",
    "### Store Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f202a71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add attributes\n",
    "ds.attrs['title'] = 'Climate Data'\n",
    "ds.attrs['source'] = 'Satellite Observations'\n",
    "ds.attrs['processing_date'] = '2024-01-01'\n",
    "\n",
    "# Variable attributes\n",
    "ds['temperature'].attrs['units'] = 'Kelvin'\n",
    "ds['temperature'].attrs['long_name'] = 'Air Temperature'\n",
    "\n",
    "# Save with metadata\n",
    "ds.to_zarr('data_with_metadata.zarr')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f650fe9d",
   "metadata": {},
   "source": [
    "### Consolidated Metadata\n",
    "\n",
    "Improve performance with consolidated metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e3c07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write with consolidated metadata\n",
    "ds.to_zarr('data.zarr', consolidated=True)\n",
    "\n",
    "# Or consolidate existing Zarr\n",
    "from zarr.convenience import consolidate_metadata\n",
    "consolidate_metadata('data.zarr')\n",
    "\n",
    "# Read with consolidated metadata (faster)\n",
    "ds = xr.open_zarr('data.zarr', consolidated=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913d792c",
   "metadata": {},
   "source": [
    "## Real-World Example: Sentinel-2 Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05767bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pystac_client\n",
    "from odc.stac import load as stac_load\n",
    "\n",
    "# Search for Sentinel-2 data\n",
    "catalog = pystac_client.Client.open(\n",
    "    'https://earth-search.aws.element84.com/v1')\n",
    "\n",
    "search = catalog.search(\n",
    "    collections=['sentinel-2-c1-l2a'],\n",
    "    bbox=[lon_min, lat_min, lon_max, lat_max],\n",
    "    datetime='2023-01-01/2023-12-31',\n",
    "    query={'eo:cloud_cover': {'lt': 30}}\n",
    ")\n",
    "items = search.item_collection()\n",
    "\n",
    "# Load as XArray with Dask\n",
    "ds = stac_load(\n",
    "    items,\n",
    "    bands=['red', 'green', 'blue', 'nir'],\n",
    "    resolution=10,\n",
    "    chunks={'time': 10, 'x': 512, 'y': 512}\n",
    ")\n",
    "\n",
    "# Calculate NDVI\n",
    "ndvi = (ds.nir - ds.red) / (ds.nir + ds.red)\n",
    "\n",
    "# Save to Zarr with compression\n",
    "encoding = {\n",
    "    'ndvi': {\n",
    "        'compressor': zarr.Blosc(cname='zstd', clevel=3),\n",
    "        'chunks': (10, 512, 512)\n",
    "    }\n",
    "}\n",
    "\n",
    "ndvi.to_dataset(name='ndvi').to_zarr(\n",
    "    'sentinel2_ndvi_2023.zarr',\n",
    "    encoding=encoding,\n",
    "    consolidated=True\n",
    ")\n",
    "\n",
    "# Load and analyze\n",
    "ndvi_loaded = xr.open_zarr('sentinel2_ndvi_2023.zarr')\n",
    "monthly_mean = ndvi_loaded.resample(time='1M').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3281e6b",
   "metadata": {},
   "source": [
    "## Performance Tips\n",
    "\n",
    "### 1. Use Consolidated Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abce7bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Always use consolidated metadata for cloud storage\n",
    "ds.to_zarr('data.zarr', consolidated=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47762c64",
   "metadata": {},
   "source": [
    "### 2. Choose Appropriate Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb180fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match chunks to access patterns\n",
    "# Time-series: large time chunks\n",
    "# Spatial: large spatial chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fcf108",
   "metadata": {},
   "source": [
    "### 3. Use Compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d06d7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Blosc with zstd is usually best\n",
    "compressor = zarr.Blosc(cname='zstd', clevel=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe70b293",
   "metadata": {},
   "source": [
    "### 4. Parallel I/O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0800223d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Dask for parallel operations\n",
    "ds.to_zarr('data.zarr', compute=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f07abd",
   "metadata": {},
   "source": [
    "### 5. Avoid Small Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0bc008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bad: too many small chunks\n",
    "chunks = (1, 10, 10)  # Only 100 items per chunk\n",
    "\n",
    "# Good: reasonable chunk size\n",
    "chunks = (10, 512, 512)  # ~2.6M items per chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6777f594",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "\n",
    "### Issue: Slow Reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe2c3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: Check chunk size and use consolidated metadata\n",
    "ds = xr.open_zarr('data.zarr', consolidated=True)\n",
    "print(ds.chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2834dc24",
   "metadata": {},
   "source": [
    "### Issue: Large File Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7ada38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: Use compression\n",
    "ds.to_zarr('data.zarr', \n",
    "           encoding={'var': {'compressor': zarr.Blosc(cname='zstd', clevel=5)}})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5ca3cb",
   "metadata": {},
   "source": [
    "### Issue: Memory Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcd7887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: Use smaller chunks\n",
    "ds = xr.open_zarr('data.zarr', chunks={'time': 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bafdf7e",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "!!! success \"What You Learned\"\n",
    "    - Zarr is optimized for cloud storage and parallel access\n",
    "    - Chunking strategy depends on access patterns\n",
    "    - Compression reduces storage costs\n",
    "    - Consolidated metadata improves performance\n",
    "    - Zarr works seamlessly with XArray and Dask\n",
    "    - Supports appending and parallel writes\n",
    "    - Multiple cloud storage backends supported\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "\u2192 Continue to [XEE for Earth Engine](xee.ipynb)\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "- [Zarr Documentation](https://zarr.readthedocs.io/)\n",
    "- [Zarr Tutorial](https://zarr.readthedocs.io/en/stable/tutorial.html)\n",
    "- [XArray Zarr Guide](https://docs.xarray.dev/en/stable/user-guide/io.html#zarr)\n",
    "- [Pangeo Zarr Guide](https://pangeo.io/data.html#zarr)\n",
    "- [Cloud-Optimized Formats](https://guide.cloudnativegeo.org/)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}