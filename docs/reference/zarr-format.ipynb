{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2aaec5e",
   "metadata": {},
   "source": [
    "# Zarr Format Reference\n",
    "\n",
    "## Overview\n",
    "\n",
    "Zarr is a format for the storage of chunked, compressed, N-dimensional arrays. It's designed for use in parallel computing and cloud storage environments.\n",
    "\n",
    "## Key Features\n",
    "\n",
    "- **Chunked Storage**: Data divided into regular chunks\n",
    "- **Compression**: Multiple compression algorithms supported\n",
    "- **Cloud-Optimized**: Efficient for object storage (S3, GCS, Azure)\n",
    "- **Parallel I/O**: Concurrent reads and writes\n",
    "- **Language-Agnostic**: Implementations in Python, Julia, C++, Java\n",
    "- **Metadata**: Flexible JSON metadata storage\n",
    "\n",
    "## Storage Structure\n",
    "\n",
    "### Directory Layout\n",
    "\n",
    "```\n",
    "data.zarr/\n",
    "├── .zarray              # Array metadata\n",
    "├── .zattrs              # User attributes\n",
    "├── .zgroup              # Group metadata (if applicable)\n",
    "└── 0.0.0                # Chunk files\n",
    "    ├── 0.0.1\n",
    "    ├── 0.1.0\n",
    "    ├── 0.1.1\n",
    "    └── ...\n",
    "```\n",
    "\n",
    "### Metadata Files\n",
    "\n",
    "#### .zarray\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"chunks\": [10, 512, 512],\n",
    "    \"compressor\": {\n",
    "        \"id\": \"blosc\",\n",
    "        \"cname\": \"zstd\",\n",
    "        \"clevel\": 3,\n",
    "        \"shuffle\": 1\n",
    "    },\n",
    "    \"dtype\": \"<f4\",\n",
    "    \"fill_value\": \"NaN\",\n",
    "    \"filters\": null,\n",
    "    \"order\": \"C\",\n",
    "    \"shape\": [365, 5000, 5000],\n",
    "    \"zarr_format\": 2\n",
    "}\n",
    "```\n",
    "\n",
    "#### .zattrs\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"title\": \"NDVI Time Series\",\n",
    "    \"source\": \"Sentinel-2\",\n",
    "    \"units\": \"dimensionless\",\n",
    "    \"valid_range\": [-1.0, 1.0]\n",
    "}\n",
    "```\n",
    "\n",
    "## Chunking\n",
    "\n",
    "### Chunk Size Selection\n",
    "\n",
    "**Formula**: Aim for 10-100 MB per chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b7154d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def optimal_chunks(shape, dtype, target_mb=10):\n",
    "    \"\"\"Calculate optimal chunk dimensions.\"\"\"\n",
    "    itemsize = np.dtype(dtype).itemsize\n",
    "    target_items = (target_mb * 1024 * 1024) / itemsize\n",
    "    \n",
    "    # Distribute evenly across dimensions\n",
    "    chunk_dim = int(target_items ** (1/len(shape)))\n",
    "    chunks = tuple(min(chunk_dim, s) for s in shape)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Example\n",
    "shape = (365, 5000, 5000)\n",
    "chunks = optimal_chunks(shape, 'float32', target_mb=10)\n",
    "print(f\"Recommended chunks: {chunks}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc410aef",
   "metadata": {},
   "source": [
    "### Access Pattern Optimization\n",
    "\n",
    "#### Time Series Access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e315daa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize for temporal access\n",
    "chunks = (1, 1000, 1000)  # Small time chunks, large spatial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62018401",
   "metadata": {},
   "source": [
    "#### Spatial Access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45822a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize for spatial access\n",
    "chunks = (365, 100, 100)  # Large time chunks, small spatial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c5fac8",
   "metadata": {},
   "source": [
    "#### Balanced Access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf7364b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balanced for mixed access\n",
    "chunks = (10, 512, 512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2138a66e",
   "metadata": {},
   "source": [
    "## Compression\n",
    "\n",
    "### Blosc (Recommended)\n",
    "\n",
    "Fast compression with multiple algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb47ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numcodecs import Blosc\n",
    "\n",
    "# Fast compression (LZ4)\n",
    "compressor = Blosc(cname='lz4', clevel=5, shuffle=Blosc.SHUFFLE)\n",
    "\n",
    "# Balanced (Zstandard)\n",
    "compressor = Blosc(cname='zstd', clevel=3, shuffle=Blosc.SHUFFLE)\n",
    "\n",
    "# High compression (Zstandard)\n",
    "compressor = Blosc(cname='zstd', clevel=9, shuffle=Blosc.BITSHUFFLE)\n",
    "\n",
    "# Use with Zarr\n",
    "import zarr\n",
    "z = zarr.open('data.zarr', mode='w', shape=(1000, 1000), \n",
    "              chunks=(100, 100), compressor=compressor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b33a42",
   "metadata": {},
   "source": [
    "### Other Compressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e1abac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numcodecs import Zlib, GZip, BZ2, LZMA\n",
    "\n",
    "# Zlib (good compression, moderate speed)\n",
    "compressor = Zlib(level=5)\n",
    "\n",
    "# GZip (widely compatible)\n",
    "compressor = GZip(level=6)\n",
    "\n",
    "# BZ2 (high compression, slow)\n",
    "compressor = BZ2(level=9)\n",
    "\n",
    "# LZMA (highest compression, slowest)\n",
    "compressor = LZMA(preset=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74e18d2",
   "metadata": {},
   "source": [
    "### Compression Comparison\n",
    "\n",
    "| Compressor | Speed | Ratio | Use Case |\n",
    "|------------|-------|-------|----------|\n",
    "| LZ4 | Very Fast | Low | Real-time processing |\n",
    "| Zstandard | Fast | Good | General purpose |\n",
    "| Zlib | Medium | Good | Compatibility |\n",
    "| BZ2 | Slow | High | Archival |\n",
    "| LZMA | Very Slow | Highest | Long-term storage |\n",
    "\n",
    "## XArray Integration\n",
    "\n",
    "### Writing to Zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc3e985",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "from numcodecs import Blosc\n",
    "\n",
    "# Create dataset\n",
    "ds = xr.Dataset({\n",
    "    'temperature': (['time', 'y', 'x'], data)\n",
    "})\n",
    "\n",
    "# Configure encoding\n",
    "encoding = {\n",
    "    'temperature': {\n",
    "        'compressor': Blosc(cname='zstd', clevel=3),\n",
    "        'chunks': (10, 512, 512)\n",
    "    }\n",
    "}\n",
    "\n",
    "# Write to Zarr\n",
    "ds.to_zarr('data.zarr', encoding=encoding, consolidated=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d09c4c",
   "metadata": {},
   "source": [
    "### Reading from Zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7172e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open Zarr store\n",
    "ds = xr.open_zarr('data.zarr', consolidated=True)\n",
    "\n",
    "# With custom chunks\n",
    "ds = xr.open_zarr('data.zarr', chunks={'time': 5})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43bd9a5a",
   "metadata": {},
   "source": [
    "### Appending Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844c588a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial write\n",
    "ds1.to_zarr('timeseries.zarr', mode='w')\n",
    "\n",
    "# Append along time dimension\n",
    "ds2.to_zarr('timeseries.zarr', append_dim='time')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8a0dd4",
   "metadata": {},
   "source": [
    "## Cloud Storage\n",
    "\n",
    "### AWS S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e3fb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import s3fs\n",
    "import xarray as xr\n",
    "\n",
    "# Create S3 filesystem\n",
    "s3 = s3fs.S3FileSystem(anon=False)\n",
    "\n",
    "# Write to S3\n",
    "store = s3fs.S3Map(root='s3://my-bucket/data.zarr', s3=s3)\n",
    "ds.to_zarr(store, mode='w', consolidated=True)\n",
    "\n",
    "# Read from S3\n",
    "ds = xr.open_zarr(store, consolidated=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfb9b62",
   "metadata": {},
   "source": [
    "### Google Cloud Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672dd0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gcsfs\n",
    "\n",
    "# Create GCS filesystem\n",
    "gcs = gcsfs.GCSFileSystem(token='anon')\n",
    "\n",
    "# Write to GCS\n",
    "store = gcsfs.GCSMap('gs://my-bucket/data.zarr', gcs=gcs)\n",
    "ds.to_zarr(store, mode='w', consolidated=True)\n",
    "\n",
    "# Read from GCS\n",
    "ds = xr.open_zarr(store, consolidated=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bcdf9f",
   "metadata": {},
   "source": [
    "### Azure Blob Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae68ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import adlfs\n",
    "\n",
    "# Create Azure filesystem\n",
    "fs = adlfs.AzureBlobFileSystem(account_name='myaccount')\n",
    "\n",
    "# Write to Azure\n",
    "store = fs.get_mapper('container/data.zarr')\n",
    "ds.to_zarr(store, mode='w', consolidated=True)\n",
    "\n",
    "# Read from Azure\n",
    "ds = xr.open_zarr(store, consolidated=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89230974",
   "metadata": {},
   "source": [
    "## Consolidated Metadata\n",
    "\n",
    "Improves performance by reducing number of reads.\n",
    "\n",
    "### Create Consolidated Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9a68d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# During write\n",
    "ds.to_zarr('data.zarr', consolidated=True)\n",
    "\n",
    "# After write\n",
    "from zarr.convenience import consolidate_metadata\n",
    "consolidate_metadata('data.zarr')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d50c176",
   "metadata": {},
   "source": [
    "### Use Consolidated Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7ad534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read with consolidated metadata (faster)\n",
    "ds = xr.open_zarr('data.zarr', consolidated=True)\n",
    "\n",
    "# Without consolidated metadata (slower)\n",
    "ds = xr.open_zarr('data.zarr', consolidated=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b876e1",
   "metadata": {},
   "source": [
    "## Parallel I/O\n",
    "\n",
    "### Parallel Writing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03056fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "import dask.array as da\n",
    "\n",
    "client = Client()\n",
    "\n",
    "# Create Dask array\n",
    "data = da.random.random((1000, 5000, 5000), chunks=(10, 500, 500))\n",
    "\n",
    "# Create dataset\n",
    "ds = xr.Dataset({'data': (['time', 'y', 'x'], data)})\n",
    "\n",
    "# Parallel write\n",
    "ds.to_zarr('large_data.zarr', compute=True, consolidated=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e546ad",
   "metadata": {},
   "source": [
    "### Parallel Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fd5b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open with Dask chunks\n",
    "ds = xr.open_zarr('large_data.zarr', chunks={'time': 10})\n",
    "\n",
    "# Operations are parallelized\n",
    "result = ds.mean(dim='time').compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150f2ce7",
   "metadata": {},
   "source": [
    "## Filters\n",
    "\n",
    "Apply transformations before compression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccea5e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numcodecs import Delta, FixedScaleOffset\n",
    "\n",
    "# Delta encoding (for time series)\n",
    "filters = [Delta(dtype='i4')]\n",
    "\n",
    "# Scale and offset\n",
    "filters = [FixedScaleOffset(offset=0, scale=0.01, dtype='f4')]\n",
    "\n",
    "# Use with Zarr\n",
    "z = zarr.open('data.zarr', mode='w', shape=(1000,), \n",
    "              filters=filters, compressor=compressor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2655d5",
   "metadata": {},
   "source": [
    "## Groups and Hierarchies\n",
    "\n",
    "### Creating Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900a2800",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zarr\n",
    "\n",
    "# Create root group\n",
    "root = zarr.open_group('data.zarr', mode='w')\n",
    "\n",
    "# Create subgroups\n",
    "temp_group = root.create_group('temperature')\n",
    "precip_group = root.create_group('precipitation')\n",
    "\n",
    "# Create arrays in groups\n",
    "temp_group.create_dataset('daily', shape=(365, 100, 100), chunks=(10, 50, 50))\n",
    "precip_group.create_dataset('daily', shape=(365, 100, 100), chunks=(10, 50, 50))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfdfadd",
   "metadata": {},
   "source": [
    "### Reading Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686768e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open group\n",
    "root = zarr.open_group('data.zarr', mode='r')\n",
    "\n",
    "# Access subgroups\n",
    "temp = root['temperature']\n",
    "precip = root['precipitation']\n",
    "\n",
    "# Access arrays\n",
    "daily_temp = temp['daily']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ded18a",
   "metadata": {},
   "source": [
    "## Best Practices\n",
    "\n",
    "### 1. Use Consolidated Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390901af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Always use for cloud storage\n",
    "ds.to_zarr('s3://bucket/data.zarr', consolidated=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545fc8e9",
   "metadata": {},
   "source": [
    "### 2. Choose Appropriate Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efd9a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match access patterns\n",
    "# Time series: large time chunks\n",
    "chunks = (100, 512, 512)\n",
    "\n",
    "# Spatial: large spatial chunks\n",
    "chunks = (10, 1024, 1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc4f26b",
   "metadata": {},
   "source": [
    "### 3. Use Compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59321ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Always compress for cloud storage\n",
    "compressor = Blosc(cname='zstd', clevel=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d355f699",
   "metadata": {},
   "source": [
    "### 4. Avoid Small Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09d4544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Good: ~10-100 MB per chunk\n",
    "chunks = (10, 512, 512)  # ~10 MB for float32\n",
    "\n",
    "# Bad: Too small\n",
    "chunks = (1, 10, 10)  # ~400 bytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eed22e3",
   "metadata": {},
   "source": [
    "### 5. Use Appropriate Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c23018d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Good: Use smallest appropriate dtype\n",
    "ds = ds.astype('float32')\n",
    "\n",
    "# Bad: Unnecessary precision\n",
    "ds = ds.astype('float64')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd99b583",
   "metadata": {},
   "source": [
    "## Performance Optimization\n",
    "\n",
    "### Chunk Size Impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8ab129",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Test different chunk sizes\n",
    "chunk_sizes = [(5, 256, 256), (10, 512, 512), (20, 1024, 1024)]\n",
    "\n",
    "for chunks in chunk_sizes:\n",
    "    start = time.time()\n",
    "    ds.chunk(chunks).to_zarr(f'test_{chunks[0]}.zarr', mode='w')\n",
    "    write_time = time.time() - start\n",
    "    \n",
    "    start = time.time()\n",
    "    loaded = xr.open_zarr(f'test_{chunks[0]}.zarr').compute()\n",
    "    read_time = time.time() - start\n",
    "    \n",
    "    print(f\"Chunks {chunks}: Write={write_time:.2f}s, Read={read_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8ed336",
   "metadata": {},
   "source": [
    "### Compression Impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47340c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test compression algorithms\n",
    "compressors = {\n",
    "    'none': None,\n",
    "    'lz4': Blosc(cname='lz4', clevel=5),\n",
    "    'zstd': Blosc(cname='zstd', clevel=3),\n",
    "    'zlib': Zlib(level=5)\n",
    "}\n",
    "\n",
    "for name, comp in compressors.items():\n",
    "    encoding = {'data': {'compressor': comp}}\n",
    "    ds.to_zarr(f'test_{name}.zarr', encoding=encoding, mode='w')\n",
    "    \n",
    "    # Check size\n",
    "    size = sum(f.stat().st_size for f in Path(f'test_{name}.zarr').rglob('*'))\n",
    "    print(f\"{name}: {size/1e6:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a23fee",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "\n",
    "### Issue: Slow Reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2425dd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 1: Use consolidated metadata\n",
    "ds = xr.open_zarr('data.zarr', consolidated=True)\n",
    "\n",
    "# Solution 2: Check chunk size\n",
    "print(ds.chunks)\n",
    "\n",
    "# Solution 3: Use appropriate chunks for access pattern\n",
    "ds = xr.open_zarr('data.zarr', chunks={'time': 10})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0198d136",
   "metadata": {},
   "source": [
    "### Issue: Large File Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176474ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: Use compression\n",
    "encoding = {\n",
    "    'var': {'compressor': Blosc(cname='zstd', clevel=5)}\n",
    "}\n",
    "ds.to_zarr('data.zarr', encoding=encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65342ef5",
   "metadata": {},
   "source": [
    "### Issue: Memory Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfac90a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: Use smaller chunks\n",
    "ds = xr.open_zarr('data.zarr', chunks={'time': 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fde091",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "- [Zarr Documentation](https://zarr.readthedocs.io/)\n",
    "- [Zarr Tutorial](https://zarr.readthedocs.io/en/stable/tutorial.html)\n",
    "- [Zarr Specification](https://zarr-specs.readthedocs.io/)\n",
    "- [XArray Zarr Guide](https://docs.xarray.dev/en/stable/user-guide/io.html#zarr)\n",
    "- [Pangeo Zarr Examples](https://pangeo.io/data.html#zarr)\n",
    "\n",
    "## Quick Reference\n",
    "\n",
    "### Common Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb662bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zarr\n",
    "import xarray as xr\n",
    "\n",
    "# Create Zarr array\n",
    "z = zarr.open('data.zarr', mode='w', shape=(1000, 1000), \n",
    "              chunks=(100, 100), dtype='f4')\n",
    "\n",
    "# Write XArray to Zarr\n",
    "ds.to_zarr('data.zarr', mode='w', consolidated=True)\n",
    "\n",
    "# Read Zarr with XArray\n",
    "ds = xr.open_zarr('data.zarr', consolidated=True)\n",
    "\n",
    "# Append data\n",
    "ds.to_zarr('data.zarr', append_dim='time')\n",
    "\n",
    "# Consolidate metadata\n",
    "from zarr.convenience import consolidate_metadata\n",
    "consolidate_metadata('data.zarr')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8071494a",
   "metadata": {},
   "source": [
    "### Recommended Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b9d6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General purpose\n",
    "compressor = Blosc(cname='zstd', clevel=3, shuffle=Blosc.SHUFFLE)\n",
    "chunks = (10, 512, 512)  # For (time, y, x)\n",
    "\n",
    "# High compression (archival)\n",
    "compressor = Blosc(cname='zstd', clevel=9, shuffle=Blosc.BITSHUFFLE)\n",
    "\n",
    "# Fast I/O (real-time)\n",
    "compressor = Blosc(cname='lz4', clevel=5, shuffle=Blosc.SHUFFLE)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
