{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a664b84d",
   "metadata": {},
   "source": [
    "# Dask Best Practices\n",
    "\n",
    "## Overview\n",
    "\n",
    "Dask is a flexible parallel computing library for Python that scales from laptops to clusters. It's particularly useful for processing large remote sensing datasets that don't fit in memory.\n",
    "\n",
    "## Core Concepts\n",
    "\n",
    "### Lazy Evaluation\n",
    "\n",
    "Dask operations are lazy - they build a task graph without executing until you call `.compute()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8272a2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "import xarray as xr\n",
    "\n",
    "# This doesn't load data\n",
    "ds = xr.open_dataset('large_file.nc', chunks={'time': 10})\n",
    "\n",
    "# This builds a task graph (still lazy)\n",
    "result = ds.mean(dim='time')\n",
    "\n",
    "# This triggers computation\n",
    "result_computed = result.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62d5d5c",
   "metadata": {},
   "source": [
    "### Task Graph\n",
    "\n",
    "Dask builds a graph of operations to optimize execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036ecaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View task graph\n",
    "result.data.visualize(filename='task_graph.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51775fc",
   "metadata": {},
   "source": [
    "## Chunking Strategies\n",
    "\n",
    "### Chunk Size Guidelines\n",
    "\n",
    "**Optimal chunk size**: 10-100 MB per chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4d5936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate chunk size\n",
    "import numpy as np\n",
    "\n",
    "def calculate_chunk_size_mb(shape, dtype, chunks):\n",
    "    \"\"\"Calculate chunk size in MB.\"\"\"\n",
    "    itemsize = np.dtype(dtype).itemsize\n",
    "    chunk_items = np.prod([chunks[i] if i < len(chunks) else shape[i] \n",
    "                           for i in range(len(shape))])\n",
    "    return (chunk_items * itemsize) / 1e6\n",
    "\n",
    "# Example\n",
    "shape = (365, 5000, 5000)\n",
    "chunks = (10, 500, 500)\n",
    "size_mb = calculate_chunk_size_mb(shape, 'float32', chunks)\n",
    "print(f\"Chunk size: {size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637b627c",
   "metadata": {},
   "source": [
    "### Time Series Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a3dd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Good: Large time chunks for temporal operations\n",
    "ds = xr.open_dataset('timeseries.nc', chunks={'time': 100, 'x': 512, 'y': 512})\n",
    "\n",
    "# Bad: Small time chunks\n",
    "ds = xr.open_dataset('timeseries.nc', chunks={'time': 1, 'x': 512, 'y': 512})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa341dc",
   "metadata": {},
   "source": [
    "### Spatial Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bedc8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Good: Balanced spatial chunks\n",
    "ds = xr.open_dataset('spatial.nc', chunks={'time': 10, 'x': 512, 'y': 512})\n",
    "\n",
    "# Bad: Unbalanced chunks\n",
    "ds = xr.open_dataset('spatial.nc', chunks={'time': 10, 'x': 5000, 'y': 10})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8e961d",
   "metadata": {},
   "source": [
    "### Auto Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8070c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let Dask decide\n",
    "ds = xr.open_dataset('file.nc', chunks='auto')\n",
    "\n",
    "# With target chunk size\n",
    "ds = xr.open_zarr('data.zarr', chunks={'time': 'auto'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9125c5d2",
   "metadata": {},
   "source": [
    "## Dask Schedulers\n",
    "\n",
    "### Single Machine (Threaded)\n",
    "\n",
    "Default scheduler, good for I/O-bound tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b535a512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatic (default)\n",
    "result = ds.mean().compute()\n",
    "\n",
    "# Explicit\n",
    "result = ds.mean().compute(scheduler='threads')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1182f997",
   "metadata": {},
   "source": [
    "### Single Machine (Processes)\n",
    "\n",
    "Better for CPU-bound tasks, avoids GIL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac366d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = ds.mean().compute(scheduler='processes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f19215b",
   "metadata": {},
   "source": [
    "### Distributed\n",
    "\n",
    "Best for large computations, provides dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d735998",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, LocalCluster\n",
    "\n",
    "# Start local cluster\n",
    "cluster = LocalCluster(\n",
    "    n_workers=4,\n",
    "    threads_per_worker=2,\n",
    "    memory_limit='4GB'\n",
    ")\n",
    "client = Client(cluster)\n",
    "\n",
    "# Computations use distributed scheduler\n",
    "result = ds.mean().compute()\n",
    "\n",
    "# Close when done\n",
    "client.close()\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137d2fae",
   "metadata": {},
   "source": [
    "## Memory Management\n",
    "\n",
    "### Persist vs Compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e7c7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute(): Load into memory as numpy/pandas\n",
    "result = ds.mean().compute()\n",
    "\n",
    "# persist(): Keep as Dask array in distributed memory\n",
    "result = ds.mean().persist()\n",
    "\n",
    "# Use persist() for intermediate results\n",
    "intermediate = ds.resample(time='1M').mean().persist()\n",
    "result1 = intermediate.max()\n",
    "result2 = intermediate.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddfa1d4",
   "metadata": {},
   "source": [
    "### Clear Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a110ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete large objects\n",
    "del large_array\n",
    "\n",
    "# Clear Dask cache\n",
    "from dask import config\n",
    "config.set(scheduler='synchronous')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44aafeb5",
   "metadata": {},
   "source": [
    "## Optimization Techniques\n",
    "\n",
    "### 1. Rechunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3088f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rechunk for different access patterns\n",
    "ds_rechunked = ds.chunk({'time': 1, 'x': 1000, 'y': 1000})\n",
    "\n",
    "# Optimize chunks\n",
    "from dask.array import rechunk\n",
    "optimized = rechunk(ds.data, chunks=(10, 512, 512))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8480b241",
   "metadata": {},
   "source": [
    "### 2. Avoid Small Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0307c80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Good: Reasonable chunk size\n",
    "ds = ds.chunk({'time': 10, 'x': 512, 'y': 512})\n",
    "\n",
    "# Bad: Too many small tasks\n",
    "ds = ds.chunk({'time': 1, 'x': 10, 'y': 10})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2eae81a",
   "metadata": {},
   "source": [
    "### 3. Use map_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e5eba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_block(block):\n",
    "    # Custom processing\n",
    "    return block * 2 + 10\n",
    "\n",
    "result = ds.map_blocks(process_block, dtype=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2185d5",
   "metadata": {},
   "source": [
    "### 4. Avoid Repeated Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4124c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Good: Compute once\n",
    "mean = ds.mean(dim='time').persist()\n",
    "result1 = mean + 10\n",
    "result2 = mean * 2\n",
    "\n",
    "# Bad: Recompute each time\n",
    "result1 = ds.mean(dim='time') + 10\n",
    "result2 = ds.mean(dim='time') * 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4c48f4",
   "metadata": {},
   "source": [
    "## Distributed Computing\n",
    "\n",
    "### Local Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5dc695",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, LocalCluster\n",
    "\n",
    "cluster = LocalCluster(\n",
    "    n_workers=4,\n",
    "    threads_per_worker=2,\n",
    "    memory_limit='2GB',\n",
    "    processes=True\n",
    ")\n",
    "client = Client(cluster)\n",
    "\n",
    "# View dashboard\n",
    "print(client.dashboard_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61ad02f",
   "metadata": {},
   "source": [
    "### Cluster Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43540331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom worker configuration\n",
    "cluster = LocalCluster(\n",
    "    n_workers=8,\n",
    "    threads_per_worker=1,\n",
    "    memory_limit='4GB',\n",
    "    processes=True,\n",
    "    silence_logs=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6292b3",
   "metadata": {},
   "source": [
    "### Adaptive Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23eea55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-scale workers\n",
    "cluster.adapt(minimum=2, maximum=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed2f03a",
   "metadata": {},
   "source": [
    "## Monitoring\n",
    "\n",
    "### Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01febad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access dashboard\n",
    "print(client.dashboard_link)\n",
    "# Usually: http://localhost:8787/status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9962c800",
   "metadata": {},
   "source": [
    "### Progress Bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3101b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "with ProgressBar():\n",
    "    result = ds.mean().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54fcee0",
   "metadata": {},
   "source": [
    "### Performance Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbd8fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import performance_report\n",
    "\n",
    "with performance_report(filename='dask-report.html'):\n",
    "    result = ds.mean().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c9fbdd",
   "metadata": {},
   "source": [
    "## Common Patterns\n",
    "\n",
    "### Time Series Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955e5e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monthly aggregation\n",
    "monthly = ds.resample(time='1M').mean()\n",
    "\n",
    "# With Dask\n",
    "monthly = ds.chunk({'time': 30}).resample(time='1M').mean().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a633f760",
   "metadata": {},
   "source": [
    "### Spatial Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57b6d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spatial mean\n",
    "spatial_mean = ds.mean(dim=['x', 'y'])\n",
    "\n",
    "# With Dask\n",
    "spatial_mean = ds.chunk({'x': 512, 'y': 512}).mean(dim=['x', 'y']).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4876a23f",
   "metadata": {},
   "source": [
    "### Rolling Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a997268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7-day rolling mean\n",
    "rolling = ds.rolling(time=7, center=True).mean()\n",
    "\n",
    "# With Dask\n",
    "rolling = ds.chunk({'time': 30}).rolling(time=7, center=True).mean().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007d72eb",
   "metadata": {},
   "source": [
    "### GroupBy Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78276e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monthly climatology\n",
    "monthly_clim = ds.groupby('time.month').mean()\n",
    "\n",
    "# With Dask\n",
    "monthly_clim = ds.chunk({'time': 30}).groupby('time.month').mean().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9908c5b5",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "\n",
    "### Memory Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e48220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 1: Smaller chunks\n",
    "ds = ds.chunk({'time': 5, 'x': 256, 'y': 256})\n",
    "\n",
    "# Solution 2: More workers with less memory each\n",
    "cluster = LocalCluster(n_workers=8, memory_limit='1GB')\n",
    "\n",
    "# Solution 3: Process in batches\n",
    "for year in range(2020, 2024):\n",
    "    subset = ds.sel(time=str(year))\n",
    "    result = subset.mean().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05c9eb2",
   "metadata": {},
   "source": [
    "### Slow Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2b3221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 1: Check chunk size\n",
    "print(ds.chunks)\n",
    "\n",
    "# Solution 2: Use distributed scheduler\n",
    "client = Client()\n",
    "\n",
    "# Solution 3: Persist intermediate results\n",
    "intermediate = ds.mean(dim='time').persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbbfcca",
   "metadata": {},
   "source": [
    "### Too Many Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5dbb15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: Increase chunk size\n",
    "ds = ds.chunk({'time': 50, 'x': 1024, 'y': 1024})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92bfac9",
   "metadata": {},
   "source": [
    "## Best Practices Summary\n",
    "\n",
    "### ✅ Do\n",
    "\n",
    "1. **Use appropriate chunk sizes** (10-100 MB)\n",
    "2. **Persist intermediate results** used multiple times\n",
    "3. **Use distributed scheduler** for large computations\n",
    "4. **Monitor with dashboard**\n",
    "5. **Close clients and clusters** when done\n",
    "6. **Rechunk for access patterns**\n",
    "7. **Use map_blocks for custom functions**\n",
    "\n",
    "### ❌ Don't\n",
    "\n",
    "1. **Create too many small chunks**\n",
    "2. **Call compute() repeatedly** on same data\n",
    "3. **Mix schedulers** in same script\n",
    "4. **Forget to close** distributed clients\n",
    "5. **Use tiny chunk sizes** (< 1 MB)\n",
    "6. **Use huge chunk sizes** (> 1 GB)\n",
    "7. **Ignore memory limits**\n",
    "\n",
    "## Configuration\n",
    "\n",
    "### Global Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8965cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "\n",
    "# Set scheduler\n",
    "dask.config.set(scheduler='threads')\n",
    "\n",
    "# Set chunk size\n",
    "dask.config.set({'array.chunk-size': '128 MiB'})\n",
    "\n",
    "# Disable task fusion\n",
    "dask.config.set({'optimization.fuse.active': False})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457c8368",
   "metadata": {},
   "source": [
    "### Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5679cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Set number of threads\n",
    "export OMP_NUM_THREADS=4\n",
    "\n",
    "# Set Dask config directory\n",
    "export DASK_CONFIG=/path/to/config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3c3a86",
   "metadata": {},
   "source": [
    "## Performance Tips\n",
    "\n",
    "### 1. Optimize I/O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad1dc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Good: Read with chunks\n",
    "ds = xr.open_zarr('data.zarr', chunks='auto')\n",
    "\n",
    "# Bad: Read without chunks\n",
    "ds = xr.open_zarr('data.zarr')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265d48a7",
   "metadata": {},
   "source": [
    "### 2. Use Efficient Formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12efc66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Good: Zarr (cloud-optimized)\n",
    "ds.to_zarr('output.zarr')\n",
    "\n",
    "# OK: NetCDF with compression\n",
    "ds.to_netcdf('output.nc', encoding={'var': {'zlib': True}})\n",
    "\n",
    "# Bad: Uncompressed NetCDF\n",
    "ds.to_netcdf('output.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f06900c",
   "metadata": {},
   "source": [
    "### 3. Minimize Data Transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a6e81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Good: Reduce before computing\n",
    "result = ds.mean(dim=['x', 'y']).compute()\n",
    "\n",
    "# Bad: Compute then reduce\n",
    "result = ds.compute().mean(dim=['x', 'y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c41fc8",
   "metadata": {},
   "source": [
    "### 4. Use Appropriate Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d302f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Good: Use smaller dtypes when possible\n",
    "ds = ds.astype('float32')\n",
    "\n",
    "# Bad: Unnecessary precision\n",
    "ds = ds.astype('float64')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9063cfbe",
   "metadata": {},
   "source": [
    "## Advanced Topics\n",
    "\n",
    "### Custom Schedulers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5943f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.threaded import get\n",
    "\n",
    "result = ds.mean().compute(scheduler=get)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c80c3e",
   "metadata": {},
   "source": [
    "### Task Priorities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88510edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# High priority tasks\n",
    "result = ds.mean().compute(priority=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460d38d6",
   "metadata": {},
   "source": [
    "### Resource Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b035f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = LocalCluster(\n",
    "    n_workers=4,\n",
    "    resources={'GPU': 1}\n",
    ")\n",
    "\n",
    "# Use resources\n",
    "result = ds.map_blocks(gpu_function, resources={'GPU': 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7838a45",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "- [Dask Documentation](https://docs.dask.org/)\n",
    "- [Dask Best Practices](https://docs.dask.org/en/stable/best-practices.html)\n",
    "- [Dask Tutorial](https://tutorial.dask.org/)\n",
    "- [Dask Examples](https://examples.dask.org/)\n",
    "- [Pangeo Dask Guide](https://pangeo.io/packages.html#dask)\n",
    "\n",
    "## Quick Reference\n",
    "\n",
    "### Common Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57aae9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load with chunks\n",
    "ds = xr.open_dataset('file.nc', chunks={'time': 10})\n",
    "\n",
    "# Compute\n",
    "result = ds.mean().compute()\n",
    "\n",
    "# Persist\n",
    "result = ds.mean().persist()\n",
    "\n",
    "# Rechunk\n",
    "ds = ds.chunk({'time': 20})\n",
    "\n",
    "# Start client\n",
    "from dask.distributed import Client\n",
    "client = Client()\n",
    "\n",
    "# Close client\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652493b2",
   "metadata": {},
   "source": [
    "### Chunk Size Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d11d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target: 10-100 MB per chunk\n",
    "# Formula: chunk_size = (chunk_items * itemsize) / 1e6\n",
    "\n",
    "# Example for float32 (4 bytes)\n",
    "# 10 MB: ~2.5M items\n",
    "# 100 MB: ~25M items\n",
    "\n",
    "# For (time, x, y) = (10, 500, 500)\n",
    "# Items = 10 * 500 * 500 = 2.5M\n",
    "# Size = 2.5M * 4 bytes = 10 MB ✓"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
